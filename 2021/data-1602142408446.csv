paper_id,name,answer,title,abstract
I5-001,Chenzhou Cui,"National Astronomical Observatories, Chinese Academy of Sciences",Teaching Resources for Virtual Observatory,"The Virtual Observatory (VO) aims to provide a research environment that will open new possibilities for scientific research based on data discovery, efficient data access, and interoperability. Besides being a data-intensively online astronomical research environment, VO can be a most effective resource with its excellent educational tools and knowledge bases, deriving directly from professional astronomy. Education Interest Group (EduIG) of the International Virtual Observatory Alliance (IVOA) is a communication channel between the IVOA community and the public of educators and students. During the last two decades, a large member of teaching and training resources for VO have been developed and contributed by the IVOA and its project members. In my talk, the role of VO in education and an overview of these resources will be presented. Additionally, in March 2021, the IVOA and the IAU Office of Astronomy for Development (OAD) signed a Memorandum of Understanding (MoU). The goal is to support the use astronomical data for education, development, and public outreach. International Worldwide Telescope Guided Tour Contest is listed as one pilot activity of the collaboration. The official website for the first international contest will be lunched at my talk."
X0-001,Antoine Basset,CNES,EleFits: A modern C++ API on top of CFITSIO,"EleFits is a new C++ package to read and write FITS files which focuses on (1) safety, (2) user-friendliness, and (3) performance.
This paper presents the development rationale, examples, and comparisons to the available alternatives: CFITSIO, CCfits, SFITSIO, and AFW.
(1) Safety first:
To our knowledge, EleFits is the only option which features a fully consistent and optimal internal type management system.
Nevertheless, the API is templated and agnostic of the underlying architecture, which makes usage straightforward.
(2) As for ease of use, EleFits is shown to be more compact and to involve less parameters than its alternatives, which makes it both simpler and less error-prone.
Most of the basic services of CFITSIO have been implemented already, and more advanced ones are being added regularly.
Furthermore, EleFits provides exclusive features like HDU selectors and automatic buffering of binary tables.
(3) To maximize performance, EleFits is built as a CFITSIO wrapper as thin as possible.
A benchmark was developed to compare I/O times, with CFITSIO as the reference.
While the two libraries are generally equivalent, optimizations implemented internally make EleFits even faster in some classical cases, unless the CFITSIO user spends considerable development efforts.
The package is used since years internally to the Euclid ground segment and is being open-sourced."
O6-002,Vincent Picouet,LAM,pyds9plugin: a DS9 extension for quicklook processing scalable into a multi-processing pipeline,"Current fits viewer applications (SAOImage DS9, Aladin, GINGA, Glue) have been developed to optimize the visualization of astronomical images while keeping some interesting specificities: linked-data exploration, interactive sky atlass access, flexible and extensible visualization toolkit, etc. 
 Not initially designed for image processing, these software do not address this need as it would break their conceptual integrity. This leaves it to big instrument consortium who will design their own data processing pipeline which will, most of the time, be too specific to be re-used by the astronomy community.

For this particular discipline evolving towards Jim Gray's fourth paradigm and where important part of the job relies on astronomical images analysis, this draws a new challenge for current and future imaging software: 
mimic what has been done for bio image analysis by addressing the current frontier of image processing. 

Among the different difficulties that need to be addressed to make the processing software beneficial (catalyze code collaboration, extensibility, multi-image analysis), one key aspect is to keep the essential high level interactivity between the data and the user which became a consensual feature for visualization.
For a number of reason that I will go through in the talk, the development of plugins for fits viewers application represent a very interesting way to take up the challenge of developing image processing tools and address the related difficulties.

I will then use this talk to introduce you pyds9plugin: a naive simple attempt to design such a plugin.
This DS9 quick look plugin is a public domain versatile extension I designed for DS9 visualization software. 
The plugin pushes DS9 visualization software a step further, by allowing to analyze and process in real time these images while keeping a high level of interactivity. The processing functions can then be generalized automatically to a set of images, to turn the quicklook tool into a real multi-processing pipeline.

This plugin incorporates essential functions (radial profile, 2D/3D fitting, trough-focus, stacking, background removal, source/artefact extraction, etc.) to extract quantitative and comprehensive information from imaging data sets in order to support instrumentation, reduce your observations, analyze the performance of your data, etc.
We also linked the most famous astronomical codes (SExtractor, SWARP, etc.) to the plugin to allow research-grade analysis and processing. 
As all researchers have different needs, the plugin is designed in a comprehensive way so that everyone can easily add its own macros that can then be run quickly and automatically on a set of images thanks to multi-processing.
This Pyds9plugin, available both on Pypi and Github, tries to gather a glimpse of all the possibilities that offers DS9 extensibility so that it motivates astronomers exploring this implementation approach."
I7-001,Michelle Lochner,University of the Western Cape / South African Radio Astronomy Observatory,Anomaly Detection in Astronomical Data using Machine Learning,"The next generation of telescopes such as the SKA and the Rubin Observatory will produce enormous data sets, far too large for traditional analysis techniques. Machine learning has proven invaluable in handling large data volumes and automating many tasks traditionally done by human scientists. In this talk, I will discuss how machine learning for anomaly detection can help automate the process of locating unusual astronomical objects in large datasets thus enabling new cosmic discoveries. The framework of using active learning to build a recommendation engine for interesting anomalies is general and could be applied in other fields."
O7-006,Emmanuel Caux,"IRAP/CNRS-UPS-CNRS, Toulouse, France",Artificial Intelligence for Automatic Identification of Spectral Lines,"Astronomical spectra can be made up of hundreds or even thousands of emission and absorption lines. Astronomers need to identify each line in order to be able to determine the physical conditions of the objects studied as the temperature of the sources and the column densities of the observed species (molecules and/or atoms). The constant improvement of instruments in terms of sensitivity, instantaneous spectral range, and spatial resolution capabilities produces a mass of broad spectra with high spectral resolution for which handmade line identification is ineffective and even maybe impossible. 

With the advent of BIG DATA, AI algorithms have proven to be very effective in solving complex problems (mainly related to classification and prediction tasks) for many different fields including astrophysics. The aim of this study is to automate the identification of species from their observed lines in rich astronomical spectra by combining methods in signal processing and machine learning with expert knowledge. 

This talk will cover three solutions based on (1) wavelets transform, expert knowledge and decision trees to identify the species associated to each spectral line, (2) Artificial Neural Networks to predict if a species is present in a spectrum or not and (3) a greedy algorithm that simulate successively the presence of each species of the database (and its isotopes) in order to check its correspondance in the spectrum. Last, we combine and compare these methods to improve our results. 

The results of our research, using an ALMA spectrum very rich in molecular lines combined with the use of CDMS and JPL molecular spectroscopic databases, have already allowed us to find a molecule that had never been detected in the spectrum experimented."
O4-002,Mathieu Servillat,LUTH - Observatoire de Paris,FAIR high level data for Cherenkov astronomy,"An effort is on-going to make astronomy data FAIR: Findable, Accessible, Interoperable and Reusable. The Reusability principle is particularly subtle as the question of trust is raised. Provenance information on the data is essential to ensure this trust, and it should come with the proper granularity and level of details. The effort to specify the provenance of astronomy data is shared with current and future large observatories in the context of the ESCAPE European project and the International Virtual Observatory Alliance (IVOA). We organized discussions on common approaches across all wavebands. In particular, Cherenkov astronomy provides complex data based on the detection of Cherenkov light generated in the atmosphere by air showers induced by energetic cosmic particles. Unlike current experiments such as H.E.S.S., MAGIC or VERITAS, the next generation Cherenkov Telescope Array (CTA) will be an open observatory with public access to its high level science data products. The complex treatment of raw Cherenkov data implies to attach a detailed description of the high level data and their provenance.

We developed a prototype platform to connect the first H.E.S.S. public test data release to the Virtual Observatory through an IVOA ObsTAP service: relevant metadata is mapped to the IVOA ObsCore data model main fields and additional specific metadata is provided (F, A). The high level data follows a proposed standard data format for gamma-ray astronomy (GADF) to ensure wider interoperability (I). We finally designed a provenance management system in connection with the development of pipelines and analysis tools for CTA (ctapipe and gammapy), in order to collect rich and detailed provenance information, as recommended by the FAIR principles (R). The prototype platform thus implements the main functionalities of a science gateway, including data search and access, online processing, and traceability of the various actions performed by a user."
O7-004,Valentina Cesare,"INAF, Osservatorio Astrofisico di Catania",Gaia AVU-GSR parallel solver towards exascale infrastructure,"The Gaia Astrometric Verification Unit – Global Sphere Reconstruction (AVU-GSR) Parallel Solver aims to find the astrometric parameters for ~1 billion of stars in the Milky Way, the attitude and the instrumental parameters of the Gaia satellite, and the global parameter gamma of the post Newtonian formalism RAMOD. To perform this task, the code iteratively solves a system of linear equations, A x = b, where the coefficient matrix A is large, having a dimension of ~10^10 x 10^8, and sparse.
To deal with this big data problem, the matrix A is compactified through an ad-hoc compression algorithm and only its elements different from zero are considered during computation. The matrix dimension reduces from ~10^10 x 10^8 to ~10^10 x 10^1. To solve this system, the code exploits a hybrid implementation of the iterative PC-LSQR algorithm, where the computation related to different horizontal portions of the coefficient matrix is assigned to different MPI processes. In the original code, each matrix portion is further parallelized over the OpenMP threads. To further improve the code performance, we ported the application on the GPU, replacing the OpenMP part with OpenACC. In this porting, the ~95% of the data is copied at the beginning of the entire cycle of iterations, making the code compute bound rather than memory bound. In the preliminary tests, the OpenACC code already accelerates with respect to the OpenMP version and we aim to obtain a speedup equal to 2. Further optimizations, that involve the asynchronous work of the GPU and CPU regions of the code, are in progress to obtain higher gains. The code runs on multiple GPUs, one per MPI process, and it was tested on a full node of the CINECA supercomputer Marconi100, with 4 V100 GPUs having 16 GB of memory each, in perspective of a porting on the pre-exascale system Leonardo, that will be installed at CINECA in 2022. In the next month, we are going to work on the porting of the code with CUDA."
O3-002,William O'Mullane,Vera C. Rubin Observatory,Rubin Science Platform on Google: the story so far.,"In 2020 Rubin Observatory made a tender for a three year interim data facility. This was won by Google. In partnership with Google we have now deployed the first externally used version of the Rubin Science Platform consisting of a jupyter environment and a portal. Sitting behind this is TAP access to our project developed large scale database Qserv holding a synthetic catalogue of 140 million objects and around 500TB of simulated images. This is hosted on Google Cloud Platform using Terraform and Kubernetes. The initial few hundred users were given access to this in July 2021. In this talk we will describe the system, the initial use and the coming years."
X0-002,Vicente Navarro,ESA,ESA Datalabs 2.0: Boosting Open Collaboration in ESA Space Science,"At the European Space Astronomy Centre (ESAC) near Madrid, the ESAC Science Data Centre (ESDC) hosts astronomy, planetary and heliophysics archives for ESA Space Science Missions. Also at ESAC but in the GNSS arena, the GNSS Science Support Centre (GSSC) has consolidated a reference archive in order to promote the utilization of GNSS infrastructures as an innovation catapult for a diversity of science fields with special attention to Galileo & EGNOS systems. Motivated by the increasingly challenging requirements for effective exploitation of these archives, ESA Datalabs provides an open, collaborative platform to discover and analyse data from multiple ESA domains. The platform offers “Labs” and “Pipelines” Catalogues to enable seamless access to computing capabilities co-located with ESAC archives. On one hand, the Labs Catalogue allows users to choose among a list of interactive data analysis systems; these range from specific desktop applications like Topcat, a well-known astronomical tool to manipulate and visualize tables, to general-purpose, web-based ones such as JupyterLab, widely adopted for notebook development in multiple science fields. On the other hand, the Pipelines Catalogue allows users to launch complex, batch analysis workflows for long-standing data processing. 

Building on the flexibility provided by Containers and Kubernetes clusters, ESA Datalabs abstracts users from its software complexity, designed from the ground-up as a science-centric platform. ESA Datalabs maximises end-user productivity, eliminating the download, installation and configuration of complex software environments, which are now at just one-click distance from execution. Additionally, visual editors support developers of “Labs”and “Pipelines”, guiding them through their creation and update process to populate and maintain ESA Datalabs catalogues.

Currently available as an internal demonstrator for approximately one hundred ESA internal users at datalabs.esa.int, ESA Datalabs joins the growing number of science exploitation platforms, realising ESA Science vision of a one-stop-shop to implement the “bring the code to the data” paradigm. To this purpose, ESA Datalabs catalogues already feature concept Labs and Pipelines from ESA Space Missions such as Bepi Colombo, Planck, Euclid, Juice, Gaia and JWST."
X2-001,Julien Plante,Observatoire de Paris / PSL,A novel FRB detection pipeline for NenuFAR,"NenuFAR is a very large low-frequency radiotelescope, designed to observe the sky at frequencies ranging from 10 to 85 MHz. One of the main science case is the study of pulsars and Fast Radio Bursts (FRBs), and this program already has detected hundreds of these objects and events. However, the current detection pipeline is not able to perform a 24/7 survey of the sky. This is not an issue for observation of recurring events such as pulsars, but limits the number of FRB detections, and discovery opportunities.

Traditionally, FRBs have been detected offline, because of the heavy processing (Radio Frequency Interference (RFI) mitigation, dedispersion) needed, and of the fine tuning of the detection parameters and algorithms. With NenuFAR, this approach requires recording signals at data rates ranging from 0.7 to 1.5 Gb/s, depending on the precision mode. Storing this signal fast enough and continuously becomes limiting and alternative solutions have to be explored.

Here, we propose a novel real-time FRB detection system for NenuFAR, including a custom hardware platform and a software solution, designed to detect transient events on-the-fly and trigger signal storage on event detection. This pipeline aims at providing low-latency and low-jitter using GPU computing, enabling online processing and thus overcoming the storage bottleneck. To achieve high performances, we leverage the COSMIC real-time heterogeneous computing framework, developed at Observatoire de Paris as well as GPUDirect with DPDK. This experiment on NenuFAR has also been designed as a pathfinder for more efficient transient events detection on SKA, with scalability as one of the core specifications. 

In this paper, we present the design of the experiment, detail the underlying hardware and software technologies and discuss initial results from a benchmarking campaign."
I2-001,Eric J. Murphy,National Radio Astronomy Observatory,A Next-Generation Very Large Array,"The next-generation Very Large Array (ngVLA) is being designed as a versatile interferometric array envisaged to operate as a facility of the U.S. National Science Foundation (NSF), starting in the 2030s. Building on the superb cm/mm observing conditions and existing infrastructure of the VLA site, it will deliver an order of magnitude improvement in both sensitivity and angular resolution compared to existing and planned facilities at frequencies spanning 1.2 - 116 GHz. The ngVLA will also greatly expand current U.S. VLBI capabilities by both replacing existing VLBA antennas/infrastructure with ngVLA technology and adding additional stations on 1000 km baselines to bridge the gap between baselines across the U.S. Southwest and present VLBA baselines. The ngVLA will be optimized for observations in the spectral region between the superb performance of ALMA at sub-mm wavelengths, and the future Phase I Square Kilometer Array (SKA-1) at decimeter and longer wavelengths, thus lending itself to be highly complementary with these facilities and act as a final piece in a global suite of transformational radio capabilities to be utilized by the entire astronomical community. Consequently, the ngVLA will tackle a vast range of key, outstanding questions in modern astrophysics by simultaneously delivering the capability to: unveil the formation of Solar System analogs on terrestrial scales; probe the initial conditions for planetary systems and life with astrochemistry; chart the assembly, structure, and evolution of galaxies from the first billion years to the present; use pulsars in the Galactic Center as fundamental tests of gravity; and understand the formation and evolution of stellar and supermassive black holes in the era of multi-messenger astronomy. Being highly synergistic with its contemporary facilities in space, on the ground, or underground, the ngVLA will maximize the scientific returns on additional investments made by funding agencies in the U.S. and abroad. In this talk we provide an overview of the broad ngVLA science case, an update on the project status, and discuss its complementarity with other instruments and facilities anticipated in the coming two decades."
O0-003,Austin Shen,CSIRO,A globally distributed and scalable data post-processing framework for WALLABY science,"WALLABY is the ASKAP all-sky HI survey, the post-processing for which involves mosaicking of spectral-line data-cubes, source finding, cross-matching, computing moment maps for the detected galaxies, kinematics and ad-hoc interactions with the data. The aforementioned processes are provided by a number of collaborating institutions, which are distributed internationally and utilising different computing facilities. Over the course of the survey, these institutions will process data on the scale of petabytes. The existing post-processing approach is mostly manual and labour-intensive, and has led to unnecessary logistical effort by scientists. As such, new framework is required to efficiently process large ASKAP data across international borders for the full survey.

We have developed a new framework for distributed and scalable WALLABY data post-processing. The technology stack includes PostgreSQL for relational databases, replicated across institutions with Bucardo, to provide a central location for WALLABY survey data products. Interactive web interfaces such as Django admin portals, Jupyter notebooks and VO services provide user access to the data. Computational pipelines, composed in Nextflow, allow HPC resource agnostic and parallel execution of containerised applications. In this discussion we provide an overview of the framework, system architecture, and how it will be utilised to help WALLABY scientists process ASKAP spectral-line data."
B7-001,Paul Barrett,The George Washington University,Julia in Astronomy,"Twenty-five years ago at the ADASS VI meeting in Charlottesville, VA, a BoF entitled ""Interactive Data Analysis Environments"" set in motion the development and use of Python in Astronomy. Since then, many new interpreted, or interactive, programming languages have come on the scene. One of those languages is Julia, which was designed for high performance scientific computing. It can be considered the successor to Scientific Python. This BoF is about Julia in Astronomy and has the following goals: 1) to give a brief overview of the language's features and performance, 2) to summarize the current state of the JuliaAstro packages, and 3) to identify what JuliaAstro packages should be developed in the future, both in the near and long term."
X9-006,Paul Barrett,The George Washington University,Visfit: a Julia programming language package for fitting radio interferometry visibility data,"An alternative to using the CLEAN algorithm to measure the flux density of sources in radio interferometry images is to fit the visibility (or UV) data directly using a parametrized model, such as a
delta or gaussian function. Several modules or packages currently exist for doing this fitting. However, each package has its limitations, e.g., a limited number of shapes, a limited number of
sources, a single Stokes parameter per source, or no frequency dependence. Because of these various limitations, a new package was developed called Visfit using the Julia programming language. Visfit
is fast (\~10 seconds per solution), flexible (no model limitations), and sensitive (~20% more than CLEAN). Part of its flexibility is due to Julia's piping operator which decouples the source function from
the Stokes parameters. Visfit is currently limited to reading Measurement Set (MS) data using the CASA Tables package."
X4-001,Tamara Civera,Centro de Estudios de Física del Cosmos de Aragón (CEFCA),CEFCA Catalogues Portal towards FAIR principles,"The Centro de Estudios de Física del Cosmos de Aragón (CEFCA) is carrying out from the Observatorio Astrofísico de Javalambre (OAJ, Teruel, Spain) two large area multiband photometric sky surveys, J-PLUS and J-PAS, covering the entire optical spectrum using narrow and broad band filters. These large projects require an easy and agile access to data, data interoperability and reusability as well as ensure its discovery (FAIR principles) to maximize research efficiency.
This contribution summarizes the work done in the CEFCA Catalogues Portal to enhance data publication of these large surveys following FAIR principles through Virtual Observatory (VO) services. It presents the implementation and publishing of a VO harvest registry to make data findable; the validation and improving process of all our VO services like TAP, SIAP, SCS, HiPS to be more accessible, interoperable and reusable; the effort made to offer data to improve provenance information and reusability; and all lessons learned."
O0-001,Mohammad Akhlaghi,"Centro de Estudios de Física del Cosmos de Aragón (CEFCA), Plaza San Juan, 1, E-44001 Teruel, Spain",Maneage: Managing data lineage for archivable reproducibility,"The increasing volume, diversity, and role of data and software in modern research has been very fruitful. However, these same factors, have also made it harder to describe or archive (in sufficient detail) the processing behind a scientific result within the confines of a traditional paper/report. It is thus becoming harder and harder to reproduce results (i.e., critically review by coauthors, referees or larger community) that define scientific progress. In this talk, Maneage (MANaging data linEAGE) is introduced as a working solution to this problem. Maneage (a template, in the form of a Git branch) that provides a framework to exactly reproduce a scientific analysis (from the input data and software, to the processing and creation of final report/paper/dataset. It contains instructions to download and build the necessary software (from the low-level C compiler and shell, to the higher-level science programs and all their dependencies) with the predefined configuration and without third-party package managers. It also includes instructions to run the software on the input data sets to produce the final result (all as plain-text files). Maneage will finally produce a ""dynamic"" PDF using LaTeX macros: any change in the analysis will automatically update the relevant parts of the PDF (for example numbers, tables or figures). A project defined in this template is fully managed and published in plain text and only consumes a few hundred kilo-bytes (unlike binary multi-gigabyte blobs like containers). It is thus easily to publish and archive for future usage, for example on arXiv (with the paper's LaTeX source) or Software Heritage. Maneage was a recepient of an RDA-Europe adoption grant for best adoption of the ""Workflows for Research Data Publishing"" guidelines. For more, please see https://doi.org/10.1109/MCSE.2021.3072860, or its main webpage: https://maneage.org"
T3-001,Mohammad Akhlaghi,"Centro de Estudios de Física del Cosmos de Aragón (CEFCA), Plaza San Juan, 1, E-44001 Teruel, Spain",Gnuastro hands-on tutorial for astronomical data analysis,"Gnuastro is an official GNU package of a large collection of programs to enable easy, robust, and most importantly fast and efficient, data analysis directly on the command-line. For example it can perform arithmetic operations on image pixels or table columns/rows, visualize FITS images as JPG or PDF, convolve an image with a given kernel or matching of kernels, perform cosmological calculations, crop parts of large images (possibly in multiple files), manipulate FITS extensions and keywords, and perform statistical operations. In addition, it contains
programs to make catalogs from detection maps, add noise, make mock profiles with a variety of radial functions using monte-carlo integration for their centers, match catalogs, and detect objects in an image among many other operations. Gnuastro is written to comply fully with the GNU coding standards and integrates well with all Unix-like operating systems. This enables astronomers to expect a fully familiar experience in the building, installing and command-line user interaction that they have seen in all the other GNU software that they use (core components in many Unix-like operating systems). Gnuastro's extensive library is also installed for users who want to build their own unique/custom programs."
O2-001,Sankalp Gilda,University of Florida,Uncertainty-Aware Learning for Improvements in Image Quality of the Canada-France-Hawaii Telescope,"We leverage state-of-the-art machine learning methods and a decade's worth of archival data from the Canada-France-Hawaii Telescope (CFHT) to predict observatory image quality (IQ) from environmental conditions and observatory operating parameters. Specifically, we develop accurate and interpretable models of the complex dependence between data features and observed IQ for CFHT's wide field camera, MegaCam. Our contributions are several-fold. First, we collect, collate and reprocess several disparate data sets gathered by CFHT scientists. Second, we predict probability distribution functions (PDFs) of IQ, and achieve a mean absolute error of ∼0.07′′ for the predicted medians. Third, we explore data-driven actuation of the 12 dome ``vents'', installed in 2013-14 to accelerate the flushing of hot air from the dome. We leverage epistemic and aleatoric uncertainties in conjunction with probabilistic generative modeling to identify candidate vent adjustments that are in-distribution (ID) and, for the optimal configuration for each ID sample, we predict the reduction in required observing time to achieve a fixed SNR. On average, the reduction is ∼15%. Finally, we rank sensor data features by Shapley values to identify the most predictive variables for each observation. Our long-term goal is to construct reliable and real-time models that can forecast optimal observatory operating parameters for optimization of IQ. Such forecasts can then be fed into scheduling protocols and predictive maintenance routines. We anticipate that such approaches will become standard in automating observatory operations and maintenance by the time CFHT's successor, the Maunakea Spectroscopic Explorer (MSE), is installed in the next decade."
D3-004,Peter K. G. Williams,Center for Astrophysics | Harvard & Smithsonian and American Astronomical Society,Interactively Visualizing Massive Images and Catalogs in Jupyter with AAS WorldWide Telescope,"Modern astronomical datasets are, of course, bigger than ever. Not only are they often too big to fit in most computers’ memories, more and more frequently they’re too big to even download at all. While the astronomical community has converged on a big-picture approach to this challenge — the web-based “science platform” concept — numerous specific engineering problems still need to be solved before astronomers will be fully equipped to handle the upcoming data deluge. In particular, the switch to browser-based UIs creates a need — and an opportunity — for web-native, interactive data visualization tools. This Focus Demo will showcase new capabilities that have recently been added to AAS WorldWide Telescope that empower users to interactively explore catalogs with billions of rows (using the HiPS progressive standard) and imagery with billions of pixels (using HiPS and tiled FITS formats). These features are tightly integrated with JupyterLab to provide one-click startup and dead-simple linkage with Python notebooks for data analysis. WWT’s efficient WebGL-based rendering is paired with a suite of user-friendly data-processing tools that make it easy to prepare data for visualization."
I0-002,Rachel Street,Las Cumbres Observatory,The TOM Toolkit: Power tools to enhance science and observations,"Astrophysics is being increasingly driven by science derived from alerts of 
new discoveries made by large scale surveys, issued in near real-time and 
at unprecedented data rates and volume. Much of the scientific goals of these
surveys depends on our ability to characterize these initial discoveries, 
collating information from many surveys and data archives but also demanding 
additional follow-up observations, often within hours of discovery and 
continuing for months afterwards. 
The workload of these follow-up programs is untenable without software tools to collate
data and interface programmatically with essential services in astronomy, 
including alert brokers, data archives and telescope facilities. 
Many projects have developed Target and Observation Manager Systems (TOMs) to 
fulfil this purpose, combining a flexible, searchable database of all project 
information, with an observation and data analysis control system, and 
communication and data visualization tools. I will describe the TOM Toolkit, 
an open-source package designed to make it easy for any astronomer to 
build and customize a TOM system for their science needs, including recent 
enhancements to the package and future directions for its development."
X7-001,Vito Conforti,INAF,The Data Acquisition System assessment to support the observation quality system of the ASTRI Mini-Array Project,"The ASTRI (Astrofisica con Specchi a Tecnologia Replicante Italiana) Project
was born as a collaborative international effort led by the Italian National Institute
for Astrophysics (INAF) to design and realize an end-to-end prototype of the Small-Sized Telescope (SST) ofthe Cherenkov Telescope Array (CTA) in a dual-mirror configuration (2M). The prototype, named ASTRI-Horn, is operative since 2014 at the INAF observing station located on Mt.Etna (Italy). The ASTRI Project is currently aimed to build a Mini-Array of nine ASTRI-Horn-like telescopes to be installed and operated at the Teide Observatory (Spain). The ASTRI software supports
the operations of the ASTRI-Horn prototype and, eventually, of the Mini-Array. The
Array Data Acquisition System (ADAS) of the ASTRI-Horn telescope acquires, packet by packet, the
read-out data from the back-end electronics of the ASTRI camera. The packets are then
stored locally in one raw binary file as soon as they arrive. During the acquisition, the
packets are grouped by data type (scientific, calibration, engineering) before processing
and storing the decoded data in FITS format. A quick-look component, running on the
same machine was also implemented, allowing the operator to display the decoded data during the acquisition,
was also implemented. During software testing and operations performed with the
ASTRI-Horn prototype, due to a cfitsio writing method, we experienced a bottleneck
in the raw-to-FITS binary data conversion when the acquisition rate was greater than
about 10 Megabytes per second. Thus, we decided to apply a workaround and postpone
the conversion in FITS format of the ASTRI-Horn raw data after the end of the
acquisition run. The ASTRI Mini-Array software requires monitoring quality of the data 
as soon as data are available and this evaluation is performed by the Online Observation
Quality System (OOQS). Because of this fact and to the fact that the Mini-Array consists of
9 ASTRI cameras observing in parallel, we are evaluating a solution to directly send
the decoded data from the ADAS to the OOQS, strictly reducing any
bottleneck. We studied a Redis based solution to improve the data transfer efficiency from the
data acquisition system to the OOQS component of the ASTRI Mini-
Array. In this contribution we present the results of the comparison between two approaches: the storage in the Redis database, and the Redis asynchronous publish/subscribe paradigm. In addition, we are also assessing different data formats to obtain better performance to decode and transfer data from the ADAS to the OOQS."
X9-001,Valerio Pastore,Italian National Institute for Astrophysics (INAF),The BIAS (Beck-end Independent Acquisition Subsystem) to upgrade the CIWS framework.,"TITLE:
The BIAS (Beck-end Independent Acquisition Subsystem) to upgrade the CIWS framework. 

Author list:
Valerio Pastore, Vito Conforti, Fulvio Gianotti, Massimo Trifoglio.

ABSTRACT
While existing telescopes continue producing data and scientific results, bigger telescopes and larger and more efficient detectors both from the ground and from space are becoming available to support the incoming projects and missions in the astrophysics field. We are getting into an era in which new-generation instruments will be producing terabytes of data a day. Therefore, we are confronted with non-trivial problems to solve in terms of acquiring these data, storing them so that they do not get lost, and processing them to enrich our knowledge in astrophysics. 

The Italian National Institute for Astrophysics (INAF) developed the CIWS (Customizable Instrument Workstation System) framework which provides a common and standard solution for acquiring, storing, processing, and quick-look the data acquired from scientific instruments for astrophysics. It has been used in many programs such as ASTRI and Euclid.
The main purpose of the BIAS project is to replace the DISCoS (Detector Independent Science Console Subsystem) component of the CIWS framework. The DISCoS software consists of the C programs to acquire, verify and archive in near real-time the telemetry and telecommand packets (ESA Packet Utilization Standard) in one set of files for each measurement setup.
The C language was selected to obtain excellent performance and properly manage the DISCoS component programs. Nevertheless, this language does not apply the modern software engineering paradigms (e.g. object-oriented). Furthermore, it is also onerous to customize the programs to support future projects.
In addition, some criticalities have been detected when DISCoS is commanded to switch the mode (run to idle and vice versa) which implies closing the current file and opening the new one. 

The BIAS project aims to overcome these limits without affecting the excellent performance guaranteed by a low-level language such as C.
The requirements of the BIAS subsystem are data type and format configurability, and software modularity, maintainability and extensibility to support future astrophysical projects. 
The BIAS software is under development in the C++ language and the preliminary version provides the acquisition via TCP/UDP protocol, packet checking and storage. 

We present here the results of the comparison between the BIAS and DISCoS software testing their performance in different use scenarios."
X0-003,Hugo Buddelmeijer,Kapteyn Astronomical Institute,MicadoWISE: Integrate Data Simulation with Processing Validation,"We discuss how we validate the data processing software for the MICADO instrument on the ELT by integrating the processing with the data simulator through the consortium information system MicadoWISE. The Multi-AO Imaging Camera for Deep Observations (MICADO) is a first light instrument at the Extremely Large Telescope (ELT). The design of the instrument hardware and the design of the data processing software is more intertwined than for earlier ESO instruments in order to reach the scientific limits of the instrument. MicadoWISE is an information system for MICADO that facilitates not only the data processing itself but also the design, development and testing of the processing software.

MicadoWISE integrates several components to facilitate the development of data processing software:

- A full model of the detector, instrument, telescope and atmosphere through an Instrument Reference Database,
- The data simulator ScopeSIM + data archive,
- The processing software both as Python prototypes and final C code ('recipes'),
- A representation of the data processing requirements for validation of the recipes.


This directly translates to a four step validation process for the data reduction recipes:

- Prepare simulations for different observations, both as raw data and perfectly processed data,
- Simulate the data on-the-fly or retrieve them from the archive,
- Process the data with the latest software, either on the developer workstation, or nightly in batch mode,
- Validate the software by automatic comparison of the processed simulated data to the 'true' data.

Storing and processing the simulated data in a central archive eases several development processes:

- Regression tests can automatically determine how well the requirements are met over time,
- Observations can be simulated with progressively worse edge-cases to discover the limits of the algorithms,
- Instrument effects can either be enhanced or diminished to disentangle degeneracies,
- The same environment can be used for simulated data, commissioning data and science data.

Together, MicadoWISE can deliver an effective and efficient way to develop data processing software for MICADO."
X0-004,David Keiderling,Max Planck Institute for Solar System Research,Distributed Development of the Level 1 Data Processing for Science Ground Segment for the ESA PLATO Mission,"The ESA mission PLATO is an exoplanet and stellar science mission, evaluating the photometry of stars with the main goal of finding earth-like planets around sun-like stars. The science ground segment is responsible for processing the photometric data, consisting mainly of light curves, centroid curves and imagettes in order to detect planets and characterize their host stars. The main goal for the Level 1 pipeline is to remove instrumental and systematic effects in a performant, reliable and reproducible way for a large number of target stars. 
This requires a distributed development approach which combines a wide range of expertise within the PLATO Science Ground Segment. In this poster, we introduce some key aspects of this approach, as well as the planned processing for the Level 1 data products to create science ready products for all downstream processing."
X0-004,Cilia Damiani,Max Planck Insitute for Solar System Research,Distributed Development of the Level 1 Data Processing for Science Ground Segment for the ESA PLATO Mission,"The ESA mission PLATO is an exoplanet and stellar science mission, evaluating the photometry of stars with the main goal of finding earth-like planets around sun-like stars. The science ground segment is responsible for processing the photometric data, consisting mainly of light curves, centroid curves and imagettes in order to detect planets and characterize their host stars. The main goal for the Level 1 pipeline is to remove instrumental and systematic effects in a performant, reliable and reproducible way for a large number of target stars. 
This requires a distributed development approach which combines a wide range of expertise within the PLATO Science Ground Segment. In this poster, we introduce some key aspects of this approach, as well as the planned processing for the Level 1 data products to create science ready products for all downstream processing."
I0-001,Urvashi Rau,National Radio Astronomy Observatory,Quantifying scientific correctness in radio interferometric imaging,"In an exploratory field such as observational astronomy, it is often not trivial to evaluate the degree of absolute correctness of a scientific result. It is therefore important to establish a high degree of trust in the algorithms and software that are used to produce that result. Scientific software developers and the people who use their product must be creative in using a combination of simulations, controlled tests and predictions of behavior in new situations in order to assess the operational and numerical readiness of the tools. When available, accuracy metrics derived from science goals are of immense use, but for a variety of practical reasons they are not always an option. 

This talk will discuss radio interferometric data analysis and image reconstruction as a case study to illustrate the practical complexities involved in establishing absolute correctness of scientific software used for radio astronomy. Technical, operational and sociological perspectives will be considered along with lessons learnt and strategies currently being explored towards building the requisite amount of trust in the software product. The ideas of reconstruction uncertainty, measurement noise and error propagation in the context of an entire data analysis sequence will be described alongside some key points (or challenges) when designing, producing and testing algorithms and software that will be run on a variety of operational platforms and be used on data for which the absolute truth is always unknown."
O2-002,Stephen Potter,South African Astronomical Observatory and University of Johannesburg,The Intelligent Observatory (South African Astronomical Observatory),"In the era of multi-wavelength and multi-messenger astronomy there is a wealth of global and space based facilities that are providing unprecedented opportunities to explore transient phenomena and time domain astronomy. 
To take full advantage of these opportunities requires upgrading and adapting of observatory operations and hardware. Hardware (instruments and telescopes) needs to be made as fully automated as possible, operations needs to be adapted in order to accommodate a variety of observing modes including responding to alerts, automated queues, remote and manual observing. All of this then needs to be “intelligently” managed and coordinated. The Intelligent Observatory (IO) is SAAO's (South African Astronomical Observatory) key programme to upgrade its optical/IR facilities and operations to take full advantage of these opportunities."
O3-007,Nuria Alvarez Crespo,European Space Agency,ESASky: a visualisation tool for multi-messenger astronomy,"Six years after the discovery of the first gravitational wave (GW) event, today astronomers observe merging black holes and neutron stars routinely, and this topic will become key to understand astrophysical phenomena during the next decade. One of the current challenges of multi-messenger astronomy is to link the events with their electromagnetic counterparts, since GW localisation usually involves a big sky area, from tens up to hundreds of square degrees. Archival data and catalogs are fundamental instruments to understand this rapidly evolving field, since their study helps to narrow-down the sky regions to be observed, discard already known transient sources and re-analyse data to look for missed multi-messenger events similar to the already detected ones.
ESASky is a science-driven discovery portal than allows scientists to explore interactively a large collection of astronomical data, providing a very useful tool to explore the multiwavelength sky with the click of a button. ESASky now shows the GW probability map on the sky, allowing users to look for electromagnetic counterparts for the GW events and using ESASky to quickly access all available archival electromagnetic data. In the future, we will also include the IceCube neutrino footprint."
X0-005,Marcos López-Caniego,Aurora Technology B. V. for the European Space Agency,Exploring and analysing data from the Webb Space Telescope in ESA Datalabs,"ESA Datalabs is the new science exploitation platform developed by the Data Science and Archives Division at the European Space Astronomy Centre (ESAC). This new platform sits next to ESA's astronomy archives and will allow scientists from anywhere in the world to access, explore and analyse large datasets containing the products of ESA and partner missions, without requiring large-scale data downloads while also providing on-line processing, analysis and collaborative research facilities using archive products. The users of ESA Datalabs will benefit from the latest computing infrastructure, technologies and software available pre-installed on the platform, allowing a focus on science rather than system setup. During this demo we will present the new Webb Space Telescope area that uses the ESA Datalabs capabilities for the first time. We show some example use cases of how to explore and analyse data from the European JWST archive at ESAC via Jupyter Labs, Jupyter Notebooks, astroquery and pyesasky."
I4-001,Simon O'Toole,AAO Macquarie,FAIR standards for astronomical data,"I present an overview of the ""FAIR Guiding Principles for scientific data management and stewardship"", first published in 2016, and how they relate to astronomical data management. In particular, I will discuss the connection between the FAIR principles and IVOA standards, and how data management systems with these standards implemented are close to compliance. I will then look at what extra components are required to make astronomical data FAIR. Finally, I will give a case study of the All-Sky Virtual Observatory (Australia's node of the VO) and their implementation of the FAIR principles."
X3-001,Patrick Reichherzer,Ruhr-Universität Bochum,Astro-COLIBRI: novel platform for real-time multimessenger astrophysics,"Astro-COLIBRI is a novel tool that evaluates alerts of transient observations in real time, filters them by user-specified criteria, and puts them into their multiwavelength and multimessenger context. Through fast generation of an overview of persistent sources as well as transient events in the relevant phase space, Astro-COLIBRI contributes to an enhanced discovery potential of both serendipitous and follow-up observations of the transient sky.

We present the software's architecture that comprises a REST API, both a static and a real-time database, a cloud-based alert system, as well as a website and apps for iOS and Android as clients for users. The latter provide a graphical representation with a summary of the relevant data to allow for the fast identification of interesting phenomena along with an assessment of observing conditions at a large selection of observatories around the world. We demonstrate the benefits and advantages of the tool in a live demo."
O1-002,Michelle Ntampaka,Space Telescope Science Institute,Building Trustworthy Machine Learning Models for Astronomy,"Astronomy is entering an era of data-driven science, due in part to modern machine learning techniques enabling powerful new ways to analyze data. This is a shift in our scientific approach, and requires us to ask an important question: Can we trust the black box? In this talk, I will overview methods for benchmarking and assessing our algorithms. This is an essential step not just for creating more robust data analysis techniques, but also for building confidence within the astronomy community to trust machine learning methods and results."
O1-001,Felix Grezes,The SAO/NASA Astrophysics Data System,"Building astroBERT, a language model for Astronomy & Astrophysics.","The existing search tools for exploring the NASA Astrophysics Data System (ADS) can be quite rich and empowering (e.g., similar and trending operators), but researchers are not yet allowed to fully leverage semantic search. 
For example, a query for ""results from the Planck mission"" should be able to distinguish between all the various meanings of Planck (person, mission, constant, institutions and more) without further clarification from the user.
At ADS, we are applying modern machine learning and natural language processing techniques to our dataset of recent astronomy publications to train astroBERT, a deeply contextual language model based on research at Google.
Using astroBERT, we aim to enrich the ADS dataset and improve its discoverability, and in particular we are developing our own named entity recognition tool. We present here our preliminary results and lessons learned."
X1-001,Jakub Juryšek,"Univeristy of Geneva, Astronomy Department",Full LST-1 data reconstruction with the use of convolutional neural networks,"The Cherenkov Telescope Array (CTA) will be the world's largest and the most sensitive ground-based gamma-ray observatory in the energy range from a few tens of GeV to tens of TeV. To achieve the best performance in such a wide range of primary gamma-ray photon energies, the whole array will consist of Imaging Atmospheric Cherenkov Telescopes of three different sizes. The Large-Sized Telescopes (LSTs) are designed to be the most sensitive in the low-energy band of CTA. The LST-1 prototype, currently in the commissioning phase, was inaugurated in October 2018 in La Palma (Spain) and it is the first of the four largest CTA telescopes, that will be built in the northern site of CTA.

The Random Forest method is currently employed in the reconstruction of the first data from the LST-1. This method, however, requires extensive preprocessing that includes signal integration and image parametrization, leading to the loss of information contained in the shower images. In this contribution, we will present a full-image reconstruction method using Inception deep convolutional neural network (CNN) applied on non-parametrized shower images. First, we will evaluate the performance of optimized networks on Monte Carlo simulations of LST-1 shower images, and compare the results with the performance of the standard reconstruction method. We will also show how both methods compare to real-data reconstruction."
O0-007,Julia Healy,ASTRON,Managing the data flow of the MHONGOOSE survey with open-access tools,"One of the challenges facing the SKA Precursor/Pathfinder Large Survey teams, is how to document and keep track of the data reduction process of the many observation tracks that make up the surveys. I will present an overview of the workflow used by the MHONGOOSE team to manage our data flow from submission of observation scripts to the assessment of the data reduction quality and creation of the science data products. We make use of the GitHub project management tools which allow us to link the status updates of each observation to the CARAcal configuration scripts used for the data calibration and imaging, and the reports created to assess the quality of the data. The reports, created by scripts developed by members of the MeerKAT Fornax Survey team, are an essential part of recording the data reduction process and assessing the quality of the data at each step in the pipeline. We have automated the uploading of the reports to GitHub as issues which makes it easy to monitor and track the progress of the data reduction."
O0-006,Bernie Boscoe,Occidental College,Three workflow add-ons to improve machine learning reproducibility in astronomy,"As machine learning increasingly becomes adopted into astronomy research practices, additional workflows must be added to preserve reproducible and replicable results. However, as machine learning tools are often drawn from industry or computer science domains, how these imports can segue into traditional astronomy computational practices and enhance results is not well understood. In this talk, I start from the position that machine learning models rarely can be rerun after a year’s time, let alone after a longer time span. I ask the question: How can scientific results be verified, compared to, or built upon if the artefacts necessary to examine the process to obtain results have not been saved? With this in mind, I suggest three straightforward additions to typical machine learning workflows to improve reproducibility: (1) Fixing the training data used to create the model, (2) Covering best practices in saving the model, including parameters and hyperparameters used in the final tuning, and (3) Describing additional artefacts in model evaluations that enable a further understanding of outputs that the model produced. I will conclude with steps enacted in our team’s research project using machine learning to predict galactic redshifts, and present lessons learned in our experience reproducing models using machine learning."
X1-002,Hareesh Thuruthipilly,"National Center for Nuclear Research, Poland",Self-attention based encoder models for strong lens detection,"The upcoming large scale surveys are expected to find approximately 10^5 strong gravitational systems by analyzing data of many orders of magnitude than the current era. In this scenario, non-automated techniques will be highly challenging and time-consuming. We propose a new automated architecture based on the principle of self-attention to find strong gravitational lensing. The advantages of self-attention-based encoder models over convolution neural networks to find strong lenses from the Bologna Lens Challenge is investigated. The encoder models performed better than the CNNs and were able to surpass the CNN models that participated in the bologna lens challenge by a high margin for the TPR0 and TPR10. In terms of the AUROC, the encoder models scored equivalent to the top CNN model by only using one-sixth parameters to that of the CNN. A low computational cost and complexity make it a highly competing architecture to currently used methods with CNN."
X1-003,Szymon Nakoneczny,"National Centre for Nuclear Research, Warsaw, Poland","A new catalog of 343,000 quasars with their photometric redshifts derived with machine learning from the Kilo Degree Survey","In the present era of ever larger photometric sky surveys, designing efficient automated methods for unbiased selection of specific categories of objects and prediction of their properties based on photometric data is becoming an increasingly urgent quest. A good example of such objects are quasars (QSOs), both because of our interest in their physics and possible cosmological applications.

In my talk I present a catalog of 343,000 QSOs derived from one of the largest photometric surveys up to date - the Kilo-Degree Survey (KiDS) Data Release 4. The catalog is characterised by robust photometric redshifts with uncertainties, and its high magnitude depth was achieved with artificial neural networks (ANN) applied in a controlled way. The catalog is based on optical ugri and near-infrared ZYJHKs bands. With purity 97%, completeness 94%, and redshift error (mean and scatter) 0.009 +/- 0.12, the catalog is limited at r < 23.5. The robustness of the catalog was demonstrated e.g. by comparing the number counts of our QSO candidates to models from eBOSS, and by cross-matching the QSOs with Gaia in order to confirm zero parallaxes. In the talk, I will explain how machine learning (ML) methods can be successfully applied to observations fainter than the training data known from spectroscopy. It includes building inference subsets using high dimensional t-SNE visualisations of the feature space, and properly adapting bias vs variance tradeoff of the ML models with tests against the problem of extrapolation in the feature space. Our success of the extrapolation challenges the way that models are optimised and applied at the faint data end. The resultant catalog is ready for cosmological and active galactic nucleus (AGN) studies, and developed methodology is ready to be used on future yet larger photometric datasets."
X0-006,Tjark Miener,"EMFTEL department and IPARCOS, Universidad Complutense de Madrid, E-28040 Madrid, Spain",Open-source Analysis Tools for multi-instrument Dark Matter Searches,"The nature of dark matter (DM) is still an open question in Physics. Gamma-ray and neutrino telescopes have been searching for DM signatures for several years and no detection has been obtained so far. In their quest, these telescopes have gathered a wealth of observations that, if properly combined and analyzed, can improve on the constraints to the nature of DM set by individual instruments. In this contribution, we present two open-source analysis tools aimed at performing the before mentioned combined analysis: gLike, a general-purpose ROOT-based code framework for the numerical maximization of joint likelihood functions, and LikelihoodCombiner (lklcom), a Python-based tool combining likelihoods from different instruments to produce combined exclusion limits on the DM annihilation cross-section."
X0-007,Tjark Miener,"EMFTEL department and IPARCOS, Universidad Complutense de Madrid, E-28040 Madrid, Spain",IACT event analysis with the MAGIC telescopes using deep convolutional neural networks with CTLearn,"The Major Atmospheric Gamma Imaging Cherenkov (MAGIC) telescope system consists of two Imaging Atmospheric Cherenkov Telescopes (IACTs) and is located on the Canary island of La Palma. IACTs are excellent tools to inspect the very-high-energy (few tens of GeV and above) gamma-ray sky by capturing images of the air showers, originated by the absorption of gamma rays and cosmic rays by the atmosphere, through the detection of Cherenkov photons emitted in the shower. One of the main factors determining the sensitivity of IACTs to gamma-ray sources, in general, is how well reconstructed the properties (type, energy, and incoming direction) of the primary particle triggering the air shower are. We present how deep Convolutional Neural Networks (CNNs) are being explored as a promising method for IACT full-event reconstruction. The performance of the method is evaluated on observational data using the standard MAGIC Analysis and Reconstruction Software, MARS, and CTLearn, a package for IACT event reconstruction through Deep Learning (DL)."
X0-007,Luca Romanato,University of Padua - CTLearn,IACT event analysis with the MAGIC telescopes using deep convolutional neural networks with CTLearn,"The Major Atmospheric Gamma Imaging Cherenkov (MAGIC) telescope system consists of two Imaging Atmospheric Cherenkov Telescopes (IACTs) and is located on the Canary island of La Palma. IACTs are excellent tools to inspect the very-high-energy (few tens of GeV and above) gamma-ray sky by capturing images of the air showers, originated by the absorption of gamma rays and cosmic rays by the atmosphere, through the detection of Cherenkov photons emitted in the shower. One of the main factors determining the sensitivity of IACTs to gamma-ray sources, in general, is how well reconstructed the properties (type, energy, and incoming direction) of the primary particle triggering the air shower are. We present how deep Convolutional Neural Networks (CNNs) are being explored as a promising method for IACT full-event reconstruction. The performance of the method is evaluated on observational data using the standard MAGIC Analysis and Reconstruction Software, MARS, and CTLearn, a package for IACT event reconstruction through Deep Learning (DL)."
X0-007,Sahil Yadav,BITS Pilani,IACT event analysis with the MAGIC telescopes using deep convolutional neural networks with CTLearn,"The Major Atmospheric Gamma Imaging Cherenkov (MAGIC) telescope system consists of two Imaging Atmospheric Cherenkov Telescopes (IACTs) and is located on the Canary island of La Palma. IACTs are excellent tools to inspect the very-high-energy (few tens of GeV and above) gamma-ray sky by capturing images of the air showers, originated by the absorption of gamma rays and cosmic rays by the atmosphere, through the detection of Cherenkov photons emitted in the shower. One of the main factors determining the sensitivity of IACTs to gamma-ray sources, in general, is how well reconstructed the properties (type, energy, and incoming direction) of the primary particle triggering the air shower are. We present how deep Convolutional Neural Networks (CNNs) are being explored as a promising method for IACT full-event reconstruction. The performance of the method is evaluated on observational data using the standard MAGIC Analysis and Reconstruction Software, MARS, and CTLearn, a package for IACT event reconstruction through Deep Learning (DL)."
I8-002,Meg Schwamb,Queen's University Belfast,"Exploring Mars with 150,000 Earthlings","Mars' south pole region is sculpted by the seasonal cycle of freezing and thawing of exposed carbon dioxide (CO2) ice. In the Spring, CO2 jets loft dust and dirt through cracks in the thawing CO2 ice sheet to the surface where it is thought that local surface winds blow the material into the hundreds of thousands of dark fans visible from orbit. By surveying the direction, frequency, and appearance of these dark fans (a proxy for the jets) and exploring how varying factors impact these properties, we can better understand the Martian climate. Over eons, the south polar CO2 jet process also carves channels and pits (dubbed araneiforms) into the surface. It is difficult if not impossible for computer algorithms to accurately identify the hundreds of thousands of individual fans visible within orbital imagery and map the locations of araneiforms in spacecraft observations. But these features are easily spotted by the human eye. Referred to as ‘crowdsourcing’ or ‘citizen science’, the combined assessment of many non-expert human classifiers with minimal training can equal or best that of a trained expert and in many cases outperform the best machine-learning algorithms.

Planet Four (http://www.planetfour.org) and Planet Four: Terrains (http://terrains.planetfour.org) are citizen science projects mining Mars Reconnaissance Orbiter (MRO) images to explore the seasonal processes on Mars’ polar regions. Planet Four enlists over 136,000 volunteers to map the sizes, shapes, and orientations of these fans in high resolution images. Planet Four is creating an unprecedented wind map of the south pole of Mars in order to probe how the Martian climate changes over time and is impacted year to year by dust storms and other global-scale events. Planet Four: Terrains, aims to study the distribution of the jet process across the south pole and identify new targets of interest for MRO. Over 12,000 people have helped identify araneiforms carved during the CO2 jet formation process. 

In this talk, I'll give an overview of Planet Four and Planet Four: Terrains and present the latest results from these projects from combining the multiple volunteer assessments together. In particular, I will present Planet Four fan directions, examine some of the inter-annual variability of the fans over the Martian south polar region, and compare the inferred wind directions to Martian climate models. I’ll also present preliminary results from training a neural network with the Planet Four volunteer assessments."
O1-007,Jennifer Medina,Space Telescope Science Institute,WFC3 IR Blob Classification with Machine Learning,"IR blobs are small, circular, dark artifacts in the Hubble Space Telescope's WFC3 IR images caused by particulates that are occasionally deposited onto a flat mirror that is nearly optically conjugate to the IR detector. Machine learning can potentially reduce the effort currently devoted to visually inspecting blobs. We describe how machine learning (ML) techniques have been implemented to develop software that will automatically find new IR blobs and notify the WFC3 Quicklook team. This report describes the data preparation, development of the ML model, and criteria for success. The results of our latest test cases demonstrate that the model finds blobs reliably, with the model correctly classifying blob and non-blob images 94% and 88% of the time, respectively. We also report tips and lessons learned from our experience in machine learning as a result of this project."
X0-007,Ettore Mariotti,,IACT event analysis with the MAGIC telescopes using deep convolutional neural networks with CTLearn,"The Major Atmospheric Gamma Imaging Cherenkov (MAGIC) telescope system consists of two Imaging Atmospheric Cherenkov Telescopes (IACTs) and is located on the Canary island of La Palma. IACTs are excellent tools to inspect the very-high-energy (few tens of GeV and above) gamma-ray sky by capturing images of the air showers, originated by the absorption of gamma rays and cosmic rays by the atmosphere, through the detection of Cherenkov photons emitted in the shower. One of the main factors determining the sensitivity of IACTs to gamma-ray sources, in general, is how well reconstructed the properties (type, energy, and incoming direction) of the primary particle triggering the air shower are. We present how deep Convolutional Neural Networks (CNNs) are being explored as a promising method for IACT full-event reconstruction. The performance of the method is evaluated on observational data using the standard MAGIC Analysis and Reconstruction Software, MARS, and CTLearn, a package for IACT event reconstruction through Deep Learning (DL)."
X7-002,Riccardo Smareglia,INAF,Big Data Challenge: a proposal for an active e-infra at INAF,"The presence of a robust and up-to-date IT infrastructure is one of the most important keys to face the new scientific challenges in Astrophysics. The ”BIG DATA” paradigm is affecting the majority of the modern, large-scale projects, in which often the Italian astrophysical community has an important involvement.
For these reasons, it is important for INAF to develop an e-infrastructure, in the broadest sense of the term, that best integrates the existing infrastructures (such as IA2 and CHIPP), the capabilities resulting from MoUs with Supercomputing centers (eg CINECA), collaborations with Commercial Clouds and the future infrastructure/s, such as the INAF Facility data center that will be created at the Bologna Big Data Technopole. 


This contribution will illustrate a proposal for the design of this e-infrastructure (and its development plan in the next years), which we label as DATA-STAR.

DATA-STAR has been conceived as a National Center for Big Data handling in Astrophysics and Space Sciences, with a core hosted in a centrale pole (e.g. the Bologna Big Data Technopole) and a few satellite infrastructures. Given the strong Italian involvement in the international SKA project, and the INAF commitment to provide one of the SKA Regional Centers (SRC), a possibility would be for DATA-STAR to start providing Archiving and HPC/HTC facilities for the use of the SRC and for some of the other main challenging projects in INAF. The needed network infrastructure will be supported by the GARR Consortium.

The architecture based on a core with satellites is necessary for several factors, both linked to the local development policies in the Italian system and due to the distributed nature throughout the national territory of INAF. Such a system allows a concentration and optimization of the HW part (computing and storage), but also the flexibility to develop on smaller and more easily modifiable infrastructures.

In fact, the available staff in INAF includes people who are already deeply involved in, and very expert of, all the several aspects needed for the creation of a Distributed Data Center (AAI, Archives, Interoperability, HTC, HPC, ...), but these people are distributed as well.

Finally, and above all, the growth of DATA-STAR will lead to the improvement and increase of the so-called human capital. Figures such as Software Engineering, Data Scientist and Data Steward must become an integral part of the technical / scientific activities of the institute."
O0-002,Erik Tollerud,Space Telescope Science Institute,"A Science-Centric, Open approach to Scrum","I will describe a novel approach to software development and teamwork that STScI's Data Analysis Tools Branch has been taking for the last few years, particularly for the Jdaviz visualization tools. This approach's goal is to combine the engineering benefits of Agile software development (specifically Scrum) with the need to engage scientists in the development process and the need to ensure it is Open and reproducible. The approach uses a mixed team of scientists and engineers who take on specific Scrum roles to give everyone a clear understanding of where they fit in, while ensuring the whole team has collective ownership of the work. Additionally, because the team's work is Open Source and has a major UX component, I will describe the both technical and social methods the team uses to try to ensure the development process is reproducible and visible to the outside world (while not being too ""noisy"" and full of internal details not important for the actual software). Finally, I will describe some lessons learned and how this approach might be applied to interested other astronomy software teams."
O7-005,Simon Ratcliffe,Tsolo Storage Systems,Unconstrained storage - petascale at terascale prices.,"This talk will highlight the benefits of technology development as part of the construction of next generation radio telescopes. In particular, we will discuss a new self-managed petascale cloud storage offering, that will provide cross-subsidised storage to the local scientific community in general, and radio astronomy in particular.
The confluence of new techniques, massive data demands, and cutting edge open-source technologies combine to deliver previously unobtainable pricepoints."
O1-007,Frederick Dauphin,Space Telescope Science Institute,WFC3 IR Blob Classification with Machine Learning,"IR blobs are small, circular, dark artifacts in the Hubble Space Telescope's WFC3 IR images caused by particulates that are occasionally deposited onto a flat mirror that is nearly optically conjugate to the IR detector. Machine learning can potentially reduce the effort currently devoted to visually inspecting blobs. We describe how machine learning (ML) techniques have been implemented to develop software that will automatically find new IR blobs and notify the WFC3 Quicklook team. This report describes the data preparation, development of the ML model, and criteria for success. The results of our latest test cases demonstrate that the model finds blobs reliably, with the model correctly classifying blob and non-blob images 94% and 88% of the time, respectively. We also report tips and lessons learned from our experience in machine learning as a result of this project."
X3-002,Mattias Wångblad,Winterway for European Space Agency (ESA),Visualising archives by embedding ESASky,"The European archive for the Webb Space Telescope (eJWST) has 
integrated ESASky in its GUI to provide users an alternative way of exploring the contents of the archive in a more visual way. For example selecting specific instruments and exposure types, zooming in at the location of coverage maps (MOCs) to visualise the footprints of the available observations for a given instrument in a location of the sky. At the same time users of the archive can take advantage of all the ESASky features, combining JWST metadata with that from other missions available in ESASky, an important factor considering that research using multi-wavelength and multi-mission data is rising.

All of this is possible through the updated ESASky API which allows the ESA astronomy archives to embed ESASky, customising
their GUIs to it the needs from the archives to overlay metadata, MOCs (Multi-order coverage maps) and HiPS (Hierarchical Progressive Surveys ), and to control navigation around the sky. Several archives at ESAC are expected to use this API in the coming years and further improvements are foreseen. In this talk we will show how ESASky is embedded in the eJWST archive, illustrating the interactions between ESASky and the ESA archives. In addition, we will show how straightforward it is for a user to use the ESASky API to visualise their own set of HiPS and metadata."
X3-003,Henrik Norman,Winter Way,News from ESASky,"ESASky offers astronomers an easy and interactive way to access high-quality scientific data from gamma rays to radio wavelengths. It is continuously evolving with new features and new data. In this talk I will present the latest additions, which among other things includes lightcurves from the ESA's CHaracterising ExOPlanet Satellite (CHEOPS). This is the first mission dedicated to studying bright, nearby stars that are already known to host exoplanets, in order to make high-precision observations of the planet's size as it passes in front of its host star.

We are also introducing ESASky Legacy, which serves the community a complete, self-standing catalogue collection of the ESA mission Hipparcos, providing a single unified method of accessing all Hipparcos/Tycho data products and their successors. 

Additionally, the ESASky astroquery module now gives you easy access to every single image of solar system objects of your choice. Just write the name of your favourite SSO and watch as you get direct python access to all targeted and serendipitous observations of that object. This is all possible thanks to the crossmatch pipeline developed by the ESASky team, which crossmatches the SSO orbits against the entire mission archives to find the observations where the SSO fell within the imaging instrument's field of view during the time the images were being taken."
O7-008,Nima Sedaghat,ESO,The unvierse speaks for itself: from unsupervised physics to semantic source separation,"Machine learning has been widely applied to clearly defined problems of astronomy and astrophysics. However, deep learning and its conceptual differences to classical machine learning have been largely overlooked in these fields. The broad hypothesis behind our work is that letting the abundant real astrophysical data speak for itself, with minimal supervision and no labels, can reveal interesting patterns which may facilitate discovery of novel physical relationships. 
We train an encoder-decoder architecture on the self-supervised auxiliary task of reconstruction to allow it to learn general representations without bias towards any specific task. By exerting weak disentanglement at the information bottleneck of the network, we implicitly enforce interpretability in the learned features. 
So far we have achieved interesting results in two avenues: firstly, our ""AstroMachines"" have learned to infer physical parameters such as radial velocity and effective temperature, just by watching a large number of stellar spectra and without being asked to do so. Secondly and more recently, we have observed semantic source separation abilities in the same architecture, and have reinforced it to ""randomize out"" telluric lines in stellar spectra, again in a non-supervised fashion."
D1-001,Nima Sedaghat,ESO,"RETR-SPECT: A semantic RETRieval engine for SPECtra, to seek similarities in the deeply encoded space","RETR-SPECT is a semantic retrieval (/search) engine based on self-supervised deep learning. Similarities are sought in the latent feature space of a semi-factorized VAE, as opposed to naive comparison of flux values in the initial raw space."
X0-008,Marco Lam,Tel Aviv University,An update on the RANSAC-assisted Spectral CALibration,"We report on version 0.3.0 of RASCAL (RANSAC-assisted Spectral CALibration). RASCAL is an automated wavelength calibration tool for astronomical spectrographs. The goal of the project is to allow users to calibrate arbitrary spectra with minimal user input, for example with only a reference arc line list and coarse prior information about the instrument. This most recent version of rascal offers significant improvements in calibration stability and user experience. RASCAL has been tested on a variety of instruments officially and unofficially, including SPRAT on the Liverpool Telescope, ISIS on the William Herschel Telescope, IDS on the Issac Newton Telescope, GMOS (long-slit mode) on the Gemini 8.1m Telescope, FLOYDS on the Faulkes Telescope North and South etc. It is also being integrated into the ASPIRED pipeline builder."
O7-001,Christopher ZAPART,National Astronomical Observatory of Japan,Julia meets BIG DATA: JVO experience with distributed computing,"Each year the size of FITS data cubes coming out from various telescopes keeps rising. At present the largest publicly-available FITS file handled by the Japanese Virtual Observatory (JVO) is about 350GB, which is putting an enormous strain on the existing hardware infrastructure. We anticipate ALMA FITS files to break through 1TB in the near future.

A simple solution to handling such large file sizes is to keep buying a more powerful server each year. However, in a budget-constrained astronomy data centre the reasonable alternative is to try distributed computing using commodity cluster computing. To this end, in order to keep up with the ever-rising file sizes the JVO development team has been experimenting with various forms of distributed computing using several commodity desktop computers networked via 10Gbps Ethernet. As part of a search for the most convenient and performant distributed computing paradigm, since late 2018 we have gone through three programming language changes: from Rust to C/C++17, then a hybrid CoArray Fortran 2018 augmented by pure C, finally to settle for Julia due to its superior asynchronous distributed computing capabilities.

The talk discusses in detail the rationale behind such drastic programming language changes as well as showcases the latest cluster edition (still under development) of FITSWebQL v5, code-named FITSWEBQL SE (Supercomputer Edition). The purpose of the JVO FITSWebQL software is to provide an interactive preview of FITS files via a web browser, without a need to download the underlying FITS files. The latest Julia FITSWEBQLSE makes it possible to preview interactively (in near real-time) over 350GB-large FITS files from the comfort of a web browser."
O3-005,Ivan Katkov,New York University Abu Dhabi,Fast interactive web-based data visualizer of panoramic spectroscopic surveys,"Panoramic IFU spectroscopy is a core tool of modern observational astronomy and is especially important for galaxy physics. Many massive IFU surveys, such as SDSS MaNGA (10k targets), SAMI (3k targets), Califa (600 objects), Atlas3D (260 objects) have recently been released and made publicly available to the broad astronomical community. The complexity and massiveness of the derived data products from spectral cubes makes visualization of the entire dataset challenging, but nevertheless very important and crucial for scientific output.

Based on our past experience with visualization of spectral and imaging data built in the frame of the VOxAstro Initiative projects, we are now developing online web service for interactive visualizing spectroscopic IFU datasets. Our service provides convenient access and visualization tool for spectral cubes from the publicly available surveys (MaNGA, SAMI, Califa, Atlas3D) and results of their modeling, as well as maps of parameters derived from the cubes, implementing the connected views concept. Here we will describe the core components and functionality of the service, including an API implementation that combines REST API and GraphQL endpoints on top of Django+Postgres backend as well as a fast and responsive user interface built using the modern Vue.js-based framework Quasar."
D0-001,G. Bruce Berriman,Caltech/IPAC-NExScI,Astronomical Image Processing at Scale With Pegasus and Montage.,"Image processing at scale is a powerful tool for creating new datasets and integrating them with existing data sets, and performing analysis and quality assurance investigations. Workflow managers offer advantages in this type of processing, which involve multiple data access and processing steps. Generally, they enable automation of the workflow by locating data and resources, recovery from failures, and monitoring of performance, We demonstrate in this focus demo the use of the Python API of the Pegasus Workflow Manager (https://pegasus.isi.edu/) to manage processing of images to create mosaics with the Montage Image Mosaic engine (http://montage.ipac.caltech.edu). Since 2001, Pegasus has been developed and maintained at ISI, USC. Montage was in fact one of the first applications used to design Pegasus and optimize its performance. Pegasus has since found application in many areas of science. LIGO exploited it in making discoveries of black holes (https://pegasus.isi.edu/application-showcase/ligo/) . The Vera Rubin Observatory used it to compare the cost and performance of processing images on cloud platforms (https://pegasus.isi.edu/2021/03/17/astronomical-image-processing-in-the-cloud/). While these are examples of projects at large scale, investigations on local clusters of machines by small team can benefit from Pegasus as well. We describe how astronomers can use the Pegasus Python API to plan and execute workflows to create visualizations of the sky that comply with the Hierarchical Progressive Survey (HiPS) standards, on local and on cloud platforms."
X4-004,G. Bruce Berriman,Caltech/IPAC-NExScI,The International Virtual Observatory in 2021,"The International Virtual Observatory Alliance (IVOA) develops the technical standards needed for seamless discovery of and access to astronomy data worldwide, with the goal of realizing the Virtual Observatory (VO). Founded in 2002, the IVOA was an early advocate of what are now known as Findable, Accessible, Interoperable and Reusable (FAIR) principles. There are 22 member organizations, and astronomical communities from other nations have shown interest in joining the IVOA. This paper describes the activities of the IVOA in 2021; summarizes the activities at the May and November 2021 virtual ""interoperability meetings""; describes the impact of IVOA activities on the astronomy community; and discusses its prospects for 2022 and beyond."
O3-001,Tim Hostetler,"Harvard-Smithsonian Center for Astrophysics, 60 Garden Street, Cambridge, MA 02138, USA",Web accessibility trends and implementation in dynamic web applications,"The NASA Astrophysics Data System (ADS), a critical research tool, strives to provide the most accessible and inclusive environment for the analysis and exploration of the astronomical literature. Part of this goal involves creating a digital platform that can accommodate all people, including those with disabilities that would benefit from alternative ways to present the information provided by the website. NASA ADS follows the official Web Content Accessibility Guidelines (WCAG) standard for ensuring accessibility of all its applications, striving to exceed this standard where possible. Through the use of both internal audits and external expert review based on these guidelines, we have identified many areas for improving accessibility in our current web application, and have implemented a number of updates to the UI as a result of this. We present an overview of some current web accessibility trends, discuss our experience incorporating these trends in our web application, and discuss the lessons learned and recommendations for future projects."
O7-007,Jonathan Kenyon,Rhodes University/SARAO,QuartiCal - embarassingly parallel calibration using Numba and Dask,"We live in the era of Big Data. Where once it was a looming threat, a problem for our future selves, it is now very much upon us. Existing radio interferometers, such as the MeerKAT and LOFAR, already produce unprecedented quantities of visibility data, and even they will soon be dwarfed by the planned SKA and ngVLA. Despite the modernity of these instruments, they will still require extensive calibration in order to correct various science-limiting effects. Thus calibration is and will remain an integral part of radio interferometric data reduction. This has motivated the development of QuartiCal, a Python application that leverages a couple of contemporary packages to make calibration scalable, distributable and fast. The first such package is Dask, a library for parallel and distributed computing with Python. It provides parallel, Big Data collections that extend familiar interfaces (e.g. NumPy, Pandas). Using these collections, it is possible to construct task graphs that can be understood by Dask’s dynamic task schedulers. This allows appropriately written code to scale from executing locally on a laptop to remotely on a compute cluster. The second package of interest is a Numba. Numba is a just-in-time compiler for a subset of Python/NumPy that can provide C-like speed without forfeiting the expressiveness and dynamism of Python. It has been used to extensively optimize the computationally demanding components of the calibration algorithms. QuartiCal convincingly outperforms its predecessor, CubiCal, in terms of both wall time and memory footprint. Finally, in testing QuartiCal, we have found that the Measurement Set (backed by the Casacore Table Data System) can limit parallel performance."
D3-003,Ricky O'Steen,Space Telescope Science Institute,Jdaviz: An interactive visual data analysis tool for JWST in the Jupyter platform,"Jdaviz is a package of astronomical data analysis visualization tools based on the Jupyter platform. These GUI-based tools link data visualization and interactive analysis. They are designed to work within a Jupyter notebook cell, as a standalone desktop application, or as embedded windows within a website – all with nearly-identical user interfaces. Jdaviz has been designed specifically to support JWST use cases, but can be used for a wide range of astronomical data.

This demo will explain the technology stack underlying Jdaviz that works to connect the front-end interactive application with the astropy-based analysis suite on the back-end, including Glue, glue-jupyter, glue-astronomy, ipyveutify, and bqplot. I will give brief demonstrations of the tools available for analyzing data cubes, single- and multi-object spectroscopy, and images."
O3-003,Brent Miszalski,"Australian Astronomical Optics, Macquarie University",Data Central’s Data Aggregation Service,"Astronomical data are now accessible from a wide variety of online services. Astronomers can access many of these services programmatically (e.g. from Python) to unlock powerful new research outcomes. Aggregating heterogeneous observational data for one or several targets is a common task performed by astronomers that benefits substantially from the growing ecosystem of online services. We introduce Data Central's Data Aggregation Service (DAS), an interactive web application that leverages Aladin Lite to display images and catalogues resulting from multiple online service queries of a given target. The DAS is a high-performance technology demonstrator for modern asynchronous Python that enables fast, simultaneous execution of queries. Query results may also be directed to a Pipeline As a Web Service (PAWS), whereby a data reduction pipeline is executed in the background and the results displayed upon completion. Examples include on-demand mosaic production for the SkyMapper Southern Sky Survey and archival ESO FORS2 and Gemini GMOS images. We conclude with an overview of future plans, focusing on user customisation of DAS and adapting DAS to cater for large target catalogues. We anticipate that services like DAS will play an essential role in simplifying access to the massive data volumes expected from next generation facilities."
O1-004,Samorodova Ekaterina,Lomonosov Moscow State University,Astronomical Data Approximation Based on Neural Network Models,"Modern astronomical telescopes observe millions of transients every night. Light curves of these objects help to deduce their physical and phenomenological properties. A light curve is a flux time series measured in one or several photometric passbands. Each passband might have a different number of observations and various time gaps between them, thus complicating further analysis. To overcome these difficulties, we propose new methods for light curve approximation based on neural network models. 

In this study, we use The Photometric LSST Astronomical Time Series Classification Challenge (PLAsTiCC) dataset. We apply shallow neural networks, bayesian neural networks, and normalizing flows to approximate observations for a single object and facilitate analysis of the light curves. We show that the approximation quality of the proposed methods significantly outperform the existing approaches based on gaussian processes. We also perform two physical analyses: supernovae type Ia classification and intensity peak estimation. For both these problems, convolutional neural networks are trained on approximated light curves. The results show that the proposed methods help to improve the quality of supernovae type identification and increase the accuracy of the intensity peak estimation compared with the gaussian processes model."
X0-001,Hubert Degaudenzi,,EleFits: A modern C++ API on top of CFITSIO,"EleFits is a new C++ package to read and write FITS files which focuses on (1) safety, (2) user-friendliness, and (3) performance.
This paper presents the development rationale, examples, and comparisons to the available alternatives: CFITSIO, CCfits, SFITSIO, and AFW.
(1) Safety first:
To our knowledge, EleFits is the only option which features a fully consistent and optimal internal type management system.
Nevertheless, the API is templated and agnostic of the underlying architecture, which makes usage straightforward.
(2) As for ease of use, EleFits is shown to be more compact and to involve less parameters than its alternatives, which makes it both simpler and less error-prone.
Most of the basic services of CFITSIO have been implemented already, and more advanced ones are being added regularly.
Furthermore, EleFits provides exclusive features like HDU selectors and automatic buffering of binary tables.
(3) To maximize performance, EleFits is built as a CFITSIO wrapper as thin as possible.
A benchmark was developed to compare I/O times, with CFITSIO as the reference.
While the two libraries are generally equivalent, optimizations implemented internally make EleFits even faster in some classical cases, unless the CFITSIO user spends considerable development efforts.
The package is used since years internally to the Euclid ground segment and is being open-sourced."
O1-003,Guido Cupani,INAF–Osservatorio Astronomico di Trieste,Advanced Data Analysis for Observational Cosmology: applications to the study of the Intergalactic Medium,"The analysis of absorption features along the line of sight to distant sources is an invaluable tool for observational cosmology, giving a direct insight into the physical and chemical state of the inter- and circumgalactic medium, which can be used to constrain both the cosmic evolution (abundance of primordial elements, reionization, etc.) and fundamental physics (dark matter, variability of constants, general relativity, etc.). Such endeavour entails two basic requirements: the accessibility of bright QSOs as background beacons, to be observed with current and future earth-based facilities (e.g. the ESO VLT and ELT), and the availability of software resources to properly analyze the data, extracting the relevant information in a reliable and reproducible way. In this talk, we will present the latest results we obtained in both direction within the QUBRICS project (QUasars as BRIght beacons for Cosmology in the Southern hemisphere). We will describe how machine learning techniques (canonical component analysis, probabilistic random forest) were applied to detect hundreds of previously unknown QSOs in the southern hemisphere, and how state-of-the art software like QSFit and Astrocook was integrated in the analysis of the targets, opening up new possibilities for the next era of intergalactic medium observations."
O7-003,jsalgado,SKAO,Designing the SKA Regional Centre Network,"The construction of the world's largest radio telescope, the SKA, has been approved and begins this year. In order to cope with the huge amount of data expected, an international network of SKA Regional Centres (SRC), the SRCNet, is being defined and developed. The SRCs will be receiving and managing hundreds of petabytes of data from the SKA Observatory (SKAO), providing the infrastructure, tools and user support necessary to enable science with the SKA data. The SRC community and SKAO have engaged in a challenge to design a globally distributed system SRCNet that will equip scientists worldwide with the necessary means for easy access to the globally distributed data and compute infrastructure and to provide remote data analysis tools using the SaaS approach. The SKA data flow from the generation to the SRCs and data exploitation represent the biggest challenge on astronomical data handling of the era. We will present an overview of the system architecture design of SRCNet that has been prepared for the global prototyping effort that begins in 2022. We will also discuss the distributed development framework that we intend to use during the prototyping effort."
O3-004,Venustiano Soancatl Aguilar,University of Groningen,A Tool to Explore Astronomical Databases and Transform Data into Planetarium Formats,"The Virtual Observatory can provide a valuable source of material
 for presentations in digital planetaria. However, as discussed in a
 previous paper, importing data from the Virtual Observatory can be a
 time consuming process. Data must be downloaded manually from a web
 portal or programatically by using an API (Application Program
 Interface). The data must then be transformed into one of the
 formats used in digital planetaria: this can be done by ad-hoc
 scripts, but writing them takes time and is prone to error. The
 Data2Dome consortium, provides an excellent source of data for
 digital planetaria, but does not provide a tool to access archival
 catalogues in a flexible way. Here we present a tool that can
 accelerate the transformation of astronomical data into formats that
 can be used by systems commonly used in digital planetaria. The tool
 consist of a graphical user interface that gives the ability to
 explore and query a range of different astronomical databases, such
 as KiDS, GAIA and SDSS. It can store queries in a database, and
 transform the data into appropriate planetarium formats. Prototypes
 of this tool have been used to prepare data for use in presentations
 in the Dot Live Planetarium in Groningen. It has been shown to be a
 convenient method of preparing presentations by professional
 astronomers on specialist subjects."
O7-003,Slava Kitaeff,,Designing the SKA Regional Centre Network,"The construction of the world's largest radio telescope, the SKA, has been approved and begins this year. In order to cope with the huge amount of data expected, an international network of SKA Regional Centres (SRC), the SRCNet, is being defined and developed. The SRCs will be receiving and managing hundreds of petabytes of data from the SKA Observatory (SKAO), providing the infrastructure, tools and user support necessary to enable science with the SKA data. The SRC community and SKAO have engaged in a challenge to design a globally distributed system SRCNet that will equip scientists worldwide with the necessary means for easy access to the globally distributed data and compute infrastructure and to provide remote data analysis tools using the SaaS approach. The SKA data flow from the generation to the SRCs and data exploitation represent the biggest challenge on astronomical data handling of the era. We will present an overview of the system architecture design of SRCNet that has been prepared for the global prototyping effort that begins in 2022. We will also discuss the distributed development framework that we intend to use during the prototyping effort."
O3-008,Maria Arevalo Sanchez,RHEA Group for ESA,Grand opening of the European JWST Archive at the ESAC Science Data Centre,"The James Webb Space Telescope (JWST) is the next major space science observatory and will complement and extend the discoveries that the Hubble Space Telescope (HST) has made so far. NASA, the European Space Agency (ESA) and the Canadian Space Agency (CSA) have participated together in the design, construction and operations definition of this great space observatory that will allow astronomers to understand the Early Universe, evolution of galaxies over the time, the stellar life cycle and the properties of both Solar System and extrasolar planets, including the search for the building blocks of life. Providing astronomers with an easy access to observations that will lead to such a set of top-level discoveries requires a robust and user friendly access.

The ESAC Science Data Centre (ESDC) has been in charge of providing a European JWST Archive (eJWST) predominantly aimed at the European scientific community but open to all scientists around the world. Within the established partnership of NASA, ESA and CSA, we are making use of common procedures and tools in order to synchronise JWST metadata and data contents mirror of the Mikulski Archive for Space Telescopes (MAST) JWST archive at the Space Telescope Science Institute (STScI). The ESDC has developed a carefully designed graphical user interface plus several backend services based on Virtual Observatory (VO) protocols to access both private (data stored at STScI) and public observations (data also stored at ESDC).

We will describe the graphical user interface and the integration of ESASky to allow users to put JWST observations data and metadata in the context of multi-wavelength science , and several viewers developed for quick exploration of data cubes and images. Being a fully Table Access Protocol (TAP) based archive, the eJWST Archive provides also several means to accelerate science by providing users access with dedicated python modules in packages such as an eJWST-specific Astroquery module and searches based on the ADQL query language."
X1-004,Miguel Flores R.,Universidad de Guadalajara,Stellar Spectra Models Classification and Parameter Estimation Using Machine Learning Algorithms,"The growth of sky surveys and the large amount of stellar spectra in the current databases, has generated the necessity of developing new methods to estimate stellar parameters, a fundamental task on stellar research. In this work, we present a comparison of different machine learning algorithms, used for the classification of stellar synthetic spectra and the estimation of fundamental stellar parameters including temperature, luminosity, surface gravity, mass, and rotational velocity. For both tasks, we established a group of supervised learning models, and propose a database of measurements with the same structure to train the algorithms. These data include equivalent-width measurements over noisy synthetic spectra in order to replicate the natural noise on a real observed spectrum. Different levels of signal to noise ratio are considered for this analysis."
X3-004,Javier Espinosa Aranda,ESA - ESAC / RHEA for ESA,Design and Development of Modern Science Archives at the ESAC Science Data Centre,"The key objectives of the ESAC Science Data Centre (ESDC) are the long term preservation of the data from a wide variety of missions and the provision of access to scientists all around the world through different entry points such as web applications, command-line interfaces, Astroquery modules and Jupyter notebooks. These tasks require the design of cost-effective solutions based on, where possible, the homogenization of technologies across Science Archives, with the intention of increasing the maintainability amongst the team of software engineers and the development of common modules across projects. Also, with the aim of promoting reusability across shared components between missions and facilitating their integration and the implementation of common interface guidelines while always balancing this against state-of-the-art frameworks to achieve the maximum performance when processing huge datasets and, as trends in society require supporting cross-platform availability such as laptops and mobile phones.

This paper gives a quick overview of the different UI frameworks that have been used at ESDC over the years, as an introduction that will lead us to a new platform using Angular that is currently being applied in some Science Archives such as the Human and Robotic Exploration Data Archive, the Integral Science Legacy Archive, the Solar and Heliospheric Observatory Science Archive and even the European Hubble Space Telescope Archive. The criteria for choosing this Javascript framework by ESDC software engineers will be described with a focus on modernity, responsiveness, maintainability, flexibility, testing and reusability. In addition to this, some new projects will be presented, showing examples of the common L&F and practices agreed across missions to provide easy and intuitive access to users and the creation and integration of custom modules, including full ESASky integration, image viewers and data cube visualizers that have been developed in the context of one mission but are currently reused by several of these Science Archives."
X9-002,Yan Grange,ASTRON,"Astronomical data organization, management and access in Scientific Data Lakes","The data volume coming out of telescopes is constantly increasing. Often the archive of astronomical data is stored over a distributed storage infrastructure, provided by independent compute centres. Such a data archive of distributed nature requires an overarching data management orchestration structure composed by tools able to handle data storage and cataloguing, steering data transfers and integrate different storage systems and protocols, while maintaining a common authorisation and authentication layer and being aware of data locality and data policies. This data management infrastructure, which we will refer to as the Data Lake, hides the inherent complexity and is perceived as a single entity, providing transparent access to end users wanting to obtain data from the archive.

Particle physics is another domain that has been in need of complex, distributed data management systems. The experiments at the LHC accelerator at CERN generate hundreds of petabytes of data per year, this data is distributed to partner sites and users globally using national compute facilities. Several tools were developed to address these distributed computing challenges in the context of WLCG (Worldwide LHC Computing Grid). The work being carried out in the ESCAPE project and in the Data Infrastructure for Open Science (DIOS) work package is to prototype a Data Lake harnessing different physics scientific disciplines addressing FAIR standards and Open Data approaches. In this work we will present how the Data Lake prototype is applied to address astronomical data use cases. We will introduce the software stack and the experiments we execute, discuss the differences between the domains and show some preliminary results of the evaluation."
O1-004,Konstantin Malanchev,,Astronomical Data Approximation Based on Neural Network Models,"Modern astronomical telescopes observe millions of transients every night. Light curves of these objects help to deduce their physical and phenomenological properties. A light curve is a flux time series measured in one or several photometric passbands. Each passband might have a different number of observations and various time gaps between them, thus complicating further analysis. To overcome these difficulties, we propose new methods for light curve approximation based on neural network models. 

In this study, we use The Photometric LSST Astronomical Time Series Classification Challenge (PLAsTiCC) dataset. We apply shallow neural networks, bayesian neural networks, and normalizing flows to approximate observations for a single object and facilitate analysis of the light curves. We show that the approximation quality of the proposed methods significantly outperform the existing approaches based on gaussian processes. We also perform two physical analyses: supernovae type Ia classification and intensity peak estimation. For both these problems, convolutional neural networks are trained on approximated light curves. The results show that the proposed methods help to improve the quality of supernovae type identification and increase the accuracy of the intensity peak estimation compared with the gaussian processes model."
O4-001,Marco Molinaro,INAF,Supporting FAIR principles in the Astrophysics Community - the European experience,"FAIR principles have “the intent” to “act as a guideline for those wishing to enhance the reusability of their data holdings” and “put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals”.
Interoperability, one core of these principles, especially when dealing with automated systems’ ability to interface with each other, requires open standards to avoid restrictions that negatively impact the user's experience.
Open-ness of standards is best supported when the governance itself is open and includes a wide range of community participation.
In this contribution we report our experience with the FAIR principles, interoperable systems and open governance in astrophysics. We report on activities that have matured within the EU H2020 ESCAPE project, CEVO work package, with a focus on interfacing the EOSC architecture and Interoperability Framework and including additional experience from other EU funded projects dealing with data holdings in the general domain of astrophysics (and specifically the Virtual Observatory framework).
We will touch on our understanding of the above concepts related to research results value, metrics and data quality, community and governance, cross-domain interaction and in-domain metadata enrichment towards machine actionability."
X0-001,Manuel Grizonnet,CNES,EleFits: A modern C++ API on top of CFITSIO,"EleFits is a new C++ package to read and write FITS files which focuses on (1) safety, (2) user-friendliness, and (3) performance.
This paper presents the development rationale, examples, and comparisons to the available alternatives: CFITSIO, CCfits, SFITSIO, and AFW.
(1) Safety first:
To our knowledge, EleFits is the only option which features a fully consistent and optimal internal type management system.
Nevertheless, the API is templated and agnostic of the underlying architecture, which makes usage straightforward.
(2) As for ease of use, EleFits is shown to be more compact and to involve less parameters than its alternatives, which makes it both simpler and less error-prone.
Most of the basic services of CFITSIO have been implemented already, and more advanced ones are being added regularly.
Furthermore, EleFits provides exclusive features like HDU selectors and automatic buffering of binary tables.
(3) To maximize performance, EleFits is built as a CFITSIO wrapper as thin as possible.
A benchmark was developed to compare I/O times, with CFITSIO as the reference.
While the two libraries are generally equivalent, optimizations implemented internally make EleFits even faster in some classical cases, unless the CFITSIO user spends considerable development efforts.
The package is used since years internally to the Euclid ground segment and is being open-sourced."
O5-001,Fran Jiménez-Esteban,CAB (INTA-CSIC),European Virtual Observatory Schools,"The European Virtual Observatory (Euro-VO; https://www.euro-vo.org/) initiative organises regular VO schools since 2008. The goals are twofold: on the one hand, to expose early-career European astronomers to the variety of currently available VO tools and services so that they can use them efficiently for their own research and, on the other hand, to gather their feedback on the VO tools and services and the school itself. During the schools, VO experts guide participants on the usage of the tools through a series of predefined real science cases, an activity that took most of the allocated time. Participants also have the opportunity to develop their own science cases under the guidance of VO tutors. These schools have demonstrated to be very useful for students, since they declare to regularly use the VO tools in their research afterwards, and for us, since we have first hand information about the user needs. In this talk, I will introduce our VO schools, explain the approach we follow, and present the training materials that we have developed along the years."
D0-001,Ewa Deelman,,Astronomical Image Processing at Scale With Pegasus and Montage.,"Image processing at scale is a powerful tool for creating new datasets and integrating them with existing data sets, and performing analysis and quality assurance investigations. Workflow managers offer advantages in this type of processing, which involve multiple data access and processing steps. Generally, they enable automation of the workflow by locating data and resources, recovery from failures, and monitoring of performance, We demonstrate in this focus demo the use of the Python API of the Pegasus Workflow Manager (https://pegasus.isi.edu/) to manage processing of images to create mosaics with the Montage Image Mosaic engine (http://montage.ipac.caltech.edu). Since 2001, Pegasus has been developed and maintained at ISI, USC. Montage was in fact one of the first applications used to design Pegasus and optimize its performance. Pegasus has since found application in many areas of science. LIGO exploited it in making discoveries of black holes (https://pegasus.isi.edu/application-showcase/ligo/) . The Vera Rubin Observatory used it to compare the cost and performance of processing images on cloud platforms (https://pegasus.isi.edu/2021/03/17/astronomical-image-processing-in-the-cloud/). While these are examples of projects at large scale, investigations on local clusters of machines by small team can benefit from Pegasus as well. We describe how astronomers can use the Pegasus Python API to plan and execute workflows to create visualizations of the sky that comply with the Hierarchical Progressive Survey (HiPS) standards, on local and on cloud platforms."
O0-004,Athanaseus Javas Ramaila,SARAO,AIMfast: An Astronomical Image Fidelity Assessment Tool,"Radio telescopes spanning various locations across the Earth today are used as interferometer arrays to examine fine radio sky, providing scientists with a tremendous amount of data that can then be analysed to acquire objective information to understand the universe. Despite difficulties with large volumes of data coming from radio telescopes, radio interferometric reduction and imaging techniques show promising results in the current data-rich era. There is an excellent effort in the development of software in radio astronomy, and it is becoming available to the scientific community. Image fidelity is a measure of the accuracy of the reconstructed sky brightness distribution. A related metric, dynamic range, can also be used to measure the degree to which imaging artifacts around strong sources are suppressed, which implies a higher fidelity of the on-source reconstruction.

The main objective of this project is to develop a framework that allows for the evaluation of calibration, imaging and source finding techniques in astronomy. We quantify the ability of these techniques based on the recovery of source properties such as flux and astrometry. AIMfast (https://github.com/Athanaseus/aimfast) allows astronomers to analyse source catalogues by efficiently cross-matching these properties. Furthermore, global and localised image statistics (e.g. rms, mad, skewness etc.) can be computed using this tool. It is a Python-based package that can be executed from the command line and easily be integrated into a workflow environment. All visualisation outputs are saved in a portable Html format that can be loaded in any web browser. Moreover, a JSON file with all the statistics and source properties can be generated to use in other software programs."
D0-001,John Good,"Infrared Processing and Analysis Center, Caltech",Astronomical Image Processing at Scale With Pegasus and Montage.,"Image processing at scale is a powerful tool for creating new datasets and integrating them with existing data sets, and performing analysis and quality assurance investigations. Workflow managers offer advantages in this type of processing, which involve multiple data access and processing steps. Generally, they enable automation of the workflow by locating data and resources, recovery from failures, and monitoring of performance, We demonstrate in this focus demo the use of the Python API of the Pegasus Workflow Manager (https://pegasus.isi.edu/) to manage processing of images to create mosaics with the Montage Image Mosaic engine (http://montage.ipac.caltech.edu). Since 2001, Pegasus has been developed and maintained at ISI, USC. Montage was in fact one of the first applications used to design Pegasus and optimize its performance. Pegasus has since found application in many areas of science. LIGO exploited it in making discoveries of black holes (https://pegasus.isi.edu/application-showcase/ligo/) . The Vera Rubin Observatory used it to compare the cost and performance of processing images on cloud platforms (https://pegasus.isi.edu/2021/03/17/astronomical-image-processing-in-the-cloud/). While these are examples of projects at large scale, investigations on local clusters of machines by small team can benefit from Pegasus as well. We describe how astronomers can use the Pegasus Python API to plan and execute workflows to create visualizations of the sky that comply with the Hierarchical Progressive Survey (HiPS) standards, on local and on cloud platforms."
O7-002,Dino Bektesevic,Unviersity of Washington,Lessons learned by adding cloud support to Rubin software,"The Legacy Survey of Space and Time (LSST, www.lsst.org), operated by the Vera C. Rubin Observatory, is a 10-year astronomical survey due to start operations in 2022 that will image half the sky every three nights. Rubin estimates that the total amount of data that will be collected during operations to be about 60 petabytes (PB) from which a 20PB large catalog will be produced. At these data volumes reprocessing even a relatively small subset of data requires significant resources. We wanted to leverage cloud technologies to enable and accelerate data (re)processing. In this talk we describe how we adapted Rubin's image analysis software to run on AWS and the, often surprising, lessons learned while doing so."
X4-002,Damir Gasymov,Moscow M.V. Lomonosov State University,Non-parametric LOSVD analysis,"Ill-posed inverse problems are common in astronomy, and their solutions are unstable with respect to noise in the data. Solutions of such problems are typically found using two classes of methods: parametrization and fitting the data against some predefined function or a solution with a non-parametrical function using regularization. Here we are focusing on the latter non-parametric approach applied for the recovery of complex stellar line-of-sight velocity distribution (LOSVD) from the observed galaxy spectra. Development of such an approach is crucial for galaxies hosting multiple kinematically misaligned stellar components, such as 2 stellar counter-rotating disks, thin and thick disks, kinematically decoupled cores, and others. 

Stellar LOSVD recovery from the observed galaxy spectra is equivalent to a deconvolution and can be solved as a linear inverse problem. To overcome its ill-posed nature we apply smoothing regularization. Searching for an optimal degree of smoothing regularization is a challenging part of this approach. We use a so-called L-curve technique to find the optimal value. Here we present a non-parametric fitting technique, discuss its potential caveats, perform numerous tests based on synthetic mock spectra, and show real-world application to MaNGA spectral data cubes and some long-slit spectra of stellar counter-rotating galaxies."
I8-001,kumikousuda,"Subaru Telescope, National Astronomical Observatory of Japan",GALAXY CRUISE Engages Citizen Astronomers to Explore Galaxies,"GALAXY CRUISE is the first citizen science project of the National Astronomical Observatory of Japan (NAOJ). The Subaru Telescope, NAOJ’s 8.2-m optical-infrared telescope on Maunakea, Hawai`i, is now conducting an extensive survey program called the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP). GALAXY CRUISE uses its second dataset, released to the world in May 2019. Citizen Astronomers classify and identify interacting galaxies in vast cosmic images taken by the Subaru Telescope, which are displayed one after another on a PC or tablet screen. The Japanese site opened on November 1, 2019, and the English site opened on February 19, 2020. As of August 1, 2021, there are 6931 registered Citizen Astronomers from 83 countries and regions, and the total classification results have exceeded 1.6 million.

The GALAXY CRUISE website has the following unique features to engage many people to participate in this project and maintain their interest.

(1) Thorough Training and Practice Menus
Before Citizen Astronomers register to start classification, they are required to complete the three training sessions to obtain a basic knowledge of galaxies. These menus can eliminate non-professionals’ anxieties to select a wrong answer. With the support of the National Museum of Emerging Science and Innovation (Miraikan), we developed the menus by collecting data from its visitors.

(2) World View and Gamification Events
Our project is likened to a cruise ship where many crew members sail together in the cosmic ocean. We created a cruise map or the nautical chart, which represents GALAXY CRUISE’s world view from the observation map of HSC-SSP, with four “towns” (small and deep observation fields) and six “continents” (wide fields). Every town or continent is divided into multiple areas. Citizen Astronomers can earn a souvenir (commemorative illustration) when they complete a certain number of areas. Also, when they complete a stage (town or continent), a departure stamp will be added to the passport. The voyage log, passport stamps, and souvenirs can be seen on the welcome page of each Citizen Astronomer.

(3) Exploration of the Vast Universe
GALAXY CRUISE classification site uses the same engine of hscMap, a user-friendly website to access the HSC-SSP big data. Citizen astronomers can enjoy exploring the vast cosmic images captured by the Subaru Telescope while classifying galaxies.

In my talk, I will present the process of developing the user-friendly website of GALAXY CRUISE, comparing my other project to develop tactile resources for blind and visually impaired (BVI) people."
D3-002,Anthony Horton,"Australian Astronomical Optics, Macquarie University",PyCPL and PyEsoRex: The ESO Common Pipeline Library in Python,"The ESO Common Pipeline Library (CPL) comprises a set of ISO-C libraries that provide a comprehensive, efficient and robust software toolkit to develop astronomical data-reduction recipes, which has been the fundamental tool for building data reduction pipelines for ESO VLT instruments since its original release in 2003, and which will continue to be the basis for ELT instrument pipelines. CPL was developed in C for reasons of efficiency and speed of execution, and because of its maturity and widespread use, it is well tested and understood. However, as the astronomical community’s preference moves more and more towards Python as the language of choice for algorithm prototyping and data reduction, there has emerged a need to provide access to CPL for users who wish to make use of the power of CPL from a Python environment.
The PyCPL project provides this access, making it possible to run existing CPL recipes from within a Python environment as well as developing new recipes in Python. These new recipes are built using the PyCPL library, a fully object-orientated, idiomatic Python interface to the powerful CPL library, while also allowing users and developers to take full advantage of the rest of the scientific Python ecosystem. In this demo we will show how both CPL and PyCPL recipes can be executed using PyEsoRex (the PyCPL equivalent of EsoRex), both on the command line and from within an interactive Python session. We will then give an example of PyCPL recipe development, demonstrating a range of PyCPL library features in the process. Finally we will highlight some of the interoperability features of PyCPL, which enable easy conversions between PyCPL objects and objects from commonly used Python libraries such as numpy, astropy and pandas."
X3-009,Anthony Horton,"Australian Astronomical Optics, Macquarie University",PyCPL: using Pybind to bring and adapt C libraries to Python,"Authors: Anthony Heng, Aidan Farrell, Anthony Horton, Nuria Lorente, Ralf Palsa, Lars Lundin, Katrina Sealey

The ESO Common Pipeline Library (CPL) comprises a set of ISO-C libraries that provide a comprehensive, efficient and robust software toolkit to develop astronomical data-reduction recipes, which has been the fundamental tool for building data reduction pipelines for ESO VLT instruments since its original release in 2003, and which will continue to be the basis for ELT instrument pipelines. CPL was developed in C for reasons of efficiency and speed of execution, and because of its maturity and widespread use, it is well tested and understood. However, as the astronomical community’s preference moves more and more towards Python as the language of choice for algorithm prototyping and data reduction, there has emerged a need to provide access to CPL for users who wish to make use of the power of CPL from a Python environment.
The PyCPL project aims to provide this access to CPL within Python. Compiled CPL structures and algorithms will be mapped to Python equivalent objects. This presents the opportunity to execute existing data reduction pipelines from within the Python environment, in addition to providing Python developers the opportunity to write their own pipelines using the existing CPL structures and algorithms, leveraging the efficiency, speed, and maturity of CPL from a pure Python ecosystem. Included with the PyCPL package will the PyEsoRex recipe execution module, which is a Python analogue of the EsoRex execution tool. PyEsoRex allows the user to run both existing C Recipes and new Python recipes and provides a standardised interface for Python recipe development.
In this talk we will introduce PyBind11: the core library that has allowed us to connect CPL to Python, and discuss how we have employed it in the project: what has worked “out of the box”, and what has needed customisation and additional C++ and Python code to produce a pyCPL which feels natural to CPL users, and attractive to those accustomed to a Python environment."
O1-006,Fernando L. Ventura,University of Pretoria,An Unsupervised Learning approach to classifying Extended Radio Sources from deep MeerKAT images,"In this talk I present our Machine-learning based approach to analysing image-domain data from the MeerKAT telescope. Images from sensitive radio telescopes such as MeerKAT reveal unforeseen features even in familiar sources; while at the same time the spatial density of sources in these images makes manual classification and analysis unfeasible. I demonstrate that Autoencoders are in particular well-suited for this task as they should isolate key features of different morphologies while not overemphasising the source-to-source variations very common in sensitive radio images. This may also help remove potential biases and lack of repeatability in manual classification. We look at some variations of the autoencoder, including variational autoencoders and the use of pyramidal spatial pooling.

I will show some results from our use of these algorithms on images from the MeerKAT Galaxy Cluster Legacy Survey (MGCLS).

Going forward these algorithms can also be used on future and larger surveys. The number of sources likely to be observed in future surveys, especially with new and upcoming arrays such as the SKA, will far exceed the capacity for manual classification and an automated approach will be necessary."
B3-001,Laurent Michel,Astronomical Observatory of Strasbourg,TAP and the Data Models,"TAP is one of the big achievements of the VO. This protocol gives any relational database a high level of interoperability thanks to several IVOA standards:
- The TAP_SCHEMA : a description of the tables, their columns and the way they can be joined.
- ADQL: a query language, subset of SQL, with some astronomy-specific features
- UWS: a specification for a REST API to be used to handle service requests
These features provide a common way to discover the content of TAP services and to query them.
This works very well with relational data and we propose to investigate the possibility for TAP services to map searched data on data models. Indeed several data models have been developed by IVOA in order to tackle the complexity of the relationships between astronomical data features. Among those we can quote Photometry Data Model, Coordinates, Measurements, Transforms or MANGO that is well suited to describe source properties and relations to some datasets representing these sources. TAP services are able to host complex data bound with joins but the standard still misses important features to serve real model instances:
- A meta-data endpoint telling which models are available per table
- Storage of model meta-data into the TAP_SCHEMA
- Storage of coordinate frames in the TAP service
- Mechanism specifying the model on which the requested data must be mapped
- Mechanism returning multi-table responses for complex objects
- Preservation of model annotations in uploaded tables 
The purpose of the BoF is to discuss the relevance of enabling TAP services to deal with Data Models and to refine the functionalities required to implement such a capability.

We will be able to present a proof of concept based on the VOLLT framework that can annotate on the fly query responses on two archive tables using the MANGO data model. Any other contribution and point of view on this topic will be is welcome and helpful to lift up and enrich the debate ."
X3-010,Laurent Michel,Astronomical Observatory of Strasbourg,Annotating TAP responses on-the-fly against an IVOA data model,"With the success and widespread of the IVOA Table Access Protocol (1) for discovering and querying tabular data in astronomy, about one hundred of TAP services exposing altogether 22 thousands of tables are accessible from the IVOA Registries at the time of writing.
Currently the TAP protocol presents table data and metadata via a TAP_SCHEMA describing the served tables with their columns and possible joins between them.
We explore how to add an information layer, so that values within table columns can be gathered and used to populate instances of objects defined in a selected IVOA data model like Photometry, Coords, Measures, Transform or MANGO. This improves data interoperability by adding a model view common to all services. 
This information layer is provided through annotation tags, that are defined by the service which tells how the columns' values can be interpreted as attributes of instances of that model. 
Then when a TAP query is processed, our server add-on interprets the ADQL query string and produces on-the-fly, when possible, the TAP response as a VOTABLE document, decorated with the annotation of the object's fields present into the query and corresponding to the model elements templated for this service. 
This has been prototyped in Java, using the VOLLT (2) package library and a template annotation document representing elements from the MANGO(3) data model. This has been exercised on examples based on Vizier and Chandra catalogs.

(1) https://www.ivoa.net/documents/TAP/20190927/

(2) https://github.com/gmantele/vollt

(3) In development at https://github.com/ivoa-std/MANGO"
X4-010,Laurent Michel,Astronomical Observatory of Strasbourg,Object Oriented Data Model strategy in the context of IVOA Table Access Protocol services,"Object Oriented Data Model strategy in the context of IVOA Table Access Protocol services

François Bonnarel (CDS, ObAS, CNRS Strasbourg), Mireille Louys (CDS, ICube, Unistra), Laurent Michel (ObAS, CNRS Strasbourg) 

Table Access Protocol is an universal access standard for astronomical relational databases . TAP services registered in the IVOA registry, currently expose more than 22 thousands tables. 

It makes use of an astronomy extended SQL-like query language called ADQL. The database content is described in a standardized schema called the TAP_SCHEMA. Rich standardized metadata are included in the description. 
TAP services build up a query response in one single table. They extract data from the database following the structure given in the TAP_SCHEMA but provide only the selected columns. 

Beside this, IVOA developed a set of standardized object-oriented data models allowing a logical description of relationships between various pieces of data or metadata in astronomy. 
The elements queried in astronomical archive deal with photometry, coordinates, measurements, transformations, provenance and complex associations of those. 
These are described in various IVOA data models, following an Object Oriented strategy: Coords MEAS , Mango etc …. VO-DML is the IVOA recommendation to which IVOA data models should comply in order to be represented in a standard vodml-xml format. 

Mapping an object oriented-model in a relational database is a problem theoretically solved under the scope of ORM principles and rules. 

However the result of an ADQL query (as would be the case of the underlying SQL one in each specific database) is a single table. So in practice we are facing several issues for generating complex queries on one side, and for interpreation of the response on the other side.
The poster will review different solutions which have been experimented or considered by the IVOA DM WG in order to connect TAP responses (tables) with DMs instances (sub-graphs). The solutions vary according to the actual features of the data model and data structure. They should work when the database is following a predefined data model but also when we try to map a data model on legacy data. We will describe four possible solutions with examples:
- simple flat views built on top of the data model and the data. In that case the TAP schema covers the needs. An example is the ObsCore data model.
- on the fly table annotation In VOTable documents with a specific data model mapping syntax currently under development (ModelInstanceINVotable)
- organization of the ADQL query response as a normalized view with several tables and joins in the same VOTable documents with annotations delivered as quoted above.
- actual data model instance retrieval by new extensions of ADQL 

The poster will also show how TAP specification can be completed so that the TAP_SCHEMA contains metadata required to build table annotations or renormalisation"
B3-001,Mireille Louys,"Strasbourg University, ICube &CDS",TAP and the Data Models,"TAP is one of the big achievements of the VO. This protocol gives any relational database a high level of interoperability thanks to several IVOA standards:
- The TAP_SCHEMA : a description of the tables, their columns and the way they can be joined.
- ADQL: a query language, subset of SQL, with some astronomy-specific features
- UWS: a specification for a REST API to be used to handle service requests
These features provide a common way to discover the content of TAP services and to query them.
This works very well with relational data and we propose to investigate the possibility for TAP services to map searched data on data models. Indeed several data models have been developed by IVOA in order to tackle the complexity of the relationships between astronomical data features. Among those we can quote Photometry Data Model, Coordinates, Measurements, Transforms or MANGO that is well suited to describe source properties and relations to some datasets representing these sources. TAP services are able to host complex data bound with joins but the standard still misses important features to serve real model instances:
- A meta-data endpoint telling which models are available per table
- Storage of model meta-data into the TAP_SCHEMA
- Storage of coordinate frames in the TAP service
- Mechanism specifying the model on which the requested data must be mapped
- Mechanism returning multi-table responses for complex objects
- Preservation of model annotations in uploaded tables 
The purpose of the BoF is to discuss the relevance of enabling TAP services to deal with Data Models and to refine the functionalities required to implement such a capability.

We will be able to present a proof of concept based on the VOLLT framework that can annotate on the fly query responses on two archive tables using the MANGO data model. Any other contribution and point of view on this topic will be is welcome and helpful to lift up and enrich the debate ."
X3-010,Mireille Louys,"Strasbourg University, ICube &CDS",Annotating TAP responses on-the-fly against an IVOA data model,"With the success and widespread of the IVOA Table Access Protocol (1) for discovering and querying tabular data in astronomy, about one hundred of TAP services exposing altogether 22 thousands of tables are accessible from the IVOA Registries at the time of writing.
Currently the TAP protocol presents table data and metadata via a TAP_SCHEMA describing the served tables with their columns and possible joins between them.
We explore how to add an information layer, so that values within table columns can be gathered and used to populate instances of objects defined in a selected IVOA data model like Photometry, Coords, Measures, Transform or MANGO. This improves data interoperability by adding a model view common to all services. 
This information layer is provided through annotation tags, that are defined by the service which tells how the columns' values can be interpreted as attributes of instances of that model. 
Then when a TAP query is processed, our server add-on interprets the ADQL query string and produces on-the-fly, when possible, the TAP response as a VOTABLE document, decorated with the annotation of the object's fields present into the query and corresponding to the model elements templated for this service. 
This has been prototyped in Java, using the VOLLT (2) package library and a template annotation document representing elements from the MANGO(3) data model. This has been exercised on examples based on Vizier and Chandra catalogs.

(1) https://www.ivoa.net/documents/TAP/20190927/

(2) https://github.com/gmantele/vollt

(3) In development at https://github.com/ivoa-std/MANGO"
X4-010,Mireille Louys,"Strasbourg University, ICube &CDS",Object Oriented Data Model strategy in the context of IVOA Table Access Protocol services,"Object Oriented Data Model strategy in the context of IVOA Table Access Protocol services

François Bonnarel (CDS, ObAS, CNRS Strasbourg), Mireille Louys (CDS, ICube, Unistra), Laurent Michel (ObAS, CNRS Strasbourg) 

Table Access Protocol is an universal access standard for astronomical relational databases . TAP services registered in the IVOA registry, currently expose more than 22 thousands tables. 

It makes use of an astronomy extended SQL-like query language called ADQL. The database content is described in a standardized schema called the TAP_SCHEMA. Rich standardized metadata are included in the description. 
TAP services build up a query response in one single table. They extract data from the database following the structure given in the TAP_SCHEMA but provide only the selected columns. 

Beside this, IVOA developed a set of standardized object-oriented data models allowing a logical description of relationships between various pieces of data or metadata in astronomy. 
The elements queried in astronomical archive deal with photometry, coordinates, measurements, transformations, provenance and complex associations of those. 
These are described in various IVOA data models, following an Object Oriented strategy: Coords MEAS , Mango etc …. VO-DML is the IVOA recommendation to which IVOA data models should comply in order to be represented in a standard vodml-xml format. 

Mapping an object oriented-model in a relational database is a problem theoretically solved under the scope of ORM principles and rules. 

However the result of an ADQL query (as would be the case of the underlying SQL one in each specific database) is a single table. So in practice we are facing several issues for generating complex queries on one side, and for interpreation of the response on the other side.
The poster will review different solutions which have been experimented or considered by the IVOA DM WG in order to connect TAP responses (tables) with DMs instances (sub-graphs). The solutions vary according to the actual features of the data model and data structure. They should work when the database is following a predefined data model but also when we try to map a data model on legacy data. We will describe four possible solutions with examples:
- simple flat views built on top of the data model and the data. In that case the TAP schema covers the needs. An example is the ObsCore data model.
- on the fly table annotation In VOTable documents with a specific data model mapping syntax currently under development (ModelInstanceINVotable)
- organization of the ADQL query response as a normalized view with several tables and joins in the same VOTable documents with annotations delivered as quoted above.
- actual data model instance retrieval by new extensions of ADQL 

The poster will also show how TAP specification can be completed so that the TAP_SCHEMA contains metadata required to build table annotations or renormalisation"
B3-001,Dave Morris,University of Edinburgh,TAP and the Data Models,"TAP is one of the big achievements of the VO. This protocol gives any relational database a high level of interoperability thanks to several IVOA standards:
- The TAP_SCHEMA : a description of the tables, their columns and the way they can be joined.
- ADQL: a query language, subset of SQL, with some astronomy-specific features
- UWS: a specification for a REST API to be used to handle service requests
These features provide a common way to discover the content of TAP services and to query them.
This works very well with relational data and we propose to investigate the possibility for TAP services to map searched data on data models. Indeed several data models have been developed by IVOA in order to tackle the complexity of the relationships between astronomical data features. Among those we can quote Photometry Data Model, Coordinates, Measurements, Transforms or MANGO that is well suited to describe source properties and relations to some datasets representing these sources. TAP services are able to host complex data bound with joins but the standard still misses important features to serve real model instances:
- A meta-data endpoint telling which models are available per table
- Storage of model meta-data into the TAP_SCHEMA
- Storage of coordinate frames in the TAP service
- Mechanism specifying the model on which the requested data must be mapped
- Mechanism returning multi-table responses for complex objects
- Preservation of model annotations in uploaded tables 
The purpose of the BoF is to discuss the relevance of enabling TAP services to deal with Data Models and to refine the functionalities required to implement such a capability.

We will be able to present a proof of concept based on the VOLLT framework that can annotate on the fly query responses on two archive tables using the MANGO data model. Any other contribution and point of view on this topic will be is welcome and helpful to lift up and enrich the debate ."
T5-002,Dave Morris,University of Edinburgh,"Astropy, PyVO and the Radio realm","In the past years, VO standards were widely accepted and spread and many
astronomers get more and more familiar with standards and protocols like
Cone Search, SAMP and TAP/ADQL. Being skilled in these in combination with
Astropy and PyVO opens unlimited opportunities to find, to access and to
combine different datasets for further analysis. 

In this tutorial the participants will learn how to use PyVO and ObsTAP
to find services and to explore the data on these services, they will
use Daralink and SODA to perform cutouts on large images, and eventually
use PyVO and Astropy to write a SAMP handler to combine functionality of
TOPCAT and Aladin with PyvO. 
The cutouts will be performed on the Astron data service on data derived
from LOFAR. 
The cutouts will be performed on the Astron data service on LOFAR
derived data. 

We don't expect participants to be experts in above fields after this
tutorial, but to scratch on the surface of how useful these tools are and where to turn if they want to get deeper knowledge. In particular this should
include:

- TAP/ADQL and related special Obscore services
- PyVO for data discovery and data access
- How to read a standard document to develop against this standard
 (SAMP)

The used software will be: 
Python 3.6 (or newer), Astropy, PyVO, Topcat and Aladin. We will provide Python scripts in a github repository."
O1-005,Adam Zadrożny,National Centre for Nuclear Research,Natural language processing of astronomical transients reports,"Nowadays, a wealth of astronomical data is easily accessible online from machine-readable resources, such as, e.g., source catalogs or experimental data repositories. Many tools exist to extract, process and combine information from them automatically. On the other hand, there is also a considerable amount of important information hidden in the natural language sources. Particularly interesting are the GCN Circulars and Astronomers Telegrams issued shortly after an interesting transient event occurs, such as a supernova, gravitational wave, or neutrino alert. They are widely used by the multi-messenger community to inform about those events and the results of their follow-up and to plan further observations. A considerable amount of time is spent by the researchers searching, reading, and comparing information from those reports. Here we present a tool-set based on modern natural language processing techniques that allow for automatic extraction and processing of information from the GCN Circulars and Astronomers Telegrams. Data such as source name and coordinates, flux level, detection or non-detection, etc., can now be automatically obtained. The python-base code is easily adaptable to the individual researcher or experiment needs."
X7-003,Evgenii Rubtsov,SAI MSU,Hybrid minimization algorithm for computationally expensive multi-dimensional fitting,"Multi-dimensional optimization is widely used in virtually all areas of modern astrophysics. However, it is often too computationally expensive to evaluate a model on-the-fly for an arbitrary point in the parameter space. Typically, this problem is solved by pre-computing a grid of models for a predetermined set of positions in the parameter space, which are then interpolated between grid nodes to achieve continuous and differentiable behavior of the evaluated function required by gradient optimization methods. In case of complex models (e.g. spectra of stars or stellar populations) a significantly non-linear rapid change in the shape of a model lead to systematic artifacts hampering the minimization quality, e.g. by trapping a solution to some ""magic"" values. Here we present a hybrid minimization approach based on the local quadratic approximation of the chi2 profile from a discrete set of models in a multidimensional parameter space. The main idea of our approach is to eliminate the interpolation of models from the process of finding the best-fitting solution. The algorithm includes: (i) constructing a connectivity matrix for a grid of models; (ii) checking connected nodes and choosing a node with a minimum value of chi2 using a downhill/uphill climbing algorithm; (iii) finding an off-node solution from the approximation of chi2 values ​​at connected nodes with a positively definite quadratic form, (iv) determination of weights for the superposition of models at the minimum position. This approach allows us to deal with irregular multidimensional grids of models, provided there is a local basis for the required dimension. In addition to the discrete parameters of the models, one can minimize continuous functional parameters (such as Doppler radial velocities, rotational broadening for stars) at each tested node using standard gradient methods. The result is a simultaneous determination of discrete (for stellar spectra: Teff, log g, [Fe/H]) and continuous (for stellar spectra: v, vsini) parameters of the models. We present several examples of the applications of this minimization technique to the analysis of spectra of stars (VOXAstro stellar libraries) and galaxies (RCSED catalog) and compare them to ""standard"" approaches, which use interpolation in regularly spaced model grids -- they clearly demonstrate advantages of our method over standard techniques."
X3-005,Cristina Knapic,INAF -OATs,Resourcing provision: the INAF gateway,"Since the first SKA science data challenge, INAF provided storage resources to support the astronomical community. The second SKA data challenge was more demanding in therms of storage capacity and computation resources and INAF institution responded with the availability of two kind of computing resources allocated in different data centers. A unique registration portal, using a well consolidate methodology and support to reach both a remote desktop applications to use Virtual machines as well a computational cluster environment, was provided. The science INAF gateway, both in the user accounting creation and on usage of the remote desktops or bash accessare depicted."
X3-006,Cristina Knapic,INAF -OATs,The INAF-IA2 proposal to link data to papers,"Data reusability and science results enhancement are the mainstream directives of the international community, from the EU to world wide. To facilitate astronomical data finding, starting from a publication is nowadays facilitate by the usage of persistent identifiers like DOI or PIDs also for datasets, if recalled into the papers. Since INAF-ICT has its own DOI repository as well as the scientific data archives, to avoid a data duplication, a simbolic link to data is desirable. 
To reach this goal a collaboration during data set doi creation is requested to the astronomers in order to provide additional metadata to the DOI. In this way, both articles as well as archive datasets can be linked together with a really small additional effort.
The archive search result can provide a list of datasets that are in a automated way potentially, where all the metadata information are correctly filled, linked to the existing publications done with that data set or part of it.
This approach is a benefit both for archive miners than it has the aim to increase the data citation.
The architecture proposed is depicted and a discussion over it is encouraged."
X1-005,Alex Meshcheryakov,Space Research Institute (IKI),SRGz: machine learning system for physical characterisation of X-ray sources detected during SRG/eRosita mission,"I will present SRGz, the machine learning system created for optical identification, classification, and photometric redshift measurement of extragalactic X-ray sources (quasars, galaxies, galaxy clusters) detected during the SRG/eRosita All-Sky Survey. 

The practical difficulties of applying empirical ML-models to all major steps in the automatic analysis of X-ray source characterisation will be shown. I will discuss bayesian probabilistic models for multiwavelength cross-match of X-ray sources, tree-based ensembles and ANN models for optical counterpart classification and models for probabilistic photometric redshift determination. The accuracy of machine learning models depends on many factors, chief among which are the quality and diversity of available wide photometric data and training samples. 

We have built precise machine-learning models for detailed characterisation of the physical properties of X-ray sources and extensively tested them on the unique data from 3 half-year X-ray scans of eRosita All-Sky Survey in the Eastern Galactic Hemisphere."
O6-001,Kirill Grishin,SAI MSU,"Observing, calibrating and reducing near-infrared imaging mosaics","Coadding and mosaicking of astronomical imaging datasets allow us to investigate low-surface brightness features of extended objects such as galaxies, nebulae, comets etc. However, such observations in near-infrared bands require specific dithering patterns to tackle imperfections of NIR detectors and the reduced images should have a sufficiently high calibration quality (flat fielding, background subtraction) to be co-added and combined. Here we present a complete workflow for obtaining imaging mosaics with the MMT and Magellan Infrared Spectrograph (MMIRS) currently operated at the 6.5-m MMT in Arizona and open-source tools developed additionally to an existing pipeline for preparation and data reduction of mosaic observations. We describe pre-observing actions, such as design of dithering patterns and mosaic layouts and post-processing steps to perform absolute astrometric and photometric calibration, and also generate HiPS maps to display final product in Aladin / Aladin Lite. We developed a Python module that constructs dithering patterns, which can include several nearby fields to minimize overheads needed to move telescope between the fields. Post-processing tools for reduced images were developed using dedicated Python libraries and third-party software. They allow us to identify sources on images using either Photutils library procedures or SExtractor output, perform photometric measurements, match the list of sources with the reference catalogs. In our packages we use Pan-STARRS DR2 and GAIA DR2 catalogs for astrometric calibration procedure and 2MASS and UKIDSS catalogs for photometric zeropoint determination. To minimize storage space needed for reference catalogs instead of using whole datasets locally we have implemented ""on-the-fly"" generation of the required catalog subsets using wrappers of VO services implemented in PyVO. We have applied these tools to the imaging data obtained with MMIRS in J, H and Ks bands for a Coma cluster including its central dense regions with large galaxies, which make ""standard"" sky subtraction technique used in NIR imaging inaccurate. However, the results of our data processing do not display any significant sky subtraction artifacts. Our approach can be recommended for use for imaging surveys at large telescopes, which need to cover large areas of the sky including those done with instruments, which use multi-chip mosaic detectors."
X0-009,Kshitij Thorat,University of Pretoria,CARACal : Pipelining Big Data to Big Science,"With new-generation radio astronomical instruments like MeerKAT, ASKAP and LOFAR reaching stability of operations, radio astronomy can be said to have truly arrived in the age of Big Data. Multi-TB datasets produced by such telescopes have made automated data processing pipelines a necessity for a community historically used to interactive, manual data processing. 

CARACal (The Containerised, Automated Radio Astronomy Calibration Pipeline) is such an initiative, undertaken by a team of radio astronomers and computer scientists at varying academic levels and from different academic institutions. Originally envisaged to process data for the MeerKAT Large Survey Projects of Fornax and MHONGOOSE surveys, it is now used by individual as well as teams of astronomers to successfully reduce data; not only from MeerKAT but also from the uGMRT and VLA telescopes, contributing to novel discoveries in radio astronomy since its public release in early 2020. 

Built around the STIMELA pipelining framework, which employs Docker and Singularity container technology to seamlessly bring together data processing software from different suites; CARACal is also flexible enough to allow usage of custom, in-house tools. Since it is based on container technology CARACal provides reproducibility for members of global collaborations. 

In this talk, given on behalf of the CARACal developers team, I’ll use the CARACal pipeline to demonstrate and discuss the needs of pipelines in the era of big data in radio astronomy."
O7-009,Victoria Toptun,Sternberg Astronomical Institute,Homogenization of multi-wavelength photometric data for RCSEDv2,"RCSEDv2 (https://rcsed2.voxastro.org/), the second Reference Catalog of Spectral Energy Distributions of galaxies includes the largest homogeneously processed photometric dataset for 4 million galaxies assembled from several wide-field surveys. Rather than providing a collection of original measurements like NED or SIMBAD, we provide converted measurements in a form which allows one to compare data from different surveys directly. Here we describe the methodology of the photometric data homogenization. The original measurements come from different surveys: GALEX and the OM catalog of XMM-Newton (near-UV); SDSS, DESI Legacy Surveys, Dark Energy Survey, PanSTARRS, VST ATLAS, SkyMapper, KIDS (optical); UKIDSS, VHS, UHS, VIKING (near-IR); CatWISE and unWISE (near/mid-IR). They all have measurements in different photometric systems; (quasi-)integrated fluxes of extended sources are measured using various methods, e.g. Petrosian vs Kron vs asymptotic fluxes; aperture fluxes are measured in a set of specific apertures for each survey. Finally, galaxies span a range of redshifts from 0 to 1 and one has to apply k-corrections to be able to compare SEDs of sources at different redshifts. We first correct all photometric measurements for the foreground Galactic extinction, then convert them into the photometric system we adopted as a standard (GALEX + SDSS + UKIDSS + WISE). We worked out our own photometric system conversions by using a subset galaxies with photometric data available in several surveys. We computed aperture corrections into several pre-defined apertures by using published galaxy sizes / light profiles and image quality for each of the surveys. We also computed k-corrections using our own analytic approximations (the older version of this tool is available as standard functions within TOPCAT). Such a homogeneous photometric catalog allows us to build fully calibrated SEDs for the galaxies in our sample (defined by the availability of their spectra) and enables direct scientific analysis of this unique extragalactic dataset."
D3-001,Bonnarel François,CDS/ObAS CNRS UNISTRA,CASSIS and Aladin interfaced for a VO-compliant spectral data cube analysis tool,"CASSIS and Aladin interfaced for a VO-compliant spectral data cube analysis tool

J.M Glorian, P. Fernique, T.Boch, M.Boiziot, F.Bonnarel, C.Bot, S.Bottinelli, E.Caux, A.Coutens, M.Louys, C.Vastel

Spectral cubes are becoming usual data products in astronomy. This is true in various spectral domains due to the high rate data production of large projects such as MUSE in optical, LOFAR, ALMA, VLA, NOEMA or ASKAP in radio astronomy, or Chandra and XMM in Xray. And this is only a hint of what will happen with the emergence of SKA or other Petascale projects in a near future. These cubes are generally on line and easy to be found and accessed due to the great number of VO services which distribute them. Efficient Display and Analysis of such spectral cubes is a big challenge.

In this context the CASSIS team at IRAP (http://cassis.irap.omp.eu) and the Aladin team at CDS (https://aladin.u-strasbg.fr/) decided to work together on the combination of their VO applications in order to create a tool able to explore both spatial and spectral dimensions of the cubes.

CASSIS is a java tool able to discover spectra in remote services via the SSAP protocol and analyse them. It provides functionalities such as spectrum display, spectral line identification, prediction of spectra from any telescope, comparison of spectra with various models and determination of the physical parameters of the sources.

Aladin is also a java tool able to discover images and cubes and display them (either in standard bitmap format or in the IVOA HiPS format (https://www.ivoa.net/documents/HiPS/) and catalogs available in the Virtual Observatory landscape. It allows transformation, overlays and comparison of data. Data discovery makes use of specific VO features such as MOCs (https://www.ivoa.net/documents/MOC/20210324/index.html) both in spatial and time dimension.

The focus demo will show how the two Desktop applications have been extended by a common dedicated interface in such a way that they behave together like a spectral cube analysis tool. For example CASSIS can analyse a spectrum built on the fly by Aladin after hand selection and combination of voxels in a specific area of a spectral cube. Reversely CASSIS allows to select spectral ranges on such spectra and ask Aladin to display 2D images combining the corresponding spectral planes in the cube.

The tool can work both on local data available on the user's disk or on cubes discovered via the VO registry and within VO services. We will demonstrate both modes.

More sophisticated developments will occur in the future and will be announced at the end of the demo."
X3-010,Bonnarel François,CDS/ObAS CNRS UNISTRA,Annotating TAP responses on-the-fly against an IVOA data model,"With the success and widespread of the IVOA Table Access Protocol (1) for discovering and querying tabular data in astronomy, about one hundred of TAP services exposing altogether 22 thousands of tables are accessible from the IVOA Registries at the time of writing.
Currently the TAP protocol presents table data and metadata via a TAP_SCHEMA describing the served tables with their columns and possible joins between them.
We explore how to add an information layer, so that values within table columns can be gathered and used to populate instances of objects defined in a selected IVOA data model like Photometry, Coords, Measures, Transform or MANGO. This improves data interoperability by adding a model view common to all services. 
This information layer is provided through annotation tags, that are defined by the service which tells how the columns' values can be interpreted as attributes of instances of that model. 
Then when a TAP query is processed, our server add-on interprets the ADQL query string and produces on-the-fly, when possible, the TAP response as a VOTABLE document, decorated with the annotation of the object's fields present into the query and corresponding to the model elements templated for this service. 
This has been prototyped in Java, using the VOLLT (2) package library and a template annotation document representing elements from the MANGO(3) data model. This has been exercised on examples based on Vizier and Chandra catalogs.

(1) https://www.ivoa.net/documents/TAP/20190927/

(2) https://github.com/gmantele/vollt

(3) In development at https://github.com/ivoa-std/MANGO"
X4-010,Bonnarel François,CDS/ObAS CNRS UNISTRA,Object Oriented Data Model strategy in the context of IVOA Table Access Protocol services,"Object Oriented Data Model strategy in the context of IVOA Table Access Protocol services

François Bonnarel (CDS, ObAS, CNRS Strasbourg), Mireille Louys (CDS, ICube, Unistra), Laurent Michel (ObAS, CNRS Strasbourg) 

Table Access Protocol is an universal access standard for astronomical relational databases . TAP services registered in the IVOA registry, currently expose more than 22 thousands tables. 

It makes use of an astronomy extended SQL-like query language called ADQL. The database content is described in a standardized schema called the TAP_SCHEMA. Rich standardized metadata are included in the description. 
TAP services build up a query response in one single table. They extract data from the database following the structure given in the TAP_SCHEMA but provide only the selected columns. 

Beside this, IVOA developed a set of standardized object-oriented data models allowing a logical description of relationships between various pieces of data or metadata in astronomy. 
The elements queried in astronomical archive deal with photometry, coordinates, measurements, transformations, provenance and complex associations of those. 
These are described in various IVOA data models, following an Object Oriented strategy: Coords MEAS , Mango etc …. VO-DML is the IVOA recommendation to which IVOA data models should comply in order to be represented in a standard vodml-xml format. 

Mapping an object oriented-model in a relational database is a problem theoretically solved under the scope of ORM principles and rules. 

However the result of an ADQL query (as would be the case of the underlying SQL one in each specific database) is a single table. So in practice we are facing several issues for generating complex queries on one side, and for interpreation of the response on the other side.
The poster will review different solutions which have been experimented or considered by the IVOA DM WG in order to connect TAP responses (tables) with DMs instances (sub-graphs). The solutions vary according to the actual features of the data model and data structure. They should work when the database is following a predefined data model but also when we try to map a data model on legacy data. We will describe four possible solutions with examples:
- simple flat views built on top of the data model and the data. In that case the TAP schema covers the needs. An example is the ObsCore data model.
- on the fly table annotation In VOTable documents with a specific data model mapping syntax currently under development (ModelInstanceINVotable)
- organization of the ADQL query response as a normalized view with several tables and joins in the same VOTable documents with annotations delivered as quoted above.
- actual data model instance retrieval by new extensions of ADQL 

The poster will also show how TAP specification can be completed so that the TAP_SCHEMA contains metadata required to build table annotations or renormalisation"
X7-004,Anastasia V. Kasparova,"Sternberg Astronomical Institute, Moscow State University",Analytic approximations of K-corrections for galaxies out to redshift z=1,"Investigations of galactic evolution require a comparison of their photometric properties at different redshifts. For the purpose we need to correct the fluxes for the changes of effective rest-frame wavelengths of filter bandpasses, called K-corrections. At redshifts exceeding z=0.3, the wavelength shift becomes so large that typical broadband photometric bands shift into the neighboring rest frame band (e.g. observed SDSS r into rest frame SDSS g). At z=0.6--0.8 the shift reaches two or three bands (e.g. SDSS i/z into SDSS g). Therefore, we need perform K-corrections from one observed bandpass to another, so called cross-band K-corrections. Here we expand the methodology proposed by Chilingarian et al. (2010) and fit cross-band K-corrections by smooth low-order polynomial function of one observed color and a redshift -- this approach but without cross-band is implemented as standard functions in TOPCAT which can be used for galaxies at z<0.5. We also computed analytical approximations for WISE bands, which were not available in the past. Finally, we now have a complete set of K-corrections coefficients, which allow us to process photometric measurements for galaxies out to redshift z=1. We calculated standard and cross-band K-corrections for about 4 million galaxies in second Reference Catalog of Spectral Energy Distributions (RCSEDv2) of galaxies and we showed that, in cases of widely used UV, optical and near-infrared filters, our analytic approximations work really well and can be used for extragalactic data from future wide-field surveys."
X3-007,Vladimir Goradzhanov,Sternberg Astronomical Institute,A massive full spectral fitting of the RCSEDv2 dataset,"RCSEDv2 (https://rcsed2.voxastro.org/), the second Reference Catalog of Spectral Energy Distributions of galaxies, provides the largest homogeneously analyzed collection of optical galaxy spectra originating from several ground-based surveys collected between 1994 and 2019. Despite different signal-to-noise ratios, wavelength coverage, spectral resolution, aperture sizes, we have successfully re-processed and analyzed over 4 million spectra from SDSS/eBOSS, Hectospec public archive, LAMOST, 2dFGRS, WiggleZ, DEEP2/DEEP3, FAST public archive, and 6dFGS in the redshift range between z=0.0 and 1.0. For each spectral survey we created a post-processing routine to perform a conversion of spectra into a Virtual Observatory compatible format; telluric correction if needed; relative flux calibration (everywhere except SDSS/eBOSS) using spectral sensitivity curves, which we constructed using either spectrophometric standard star observations or well-calibrated spectra from SDSS/eBOSS for a subset of overlapping sources; absolute flux calibration using broad-band flux measurements in the matching apertures. Then we fitted each spectrum using the NBursts full spectrum fitting technique, that yielded stellar population and emission-line properties. To re-process 4 million spectra in case something changes in the fitting algorithms / initial guesses / model grids, we developed a parallel infrastructure which allows us to re-fit the entire dataset in a matter of a few days on a small computational cluster at Sternberg Astronomical Institute. The main products of this work are: (i) a catalog of stellar population properties (radial velocity, velocity dispersion, age, metallicity, alpha-element enhancement) and emission-line fluxes, shapes, and widths for every spectrum available using IVOA TAP; (ii) flux calibrated telluric corrected spectra and their best-fitting templates (stellar absorptions and emission lines) available using IVOA SSAP."
O3-006,Vladislav Klochkov,"Department of Physics, Moscow State University; Sternberg Astronomical Institute",Open-source web tools for visualization of imaging and spectral data in RCSEDv2,"The rapid development of Virtual Observatory technologies raises demand for customizable and easily embeddable data visualization tools, which can acquire data from the VO and be used as web components across different projects. We present a set of open-source web-tools for visualization of spectral and imaging data, which we use in the second Reference Catalogue of Spectral Energy Distributions of galaxies RCSEDv2 (https://rcsed2.voxastro.org/). Using modern web frameworks Quasar and Vue.js we developed interactive viewers providing interactive visualization for spectra and SEDs of galaxies and diagrams presenting emission line ratios determined from the analysis of their spectra (BPT diagrams). The viewers are built in Javascript which puts a minimum load on the server side while providing full interactivity for the user. The use of modern web frameworks provides full customization making the viewers easily embeddable into web-sites of astronomical archives and databases. It also provides compatibility with popular third-party web-tools such as Aladin Lite."
O2-003,Sviatoslav Borisov,"Department of Astronomy, University of Geneva; Sternberg Astronomical Institute of Moscow State University",Multi-segment and Echelle stellar spectra processing issues and how to solve them,"Modern astrophysics faces new challenges that require high-quality spectral data. Stellar spectra are a crucial component that helps us to understand properties of individual stars as well as stellar populations in star clusters and galaxies. A significant fraction of state-of-the-art spectrographs are cross-dispersed Echelle and/or have multiple setups (segments), which are combined together. This approach covers longer wavelength range at high spectral resolution but makes data reduction and post-processing non-trivial. Here we describe how to bring multi-order/multi-segment spectra to perfection by identifying various issues that one could face when working with such spectra and methods of solving them. We use examples of NIR intermediate-resolution Echelle spectra from Folded InfraRed Echellete (R~6500, Magellan Baade) and high-resolution UV-optical spectra observed with UVES (R~80000, VLT). For the UVES spectra, we developed an algorithm for the correct stitching of echelle orders which eliminates ""ripples"" in regions where they overlap; this algorithm has been successfully integrated into the processing sequence consisting of procedures provided by ESO. For the FIRE spectrograph, we developed the entire data processing pipeline from scratch. The key issue is to understand how the general principles of the optical design (telecentricity violations, flexures) can affect the final data product when using standard internal calibration frames. Multi-segment spectra can suffer from another, fully astrophysical factor -- if different setups were obtained at different epochs and the source is variable (binary/multiple and/or pulsating star), they must not be merged together and processed fully independently; this can be tested using Virtual Observatory access to publicly available catalogs (e.g. using CDS Vizier). Finally, we use VO to collect all available broad- and middle-band photometry, which we then use to perform the global spectral sensitivity correction of final merged spectra. By using these dedicated algorithms, we were able to achieve the quality of the global spectrophotometric calibration of 1--2%, which fulfill the requirement for stellar spectra intended to be used in stellar population synthesis."
X4-003,Mariia,Moscow Institute of Physics and Technology (National Research University),"Optical Variability of ""Light-weight"" Supermassive Black Holes at a Few Percent Level from ZTF Forced-Photometry Light Curves.","Active galactic nuclei (AGN) are variable on different timescales which scale linearly with the mass of the central black hole. Optical variability is considered among the most reliable selection criteria for unobscured AGN. The light curve is a time series of flux measurements, which can be derived from archival images. In this work, we consider a sample of more than 300 relatively weak AGN powered by ""light-weight"" supermassive black holes (SMBH) with masses up to 1e6 solar masses selected from archival X-ray observations and with the mass estimates from optical spectroscopy (H-alpha broad line with and flux). We use Zwicky Transient Facility (ZTF) data to detect optical variability at a few percent scale. ZTF Data Release aperture light curves cannot be used for our purpose because of systematic errors reaching 15-20% making detection of weak variability on top of bright host galaxies unfeasible. We describe how to build reliable light curves using the ZTF Forced-Photometry service, which uses difference images for flux estimates. Such light curves are much better already ""off-the-shelf"", because they are almost free of systematic errors introduced by the host galaxies and variable seeing conditions, however we need extra post-processing / filtering steps to further improve them and get rid of the remaining systematics. We set the threshold signal-to-noise ratio (SNR) to 3, points with a lower SNR were excluded. The distance to the nearest reference was set to less than 1 arcsec, pixel quality indicators helped to clear outliers related to uncorrected cosmic ray hits and hot pixels. The zero point correction was made for each observation, based on the CCD quadrant, and the average airmass of observations. A color correction was made using the zero color from the Pan-STARRS1 photometric system. For each filter, the ZTF field, CCD quadrant the light curves were studied. Then we defined the level of significance according to the chi-square criterion at which the light curve is not consistent with white noise on top of a constant. Many of the studied objects showed statistically significant variability on the timescales from days to months. Also, a strong flare with an amplitude of 0.25 mag was observed in the object J112637.74+513423.0 for about 3 months, which can be either a heavily dust obscured supernova explosion or a star rupture by tidal forces in the SMBH potential (tidal disruption event). The bottleneck limiting the scalability of our analysis and making it yet impossible to study tens of thousands of AGN is that the ZTF Forced-Photometry Service can process up to 100 objects at a time and it takes hours to weeks to process the data. Therefore, we first process the most astrophysically interesting sources then gradually expanding the sample."
X3-008,Igor Chilingarian,Center for Astrophysics -- Harvard and Smithsonian / Sternberg Astronomical Institute,RCSEDv2: the largest database of galaxy properties from a homogeneously processed multi-wavelength dataset,"Formation and evolution of galaxies and their central black holes remain some of the most important research areas in modern observational astrophysics. Despite active development of the international Virtual Observatory over the past two decades, the potential of existing archives and databases for astrophysical discovery remains largely unexplored. In 2012-2017 we created the Reference Catalog of Spectral Energy Distributions of 800,000 galaxies (RCSED, http://rcsed.sai.msu.ru/) which became the cornerstone of several successful projects devoted to search and study of rare astrophysical objects. Among others, using RCSED (i) we discovered the universal optical-UV color-color-magnitude relation of galaxies; (ii) we identified over 70% of known compact elliptical galaxies; (iii) we found a population of intermediate-mass black holes which power active galactic nuclei; (iv) we selected young analogs of ultra-diffuse galaxies and made firm conclusions regarding their formation and evolution. The three former projects were featured as ADASS tutorials in 2010, 2012, and 2015. The key difference between RCSED and existing databases of galaxy properties (NED, HyperLeda, part of SIMBAD) is that rather than providing a compilation of literature data, we perform homogeneous data analysis of spectral and photometric data using our own tools and publish derived physical properties of galaxies along with re-calibrated spectra and photometry and their best-fitting models. Now we are finishing the 2nd release of our catalog, RCSEDv2 (https://rcsed2.voxastro.org/) where we substantially expanded the spectral dataset from 800k to 4 million objects by including spectral data analysis for 10 large spectroscopic surveys (SDSS, SDSS/eBOSS, LAMOST, Hectospec, CfA redshift surveys, 2dFGRS, 6dFGS, DEEP2/3, WiggleZ). The photometric part has been also significantly expanded by including DESI Legacy Survey, DES, UHS, ESO Public Surveys, and WISE in addition to GALEX, SDSS, and UKIDSS used in the original RCSED. This makes RCSEDv2 the largest database of galaxy properties and homogeneously processed spectral and photometric data up-to-date and creates a foundation for the analysis of future large-scale spectral surveys DESI and 4MOST. We have already used RCSEDv2 for astrophysical research and I will briefly review the recent highlights."
X9-003,Juande Santander-Vela,SKAO,SKAO challenges on distributed software development for SKAO: managing a 24h collaboration from a single time zone.,"The SKA Observatory (SKAO), the intergovernmental organisation created just in 2021 to create the observatory headquartered in the UK that will deliver the SKA Phase 1 telescopes in Western Australia and South Africa, gave the green light for construction in June 2021. The telescopes have a strong component of software delivery that is being organised in an agile way through the Scaled Agile Framework (SAFe®). This talk shows the progress in our software development, and the challenges that we are facing, and some of their solutions."
X0-010,John Swinbank,ASTRON,ESAP: The ESCAPE Science Analysis Platform,"The ESCAPE project is developing an array of services and capabilities which will enable fundamental data-driven research in astronomy, astrophysics, and particle physics. These include a “data lake”, a software and services repository, and deep integration with Virtual Observatory interfaces. ESAP provides the overarching structure that binds these capabilities into a coherent system, providing scientists with a unified interface to the ESCAPE services while seamlessly integrating them with interactive data analysis and batch computing facilities. Advanced prototypes of ESAP technologies are now available, and active development is ongoing. This talk will summarize the vision for the ESAP system, describe the flexible and extensible architecture which has been adopted, and demonstrate the results achieved to date."
X7-005,Geoff Duniam,University of Western Australia,Source finding with SoFiA and very large source files,"As the new generation of large scale scientific instruments generate ever larger astronomical data products, different approaches to source finding using well tested and very capable legacy applications will need to be developed. We present our findings into our investigation of the performance implications of utilising modelled FITs data versus the original source cube for the SoFiA-2 source finding application.

We present a methodology to utilise modelled data stored in a Parquet formatted table on a Hadoop system to extract image and header data from a 3 Dimensional spectral line image cube. We compare the performance of two pipelines running the SoFiA-2 source finding process against an 835 GB FITS, with one pipeline utilising the modelled data on Parquet and one pipeline accessing the original source FITS file via sub-region calls. We present our findings which demonstrate that the modelled data solution can provide significant performance benefits in the analysis of large scale FITS files."
X3-009,Anthony Heng,Australian,PyCPL: using Pybind to bring and adapt C libraries to Python,"Authors: Anthony Heng, Aidan Farrell, Anthony Horton, Nuria Lorente, Ralf Palsa, Lars Lundin, Katrina Sealey

The ESO Common Pipeline Library (CPL) comprises a set of ISO-C libraries that provide a comprehensive, efficient and robust software toolkit to develop astronomical data-reduction recipes, which has been the fundamental tool for building data reduction pipelines for ESO VLT instruments since its original release in 2003, and which will continue to be the basis for ELT instrument pipelines. CPL was developed in C for reasons of efficiency and speed of execution, and because of its maturity and widespread use, it is well tested and understood. However, as the astronomical community’s preference moves more and more towards Python as the language of choice for algorithm prototyping and data reduction, there has emerged a need to provide access to CPL for users who wish to make use of the power of CPL from a Python environment.
The PyCPL project aims to provide this access to CPL within Python. Compiled CPL structures and algorithms will be mapped to Python equivalent objects. This presents the opportunity to execute existing data reduction pipelines from within the Python environment, in addition to providing Python developers the opportunity to write their own pipelines using the existing CPL structures and algorithms, leveraging the efficiency, speed, and maturity of CPL from a pure Python ecosystem. Included with the PyCPL package will the PyEsoRex recipe execution module, which is a Python analogue of the EsoRex execution tool. PyEsoRex allows the user to run both existing C Recipes and new Python recipes and provides a standardised interface for Python recipe development.
In this talk we will introduce PyBind11: the core library that has allowed us to connect CPL to Python, and discuss how we have employed it in the project: what has worked “out of the box”, and what has needed customisation and additional C++ and Python code to produce a pyCPL which feels natural to CPL users, and attractive to those accustomed to a Python environment."
X9-004,Paulus Lahur,CSIRO,The latest updates in Yandasoft development,"Yandasoft is a suite of radio astronomy software developed by CSIRO Space and Astronomy for the calibration and imaging of Interferometric Radio Telescope data. It was originally intended for the Australian Square Kilometre Array Pathfinder (ASKAP) project, prior to its release as open source software. 
In this presentation we would like to talk about the latest updates in Yandasoft development. We have made significant progress in the functionalities of the software, as well as its overall reliability, performance, and ease of use. We will present the underlying improvement in software engineering practices that users do not see, such as the use of Cmake to orchestrate the building process of multiple Git repositories, as well as the deployment process using Docker and Singularity containers. 
We will also present how users can get Yandasoft and use its functionalities in their processing pipeline, which might run on workstation or HPC facilities."
X1-006,Jesús Vega Ferrero,Instituto de Física de Cantabria (IFCA-CSIC),Event detection and reconstruction using Neural Networks in TES devices: a case study for Athena/X-IFU,"Transition Edge Sensors (TES) detector devices, like the core of the X-IFU instrument that will be onboard the Athena X-ray Observatory, produce current pulses as a response to the incident X-ray photons. The reconstruction of these pulses aims at recovering the energy of the impacting photons, their arrival time and their physical position in the detector. This has been traditionally performed by means of a triggering algorithm based on the derivative signal overcoming a threshold (detection) followed by an optimal filtering (to retrieve the energy of each event). However, when the arrival of the photons is very close in time, the triggering algorithm is incapable of detecting all the individual pulses which are thus piled-up. In order to improve the efficiency of the detection process, we study here an alternative approach based on Machine Learning techniques to process the pulses. For this purpose, we construct and train a series of Neural Networks (NNs) and we use a hyper-parameter Bayesian optimization to select the optimal NN architecture. This method is tested not only for the detection but also for the recovering of the arrival time and the energy of simulated X-ray pulses. The dataset used to train the NNs consists of simulations performed with sixte/xifusim software package, the Athena/X-IFU official simulator. The performance of our NN classification clearly surpasses the detection performance of the classical triggering approach for the full range of photon energy combinations showing excellent metrics. However, the reconstruction efficiency for the recovery of the energy of the photons cannot currently compete with the standard optimal filtering algorithm."
X3-011,dongliang,"Yunnan Observatories, Chinese Academy of Sciences",A friendly solar radio multi-channel flux monitoring and controlling software,"Solar Radio Burst (SRB) event is a kind of potential interference to wireless communication systems especially the satellite Navigation system such as GPS, BD and so on. Nowadays we have to monitoring the solar flux to pre-alarm these interference events. 
 So we build a new solar radio flux monitoring telescope working at L and S band which is the important Navigation band. This telescope can get eighteen channels ( nine 10MHz frequency bands with to polarities ) once in real-time. 
 Furthermore, we developed a friendly monitoring and controlling software which can set and change the observation parameters flexibly. Some key parameters can be set and change in a graph interface such as integrate time, calibration time, sampling rate. We also can set the start and end times in graph interface based on different seasons. 
 And then all the channels’ flux can be got and stored in real-time. And they are also can be clear watched from the front panel.
 After 54 days observing, the correlation index between 2840±5MHz and Sunspot Area is reached 0.93 which is an important data of space weather."
X9-007,Zhelenkova Olga,Special Astrophysical Observatory of RAS,"The prototype of the information system ""Parent Galaxies of Radio Sources""","We have been studying radio sources in different spectral ranges for a long time - in the radio, optical, infrared, X-ray ranges, and have formed a large collection of heterogeneous data that are supposed to be used for further studies of the properties of the parent galaxies of radio sources. Spectral information for each source is not complete and additions from newly published catalogs are required.
For effective operation, an information system is required, which will include a database on radio sources and their parent galaxies, a web interface for displaying and changing various information about objects. When comparing data from different sources, information about the quality and accuracy of data, about the origin and version of the catalog, its coverage area, bibliography, etc. is important.
We started developing a prototype of an information system that will help us manipulate data efficiently and conveniently. The system should allow scaling the database schema when adding new catalogs, integrating heterogeneous data, and updating information about astrophysical objects when adding new catalogs.
The database schema includes tables of source catalogs, add-ons tables that include additional material about the properties of sources based on the results of their identification, and tables that implement links ""component-source"". The schema includes tables with metadata from surveys and catalogs used to add new surveys that will be used to study radio sources."
X9-005,Dirk Petry,European Souther Observatory,Using script generators for pipeline prototyping,"Fully automated astronomical data calibration and imaging pipelines are difficult to develop without a good prototyping method which permits to bridge the time between observatory commissioning and the moment when the special features and possible problems of the data and their processing are fully understood.
In this paper I present a method which has worked well for the ALMA observatory and which is sufficiently
general to be transferable to most other projects. In short, the idea is to use a three-level data analysis software design (low-level scriptable toolkit, script generator, autonomous pipeline)
and a corresponding timing of the software development which is ramping up the effort in three stages starting at the beginning of construction, at the beginning of commissioning, and at the end of commissioning respectively.
The important design pattern which I would like to underline here is the use of script generators
as prototypes for the autonomous pipeline."
O0-005,Sphesihle Makhathini,University of the Witwatersrand,Stimela 2.0: containerization and workflow management for data pipelines,"The stimela software package (Makhathini 2018) was designed to address a glaring problem in radio astronomy software, namely, the rigid nature of data reduction pipelines, and the heterogeneous and often conflicting dependencies of the software packages involved. Stimela is a containerization and workflow management framework. More specifically, it addresses the cumbersome process of having to craft the perfect computing environment one needed to interface more than one software package for a particular data reduction. Stimela leverages container technology, as well as a modular Python-based approach, to provide a unified and simple interface from which to access a plethora of radio astronomy software packages. This has allowed users to focus on optimising their pipelines instead of fine-tuning computing environments. Examples of stimela deployment “in the field” include the CARACal and VerMeerKAT pipelines, which are two of the pipelines capable of producing quality end-to-end MeerKAT reductions. 

Three years later, stimela is ready for a second major release (stimela2) which provides a YaML-based syntax for composable processing recipes, much greater flexibility in software installations (supporting a mix of direct binary installations, virtual environments, and containers), as well as distributed pipelines that can be seamlessly deployed on most cluster infrastructures."
X7-006,Pau Tallada Crespí,CIEMAT - PIC,"Spark-based fast approximate method for computing simulated galaxy fluxes, applied to Euclid and PAU photometric surveys","The standard approach to estimate galaxy light fluxes from cosmological simulations is a complex algorithm that requires solving several integrals. For very large simulations, the computational cost of producing those fluxes for every wavelength band (filter) of a photometric survey can become prohibitive.

This work describes the approximate method that has been designed and developed to estimate the observed light fluxes reducing considerably the computational time and with the precision imposed by scientific constraints. The method has been successfully applied to compute galaxy fluxes in two relevant scientific cases: the Euclid Flagship mock galaxy catalog, and the Physics of the Accelerating Universe Survey (PAUS) galaxy simulations. This method consists in refactoring and splitting the standard algorithm based on integrals, in three independent steps: the first step consists on the calculation of the flux from the galaxy Spectral Energy Distribution (SED) including the intrinsic light extinction, the second adds the spectral emission lines contribution (also extincted), and the third corrects for the local light extinction caused by the Milky Way. The new approximate method uses a grid over the parameters space considered. In the first two steps, the method consists in building and then using for interpolation a parameter grid that covers all the range in the parameters: Spectral Energy Distribution (SED) extinction curve, object's color excess (ebv), and redshift (z). The binning of the grid has been carefully optimized to achieve the precision required by each scientific case. In the third step, the Milky Way extinction is approximated as a constant factor within the wavelength interval considered.

The new algorithm has been implemented using Apache Spark to be able to take advantage of distributed computing resources. It has been tested and executed at the Port d'Informació Científica (PIC) Big Data platform. Running in the PIC infrastructure, it has been possible to reduce the time to compute the observed flux for each object in any band, to 47 microseconds on average, more than 700 times faster than the integral method. The approximate method has been applied to calculate the observed flux from 1 billion sources in 30 different broad bands of the Euclid simulation (including the visible imager (VIS) and the near infrared 3-filter (Y, J and H) photometer (NISP-P) instruments) in less than 80 minutes using a computing platform of 300 cores. The difference in galaxy magnitude between the calculation with the integrals method and the approximated one is ∆(m)<0.01 for 99.8626% of the simulated galaxies, fulfilling the scientific requirement imposed by the Euclid project. The method has also been optimized and used to simulate the observed flux for a COSMOS-like galaxy sample in the PAUS 40 narrow band filters, fulfilling the survey requirement ∆(m)<0.01 for 99.9996% of the simulated galaxy sample."
I7-002,Andy Connolly,University of Washington,Scaling Science in the Era of Survey Astronomy,"In an era of astronomical surveys and experiments capable of generating 10s to 100s of petabytes of data a year, we face are facing the question of how we maximize the scientific potential of these billion dollar investments. We design and build experiments with sufficient computational resources to acquire, process and store the observational data they generate. We do not, however, apply to the same thought and effort to developing the software and computational infrastructure necessary for large-scale scientific analyses of data sets that are often complex, noisy and incomplete. In this talk I will discuss how we might address some of these challenges through a combination of emergent techniques and methodologies, together with lessons we can learn from other fields facing similar issues. Partial solutions will come from novel computational architectures (GPUs and FPGAs), analytics frameworks designed to run across thousands of processors (e.g. Spark and Dask), and algorithmic advances in machine learning and statistics (e.g. deep learning). To fully realize the potential of large scale survey astronomy will, however, also require changes in how we educate and train our community, how we share resources and expertise between institutions, and in particular how we integrate professional software engineering within our research infrastructure."
T3-001,Raul Infante-Sainz,Centro de Estudios de Física del Cosmos de Aragón (CEFCA),Gnuastro hands-on tutorial for astronomical data analysis,"Gnuastro is an official GNU package of a large collection of programs to enable easy, robust, and most importantly fast and efficient, data analysis directly on the command-line. For example it can perform arithmetic operations on image pixels or table columns/rows, visualize FITS images as JPG or PDF, convolve an image with a given kernel or matching of kernels, perform cosmological calculations, crop parts of large images (possibly in multiple files), manipulate FITS extensions and keywords, and perform statistical operations. In addition, it contains
programs to make catalogs from detection maps, add noise, make mock profiles with a variety of radial functions using monte-carlo integration for their centers, match catalogs, and detect objects in an image among many other operations. Gnuastro is written to comply fully with the GNU coding standards and integrates well with all Unix-like operating systems. This enables astronomers to expect a fully familiar experience in the building, installing and command-line user interaction that they have seen in all the other GNU software that they use (core components in many Unix-like operating systems). Gnuastro's extensive library is also installed for users who want to build their own unique/custom programs."
T3-001,Zahra Sharbaf,,Gnuastro hands-on tutorial for astronomical data analysis,"Gnuastro is an official GNU package of a large collection of programs to enable easy, robust, and most importantly fast and efficient, data analysis directly on the command-line. For example it can perform arithmetic operations on image pixels or table columns/rows, visualize FITS images as JPG or PDF, convolve an image with a given kernel or matching of kernels, perform cosmological calculations, crop parts of large images (possibly in multiple files), manipulate FITS extensions and keywords, and perform statistical operations. In addition, it contains
programs to make catalogs from detection maps, add noise, make mock profiles with a variety of radial functions using monte-carlo integration for their centers, match catalogs, and detect objects in an image among many other operations. Gnuastro is written to comply fully with the GNU coding standards and integrates well with all Unix-like operating systems. This enables astronomers to expect a fully familiar experience in the building, installing and command-line user interaction that they have seen in all the other GNU software that they use (core components in many Unix-like operating systems). Gnuastro's extensive library is also installed for users who want to build their own unique/custom programs."
X3-012,Eva Sciacca,INAF Osservatorio Astrofisico di Catania,Onboarding SPACE services to the European Open Science Cloud,"The H2020 NEANIAS project is implementing novel thematic services, addressing emerging challenges in underwater, atmospheric and space sciences, to be delivered to the European Open Science Cloud (EOSC). We report on the current development status of distinct clusters of highly interdisciplinary efforts for materialising EOSC services for the Astrophysics and Planetary communities focusing on visualization and data management (SPACE-VIS), map making and mosaicking (SPACE-MOS) and structure detection with Machine Learning (SPACE-ML). Our services support a collaborative research EOSC ecosystem underpinning open science practices by injecting add-on value to datasets and processing workflows. The SPACE-VIS ViaLactea, Astra Data Navigator and ADAM-SPACE services allows visual navigation, exploration and analysis of the most updated surveys and catalogues of our Galaxy, the Milky Way, and of the Solar System planets. The SPACE-MOS ViaLactea Knowledge Base and ADAM-DPS services provide backend tools to merge adjacent datasets and produce planetary cartographic products. Finally the SPACE-ML CAESAR and AstroML services extract and parametrise sources from astronomical radio interferometric maps integrating deep learning mechanisms to improve source identification and classification of sources in large-scale radio surveys. We showcase examples of real-world operational scenarios detailing on underlying challenges and outline valuable lessons learned in maturing existing technological solutions to TRL 8 in order to obtain robustly delivered FAIR (findability, accessibility, interoperability and reproducibility) cloud services populating the emerging EOSC."
X1-008,Ondřej Podsztavek,"Faculty of Information Technology, Czech Technical University in Prague",Prototype of Interactive Visualisation Tool for Bayesian Active Deep Learning,"In the era of big data in astronomy, we need to develop new methods to analyse the huge amount of astronomical data.

We are currently developing one such method: Bayesian active deep learning. The method combines Bayesian convolutional neural networks and active learning. The Bayesian network is trained to make predictions in a given task and provide predictive uncertainty. Therefore, we know which predictions the Bayesian network is uncertain about. In the active learning phase, a human expert (astronomer) labels a subsample of data that have the highest predictive uncertainty. The initial training set of the Bayesian network is extended with the labelled subsample. Then, the Bayesian network is re-trained, and the whole process repeats until we are satisfied with the performance of the Bayesian network.

To improve the method’s performance, we have developed a prototype of an interactive visualisation tool for a selection of an informative subsample of data for labelling. We want a subsample of data that will improve the Bayesian network’s performance as much as possible. That means we need a subsample that (1.) contains data with high predictive uncertainty, (2.) is diverse, (3.) but not redundant. A correctly selected subsample of data for labelling is essential for active learning because it can speed up the method. After all, labelling by a human expert is often time-consuming and expensive.

We present a tool for Bayesian active deep learning that is implemented in Python as a widget for Jupyter Notebook. The tool takes as an input a sample of data with the highest predictive uncertainty. These data are projected to 2-D with a dimensionality reduction technique (i.e. principal component analysis, t-distributed stochastic neighbor embedding, or uniform manifold approximation and projection). We visualise the projected data in an interactive scatter plot and allow a human expert to label a selected subsample of data.

With this tool, a human expert can select a correct subsample with all the previously mentioned characteristics: high predictive uncertainty, diversity, non-redundancy. This should lower the total amount of data labelled because the Bayesian network’s performance will improve faster than when the data are selected automatically."
T5-001,Agata Trovato,University of Trieste,Accessing and working with gravitational-wave open data from LIGO and Virgo,"The breakthrough discovery of gravitational waves (GW) has opened the era of GW astronomy. A total of 50 GW signals have been detected until now from the data of the first two observing runs, O1 and O2, and the first six months of the third observing run (O3a). The LIGO and Virgo data that correspond to these events have been released via the Gravitational Wave Open Science Center (GWOSC) at the web address gw-openscience.org. This tutorial will provide a learning path to future users that are not acquainted with the specificities of GW data. The users will have an overview of the content of the GWOSC website: which data are available and where to find them. They will learn to navigate the website, download and view the data using Python libraries such as gwosc and gwpy. They will learn the basics of the GW analysis, how to read the available analysis products such as skymaps and posterior samples, and how to make useful plots."
T7-001,Kuo-Song Wang,ASIAA,Visualising big data: how to get the best out of CARTA,"CARTA is a major visualization tool used by astronomers worldwide, architected for use with big image data sets. There have been 7 major public releases plus beta and patch releases to date and v2.0 was released on 4 June 2021. Check https://cartavis.org/ for more details about CARTA. 

In the CARTA tutorial, we plan to have five topics that help users to master CARTA for their own science. For a given topic, the first 5-10 minutes are reserved for the tutors to introduce the ideas with slides and live demonstrations, and the rest of the time is for hands-on practice. The final 10 minutes are dedicated to Q&A.

At the end of this CARTA tutorial learners will have grasped:
- How to set up the CARTA environment
- How to manipulate major features of CARTA in order to fulfil their science objectives for
their data sets
- How to use the help facility and report bugs.

Prerequisites
1. A laptop/desktop computer with internet connection
2. (Optional) CARTA pre-installed on your system. We will also provide a test server that
you can simply log on and use the demo images for the tutorials
3. A set of images that you are familiar with based on your research expertise if you decide
to install CARTA on your computer. A list of recommended kinds of image/catalog files
you should prepare for will be provided.
4. (Optional but recommended) Download our demo dataset if you decide to install CARTA
on your computer"
T9-001,Mateusz Malenta,The University of Manchester,Efficient and safe astronomy research with Docker,"Modern research is already highly reliable on containers and this reliability is only going to increase over the coming years. Some of the most common software stacks are already distributed in their containerised form and many more are soon going to join them.
The use of containerised research software is especially important in the context of reproducible research. Here containers can be used to create and distribute consistent research software environments. They have their uses in complex installations of interconnected libraries as well as relatively simple requirements, such as stable python environments. Containers also make the software stack and research much more accessible, by removing the need for often time-consuming installation processes and providing an isolated computing ecosystem that can be treated like a black box where only input data has to be provided by the user.
Despite their widespread use, many misconceptions and bad practices are currently prevalent within the research community about the abilities, use cases and safety of containers. If used incorrectly, containers can produce inconsistent results, fail to deploy or even expose critical security vulnerabilities to potential attackers. It is therefore important to equip the scientists with skills that will let them produce and distribute containers that are efficient, easy to use and safe from the point of view of the user and the developer."
X0-011,Anastasia Alexov,National Solar Observatory,The Struggles to find a good Electronic Document Management System (EDMS) for the DKIST Project,"The Daniel K. Inouye Solar Telescope (DKIST) has been under construction since 2010 and will start operations in late 2021. Since the project's inception, all the telescope and instrument engineering drawings (in Computer Aided Design (CAD)) and project documentation has been stored and managed using The Vault, a SolidWorks® Enterprise Product Data Management software package for the the DKIST Electronic Document Management System (EDMS). The DKIST Vault has adopted a complex workflow for documentation version and revision control and has the ability to properly display interlinked CAD drawings. A drawback of The Vault is that it is cumbersome to access and it is not a modern and easy-to-use software product.
Tasked with finding a better, reasonably priced, and more modern software solution for EDMS for DKIST Operations, we looked at over 20 different EDMS options for the future of DKIST documentation. We present our two-year journey in search of a good EDMS option that suits our needs, the many software products we tested, their pros and cons, and explain our final finding in recommending using Box for the best EDMS solution. At the start of our investigation, we learned that most of the major astronomy projects struggled with this same issue of not having a good EDMS; we hope other astronomy projects could benefit from our recommendation and lessons learned."
I6-001,Emily Lakdawalla,"The Lakdawalla Group, LLC",Taking Passengers on Planetary Exploration Vessels: Benefits of Engaging Space Enthusiast Communities with Rapidly Released Image Data,"Most space agencies host centralized archives of deep-space science mission data that are accessible to the public via the Internet. For some missions, agencies rapidly and automatically release uncalibrated versions of image data to mission websites in widely accessible formats (e.g. JPG and PNG). This generosity with data has inspired a global community of image data processing artists, who download and process science data for fun (and, rarely, profit). While these people often have scientific interests, their image processing goals usually diverge from scientific processing: they work with the data to produce aesthetically pleasing images, illustrations, and views that answer the question: what would it look like if I were there? Supporting this community has benefits for planetary science: these enthusiasts produce beautiful images, educate the public about space exploration through their social networks (including debunking conspiracy theories), and advocate for continued public support of space exploration."
X3-013,Tess Jaffe,NASA Goddard,Accessing NASA data through Python: PyVO tutorials,"Pythonic access to astronomical data has been growing thanks to libraries such as astroquery that convert different APIs from each archive into a common user interface. The Virtual Observatory offers an alternative way of finding the data that does not require knowing which archive to look at in the first place. The astropy-affiliated PyVO package is therefore becoming increasingly useful for finding data and services. The NASA Astronomical Virtual Observatory (NAVO) organization has prepared tutorials and workshops on how to use PyVO. The methods apply to all VO accessible data, not only NASA's, and we hope will provide a generally useful set of materials and examples for the astronomical community. This presentation gives a preview of these tutorials and the VO-enabled workflow they describe."
X5-001,Thomas Vuillaume,"Univ. Savoie Mont Blanc, CNRS, Laboratoire d’Annecy de Physique des Particule",The ESCAPE Data Science Summer School,"The goal of reaching an Open Science ecosystem in the European Open-Science Cloud (EOSC) is not possible without training of early career scientists. In particular, the creation and maintenance of high-quality and open software need special consideration. This is tackled within the ESCAPE H2020 project through thematic training events: the ESCAPE Data Science Summer Schools.
During these schools, young scientists in the field of astronomy, astro-particle and particle physics are taught the necessary ingredients for their software to become a part of open science by experienced code custodians.
The 2021 school's edition was a continuation of the ASTERICS/OBELICS summer schools previously organised at LAPP (Annecy, France) but has been re-organised as an online event due to the COVID crisis. It witnessed more than 1000 registered participants, showing both the need and importance of such training events.
Following the FAIR paradigm and as an example of good practices in code development, the full information of the school are openly available at https://indico.in2p3.fr/event/20424/, including scientific programme, agenda and links to all contributions (software repository, notebooks, contributions, presentations and recordings). 
In this contribution, we review the goals, organisation and participants feedback of such an event to present a return of experience for future similar events."
X1-007,Nicolò Parmiggiani,INAF/OAS Bologna,Preliminary Results of a Deep Learning Anomaly Detection Method to Identify Gamma-Ray Bursts in the AGILE Anticoincidence System,"AGILE is a space mission launched in 2007 to study X-ray and gamma-ray astronomy. The AGILE team developed real-time analysis pipelines to detect transient phenomena such as Gamma-Ray Bursts (GRBs) and to react to external science alerts received by other facilities. This automated software system implements different algorithms to analyse data acquired by the AGILE detectors and provide results in the shortest time possible. The AGILE anti-coincidence (AC) system comprises five independent panels surrounding all AGILE detectors. The primary role of the AC system is to reject background charged particles efficiently. It can also detect hard X- and gamma-ray photons in the energy range 50 - 200 KeV. Data acquired by the AC are continuously recorded in telemetry, with 1 second resolution, to monitor the high-energy background through the orbital phase. Due to geometry and satellite spinning, each AC panel exposes different directions in the sky, detecting possible astrophysical sources and transients under different angles. The count rates of each AC panel constitute a time series. The time series of all five AC panes can be considered a single multivariate time series (MTS) as the panels' count rates are all time-aligned. In this work, we present a new Deep Learning model for GRBs detection based on the analysis of MTS produced by the AGILE AC system. This model can be used to react to external science alerts received by other facilities and search for GRB. To analyse the MTS, we implemented an anomaly detection model using the Deep Convolutional Neural Network (CNN) autoencoder architecture. This model can be trained with unsupervised learning techniques without labelled data and does not require manual feature engineering. We trained the model with a dataset of MTS, binned in a time window of 40 seconds, randomly sampled from the AGILE AC data acquired throughout several years of AGILE observations. The reconstruction error of the autoencoder is the difference between the original input data and the data reconstructed by the network. It is used as the anomaly score to classify a segment of the multivariate time series. If the anomaly score is higher than a predefined threshold, that time window is flagged as an anomaly (e.g. when a GRB is detected). The threshold value is defined using the reconstruction error distribution of the test dataset and is optimised to obtain the best tradeoff between true detections and false positives. We evaluated the trained model using a list of MTSs containing anomalies (GRBs), manually selected from the AGILE AC data. Each anomalous MTS has a time coincidence with a GRB detected by other space missions, validating the anomaly classification. We used the trained model to calculate the anomaly score of these MTS and detect the GRBs. The tests confirmed the model's ability to detect transient events, providing a new promising technique to identify GRBs in the AC data stream."
X2-002,Federico Incardona,"INAF, Osservatorio Astrofisico di Catania, Via S Sofia 78, I-95123 Catania, ITALY",Optimization of the storage database for the Monitoring system of the CTA,"We present preliminary test results for the correct sizing of the bare metal hardware that will host the database of the Monitoring system (MON) for the Cherenkov Telescope Array (CTA). The MON is the subsystem of the Array Control and Data Acquisition System (ACADA) that is responsible for monitoring and logging the overall CTA array. It acquires and stores monitoring points and logging information from the array elements, at each of the CTA sites. MON is designed and built in order to deal with big data time series, and exploits some of the currently most advanced technologies in the fields of databases and Internet of Things (IoT). To dimension the bare metal hardware required by the monitoring system (excluding the logging), we performed the test campaign that is discussed in this paper. We discuss here the best set of parameters and the optimized configuration to maximize the database data writing in terms of the number of updated rows per second. We also demonstrate the feasibility of our approach in the frame of the CTA requirements."
X9-009,Graham Bell,East Asian Observatory,Starlink: the 2021A release,"Starlink is an open-source collection of software for astronomy containing tools for data reduction, analysis and visualization. It is currently maintained by the East Asian Observatory, with recent progress predominantly driven by evolving instrumentation at the James Clerk Maxwell Telescope.

We present the 2021A release, which includes the last three years of developments. HDF5 is now the default data format for Starlink data files and the associated libraries (HDS and NDF) have been updated to allow for large data arrays (> 2^32 pixels). Various applications have been updated to support these large arrays. Support for IVOA MOC regions has been added."
X6-002,Sándor Kunsági-Máté,"Eötvös Loránd University, Budapest, Hungary",Decomposition of stellar populations in simulated disk galaxies using the TensorFlow/Keras framework,"The proper mechanism of the galaxy evolution is an active research area where the analysis of the bulge-disk relation and stellar population properties are crucial. Spiral galaxies have significant bimodality in their stellar composition. The disk and the spiral arms are composed of a young, blue stellar population and the central region, the bulge contains mostly old, red stars. The proper separation of the two components in multiband photometric surveys can help to understand the stellar evolution inside spiral galaxies. It is often called as a blind source separation problem, since we do not know the exact broadband spectral energy distribution (SED) and the morphology of the sources. To solve this problem there are so-called deblending algorithms - such as Scarlet - where the SED and morphology determination are performed at the same time. In our study we estimate the SED and morphology in two distinct steps. We developed a neural network (sedNN) in Keras that can predict the two SEDs directly from the color distribution of the galaxy, and we used these SEDs as input in the morphology determination of the two sources in a TensorFlow model. We trained and tested the network on simulated galaxies where we found that sedNN can predict the SEDs with 2-3% accuracy, which is about 2-4 times better than using Scarlet."
X3-014,Felix Stoehr,ESO/ALMA,Remote visualization and previews for ALMA,"The amount of data produced by the Atacama Large Millimeter/submillimeter Array (ALMA) is huge and the files can be large. In addition to a sophisticated query functionality, users also need to be able to quickly have a look at the actual data themselves without needing to download them. To this end, we have added a powerful remote-visualization system using CARTA to the ALMA Science Archive as well as have created elaborate interactive previews."
X1-009,Hossen Teimoorinia,Canadian Astronomy Dar=ta Centre,Mapping the Diversity of Galaxy Spectra with Deep Unsupervised Machine Learning,"Modern spectroscopic surveys of galaxies such as MaNGA consist of millions of diverse spectra covering different regions of thousands of galaxies. 
We propose and implement a deep unsupervised machine learning method to summarize the entire diversity of MaNGA spectra onto a 15x15 map (DESOM-1), where neighbouring points on the map represent similar spectra. We demonstrate our method as an alternative to conventional full spectral fitting for deriving physical quantities, as well as their full probability distributions, much more efficiently than traditional resource-intensive Bayesian methods. Since spectra are grouped by similarity, the distribution of spectra onto the map for a single galaxy, i.e, its ``fingerprint"", reveals the presence of distinct stellar populations within the galaxy indicating smoother or episodic star-formation histories. We further map the diversity of galaxy fingerprints onto a second map (DESOM-2). Using galaxy images and independent measures of galaxy morphology, we confirm that galaxies with similar fingerprints have similar morphologies and inclination angles. Since morphological information was not used in the mapping algorithm, relating galaxy morphology to the star-formation histories encoded in the fingerprints is one example of how the DESOM maps can be used to make scientific inferences."
X3-015,Stéphane Erard,"LESIA, Observatoire de Paris, Université PSL, CNRS, Sorbonne Université, Université de Paris",Access to VizieR catalogues of the Solar System,"Among the 20,000+ catalogues currently available in VizieR at CDS, a small fraction (< 10%) is dedicated to Solar System studies and exoplanets. Those rely more heavily on the initial declaration of authors and prove less accurate in average because of limited support of particularities in this field. The aim of the present study is to identify a way to provide more visibility to these catalogues in the Planetary Science community.
This problem has been addressed by providing a new, additional interface to VizieR for this content. A specific VO service was implemented, which is compliant with the EPN-TAP protocol, a version of TAP associated to the EPNCore set of metadata defined in Europlanet/VESPA (currently a Proposed Recommendation in IVOA). The catalogue descriptions in the EPN-TAP table has been filled up to a certain point. This activity is extremely demanding, as it requires science expertise of each particular topic, and sometimes to dive into the papers. 
The installation of the EPN-TAP service itself under Vollt implied technical fixes on both server and client sides.
The EPN-TAP service VizieR_planets is now installed at CDS and queried by the VESPA portal (http://vespa.obspm.fr)"
X1-010,Aram Lee,University of Victoria,A deep learning approach to the discovery of anonymous Kuiper belt objects in wide-field imaging.,"Goal: To use deep learning methods to detect Kuiper belt objects (KBOs) in wide-field imaging data sets without the need to measure those images' statistical properties or specify the sky-motion of KBOs that may be present.

Method: Synthetic KBOs were added to a time series of CFHT MegaPrime images. The images were divided into sub-image-pairs to create a two-channel training set. Convolutional neural networks were applied to extract useful features from the sub-image-pairs and predict the existence of KBOs within the training set pairs. A second regressive network was then used to determine the position of KBOs within the image pairs.

Results: ImageNet classification architectures such as AlexNet, VGG, and MobileNet were effective at predicting the existence of a KBO within a sub-image pair. MobileNet was successful in predicting the approximate coordinate of the KBO within the sub-images.

Conclusion: These deep learning approaches could effectively detect the KBOs down to a Signal-to-Noise ratio of 5 per image in self-validating tests. By applying this deep learning-based detection method to arbitrary time-series data, we hope to find KBOs in non-KBO specific observational data sets, such as the Hyper SurpimeCam Subaru Strategic Program."
X3-016,Philippe Salome,"LERMA, Observatoire de Paris, PSL Research Université, CNRS, Sorbonne Université, UPMC, Paris, France",Yafits - A remote 2D/3D radio-data explorer - new features,"The Yafits (Yet Another FITS) remote viewer is an application that can be deployed via a Docker image on any server hosting a set of FITS files. The goal of Yafits is to enable a remote quick look and inspection of the 2D and 3D data. A special effort has been made for radio-astronomy observations, like ALMA, IRAM/NOEMA or SKA images and cubes. Yafits is currently used in Artemix, the ALMA RemoTE data MinIng eXperiment (http://artemix.obspm.fr). We present here recent developments of Yafits that include (i) the handling gnomonic, orthographic, azimutal and global sinusoidal projections, that are necessary for large scale image displays (ii) the addition of markers and coupling with NED source search in the 3D viewer (iii) the search of molecular lines and their overplot in the spectrum viewer (iv) the addition of simple data treatments like Hanning smooth or line luminosity computation. Interoperability with Aladin and Cassis have been consolidated. Yafits is fully operational and available via gitlab."
X1-011,Simone Riggi,INAF-OACT,Radio source analysis services for the SKA and precursors,"New developments in data processing and visualization are being made in preparation for upcoming radioastronomical surveys planned with the Square Kilometre Array (SKA) and its precursors. A major goal is enabling extraction of science information from the data in a mostly automated way, possibly exploiting the capabilities offered by modern computing infrastructures. In this context, the integration of source analysis algorithms into data visualization tools is expected to significantly improve and speed up the cataloguing process of large area surveys. 
To this aim, the CIRASA project was recently started to develop and integrate a set of services for source extraction, classification and analysis into the ViaLactea visual analytic platform and knowledge base archive. In this contribution, we will present the project objectives and tools that have been developed, interfaced and deployed so far on the prototype European Open Science Cloud (EOSC) infrastructure provided by the H2020 NEANIAS project."
X3-017,Andrea Bulgarelli,INAF/OAS Bologna,The AGILEScience App to execute gamma-ray scientific analyses from mobile devices,"AGILE is a space mission launched in 2007 to study X-ray and gamma-ray phenomena. The AGILE Team is involved in the multi-messenger campaigns to send and receive science alerts about transient events such as Gamma-ray Bursts (GRB) or Gravitational Waves (GW). The fast reaction time is essential to send communications about transient phenomena in the shortest time possible. For this reason, the AGILE Team developed several real-time analysis pipelines to analyse data and follow-up external science alerts. However, the results obtained by these pipelines are preliminary and must be validated with manual analyses that are the bottleneck of the workflow. To speed up the scientific analysis performed by scientists, the AGILE Team developed the AGILEScience mobile application that, in addition to present to the public the main scientific results obtained by AGILE, offers to the AGILE Team a password-protected section used to perform gamma-ray analysis and visualise results.

We present in this contribution an improved functionality of the AGILEScience mobile application that aims to enable the AGILE Team to execute a full scientific analysis using their mobile devices. 

The AGILEScience application is a mobile software developed for the iOS and Android Operating Systems (representing almost 100% of the mobile market). The application has an interface with a remote server that executes the processes on the computing servers. The users can prepare and submit scientific analyses using the application on their mobile devices. The application has predefined forms to drive the users during the insertion of the required parameters to execute several kinds of analyses. When the analysis is completed, the system sends an email to notify the user that can visualise the results (e.g. plots, tables, and HTML pages) through the application. 
In addition, the App allows the monitoring of the data flow of the satellite and shows the results of different AGILE automated pipelines in a single interface.

This possibility to perform scientific analysis from a mobile phone enables the AGILE researchers to perform fast scientific analyses remotely using their mobile devices to validate the preliminary results obtained with the automated pipelines. The possibility to execute manual analyses directly from the mobile devices significantly reduces the overall reaction time of the AGILE Team for the follow-up of transient phenomena."
X6-001,Ulrich Mbou Sob,Rhodes University,Accelerating the search of optimal radio interferometric gain solution intervals using xarray-ms and dask.,"Solving for gains at longer time and frequency intervals than the data resolution is a well-proven technique for improving signal-to-noise ratio and regularising calibration algorithms. In our previous work, we investigated how different factors, namely, the noise level in the data, the intrinsic variability of the gains, degree of model completeness, and the presence of radio frequency interference, impact the selection of optimal solution intervals during calibration. We suggested a brute force algorithm that requires performing the search separately for each chunk of data, antenna and possible direction in the sky. In this work, we present a fast implementation of the algorithm based on xarray-ms and dask."
X0-015,Javier Moldon,IAA-CSIC,SPSRC: cloud computing in the Spanish prototype of a SKA Regional Centre,"The Square Kilometre Array (SKA) will be the most sensitive radio telescope on Earth. The unprecedented sensitivity of the instrument will help astronomers to address fundamental questions in astrophysics, fundamental physics and astrobiology. The large amounts of data produced by the SKAO telescopes in Australia and South Africa will be distributed to a network of SKA Regional Centres (SRCs) worldwide where astronomers will have access to the data and to computational resources to extract scientific knowledge from the SKA data. Here we provide an update on the development of the Spanish prototype of an SRC (SPSRC), which is providing support to both scientific activities, specially those involving SKA precursors and pathfinders, as well as prototyping activities, developing and testing cloud-based services to enable advanced data access and processing. We will describe the hardware and software of the OpenStack cloud platform and the main services provided to scientists. We will also comment on the type of scientific projects we currently host, the development projects being conducted, and the training activities aiming to prepare the community for the SKA by promoting Open Science practices as a way to enable scientific reproducibility as well as a key tool to address the SKA challenges."
X0-012,David Rodriguez,Space Telescope Science Institute,The SIMPLE Archive,"We present the SIMPLE Archive alongside its database management tool, AstrodbKit2. SIMPLE is an archive of low mass stars, brown dwarfs, and exoplanets driven by community curation and review using GitHub. SIMPLE relies on AstrodbKit2 to convert back and forth from a document-store model of the database, to a more standard relational database that can be used with established packages like SQLAlchemy. In this poster, we present the architecture of the SIMPLE database and how using AstrodbKit2 facilitates a git workflow for reviewing and approving database modifications.

SIMPLE is available at https://github.com/SIMPLE-AstroDB/SIMPLE-db 

AstrodbKit2 is available at https://github.com/dr-rodriguez/AstrodbKit2"
T5-002,Hendrik Heinl,CDS/CNRS,"Astropy, PyVO and the Radio realm","In the past years, VO standards were widely accepted and spread and many
astronomers get more and more familiar with standards and protocols like
Cone Search, SAMP and TAP/ADQL. Being skilled in these in combination with
Astropy and PyVO opens unlimited opportunities to find, to access and to
combine different datasets for further analysis. 

In this tutorial the participants will learn how to use PyVO and ObsTAP
to find services and to explore the data on these services, they will
use Daralink and SODA to perform cutouts on large images, and eventually
use PyVO and Astropy to write a SAMP handler to combine functionality of
TOPCAT and Aladin with PyvO. 
The cutouts will be performed on the Astron data service on data derived
from LOFAR. 
The cutouts will be performed on the Astron data service on LOFAR
derived data. 

We don't expect participants to be experts in above fields after this
tutorial, but to scratch on the surface of how useful these tools are and where to turn if they want to get deeper knowledge. In particular this should
include:

- TAP/ADQL and related special Obscore services
- PyVO for data discovery and data access
- How to read a standard document to develop against this standard
 (SAMP)

The used software will be: 
Python 3.6 (or newer), Astropy, PyVO, Topcat and Aladin. We will provide Python scripts in a github repository."
X4-005,Tracy Chen,Caltech/IPAC-NED,Best Practices for Data Publication in the Astronomical Literature,"We present an overview of best practices for publishing data in astronomy and astrophysics journals. These recommendations are intended as a reference for authors to help prepare and publish data in a way that will better represent and support science results, enable better data sharing, improve reproducibility, and enhance the reusability of data. We highlight here the recommendations that are closely following the Findability, Accessibility, Interoperability, and Reuse (FAIR) standards for publishing, archiving, and accessing astronomical data. We encourage authors, journal editors, referees, and publishers to implement the best practices reviewed here, as well as related recommendations from international astronomical organizations such as the International Astronomical Union (IAU) and International Virtual Observatory Alliance (IVOA) for publication of nomenclature, data, and metadata."
X7-007,Brian Kent,NRAO,Utilizing Open Shading Language and Blender for Data Cube Volume Visualization,"We describe a method of rendering data cubes using the software package Blender and the Open Shading Language. OSL is used by nearly all visual effects and rendering packages as a standard graphics shading language, with syntax similar to C. OSL can be used to design renders within the Blender node compositor and incorporated into data visualization scenarios. We show the workflow including steps of (1) data import, (2) creating the OSL shader, (3) connecting nodes end to end, (4) camera tracking, and (5) rendered output. We apply this workflow to rendering astronomical data cubes."
X0-013,long,Boise State University,Automated Spectroscopic Analysis using Genetic Algorithms,"We developed an automated spectroscopic analysis tool based on Genetic Algorithms (GA) to accelerate spectral fitting with reduced human intervention but improved efficiency and reproducibility. This automation is vital in the era when current and upcoming detectors acquire massive data at rates orders of magnitude greater than current collection rates. We use GA, a robust metaheuristic method to evaluate the optimization problem of fitting and obtain global optimum of parameters from various physics models. We start with a population of temporary solutions consisting of model parameters called chromosomes. We then select part of the best solutions by evaluating their fitness value and random solutions for mixing, i.e., crossover for the next generation of solutions. Next, we apply mutation operators to modify existing solutions by disturbing them by random chance. We also provide several types of selection operators to avoid ill-condition the GA operators may encounter in specific applications. The method has been applied to X-ray spectral data of a starburst galaxy NGC 253 from XMM-Newton reflection grating spectrometer, and compared with direct fitting using Xspec. The fitting considered a complicated physical model that includes thermal emission from the diffuse hot plasma as well as the charge exchange emission due to its interaction with the cold gas, besides the normal components of the bright point sources and the foreground absorption."
X0-014,jhaase,Max-Planck Institute for Extraterrestrial Physics,The SciServer at MPE - Enabling collaboration for eROSITA & HETDEX,"In 2020 a copy of the JHU SciServer was installed at the Max-Planck Computing & Data Facility (MPCDF) for the Institute for Extraterrestrial Physics (MPE).
This local installation of the science platform is to serve as a collaboration tool for the eROSITA and HETDEX projects, 
allowing astronomers from both international collaborations to work on their data in one place in a safe. yet flexible manner.
This Poster gives an overview over the projects, the SciServer system setup and the choices made along the way."
X1-012,Iryna Vavilova,"Main Astronomical Observatory, National Academy of Sciences of Ukraine",Machine Learning of Galaxy Classification by their Images and Photometry,"We present two approaches for classification of 316,031 galaxies from the SDSS at z<0.1 with machine learning. Using photometry parameters of galaxies and human labeling, Naive Bayes, Logistic Regression, Support Vector Machine, Random Forest, and k-Nearest Neighbors we obtained that Support Vector Machine gives the highest accuracy for the binary galaxy morphological classification (96.1 % for early type galaxies, 96.9% for late type galaxies). Using CNNs as a model for the image-based inference of detailed morphology we attained 93% accuracy on average to classify these galaxies for five visual classes (completely rounded, rounded in-between, smooth cigar-shaped, edge-on, and spiral) and 34 morphological classes by their peculiarities. We obtained that CNNs with adversarial validation and adversarial image data augmentation improves classification of smaller and fainter SDSS galaxies with m_{r} <17.7. We discuss labeling bias and problem points of both classification approaches in context of the influence of various galaxy parameters on the machine learning accuracy."
X7-008,Mark Taylor,University of Bristol,"Taplint, the TAP Service Validator","TAP, the Table Access Protocol, is a Virtual Observatory standard allowing client software to interact with remote database services in a standardised way, including acquiring rich metadata and submitting simple or complex queries in an SQL-like language. It underlies much current access to astronomy data archives, providing very powerful query capabilities, and is a VO success story. The protocol stack involved however is quite complex, involving a dozen or so separate IVOA documents, and implementors face many opportunities to make mistakes.

Taplint, part of the STILTS package, is a suite of tests intended to help TAP implementors check that a TAP service is behaving in compliance with the various applicable standards. We present here an overview of its capabilities and usage."
X3-018,Giuseppe Tudisco,INAF - Osservatorio Astrofisico di Catania,ViaLactea: a distributed Visual Analytic system for exploring our Galactic ecosystem,"Space missions and ground-based facilities collect increasingly huge amounts of data that demand novel approaches to data processing, storage, visualization and analysis. Next-generation facilities, such as the Square Kilometre Array (SKA), are going to increase the volume of data at a rate higher than the ability to analyze them. New approaches combining visualization and data processing are necessary to enable astronomers to analyze astronomical data and extract meaningful knowledge. We present ViaLactea, an ecosystem that offers the Astrophysics and Planetary communities highly interactive visual analytic interfaces enabling effective exploitation of multi-wavelength observations of the Milky Way Galactic Plane, ranging from the near-infrared to the radio spectrum. We showcase a desktop application, and further present interfaces that include collaborative web solutions for multi-user support underpinned by efficient remote server CPU and GPU rendering, and support of mobile and desktop devices. All underlying data is managed by a dedicated data service, namely the ViaLactea Knowledge Base, that provides object catalogs and Spectral Energy Distribution model outputs to carry out correlation analysis workflows for studying the star formation process in our Galaxy. ViaLactea strongly promotes FAIR data and Open Science practices and is integrated within the European Open Science Cloud (EOSC)."
X1-013,Alejandro Barrientos,Universidad Tecnica Federico Santa Maria / Joint ALMA Observatory,"Prediction of molecular parameters from astronomical emission lines, using Neural Networks","Molecular astronomy is a field that is blooming in the era of large observatories such as the Atacama Large Millimeter/submillimeter Array (ALMA). With modern, sensitive, and high spectral resolution radio telescopes like ALMA and the Square Kilometer Array, the size of the data cubes is rapidly escalating, generating a need for powerful automatic analysis tools. This work explores the ability to perform predictions of molecular parameters, such as excitation temperature (Tex) and column density (log(N)) from astronomical spectral lines by the use of neural networks. We used as test cases the spectra of CO, HCO+, SiO and CH3CN between 80 and 400 GHz. Training spectra were generated with MADCUBA, a state-of-the-art spectral analysis tool. Our algorithm was designed to allow the generation of predictions for multiple molecules in parallel, in a way that is scalable, and presents a linear speedup. Using neural networks, we can predict the column density and excitation temperature of these molecules with a mean absolute error of 8.5% for CO, 4.1% for HCO+, 1.5% for SiO and 1.6% for CH3CN. The prediction accuracy depends on the noise level, line saturation, and number of transitions. We performed predictions upon real ALMA data. The values predicted by our neural network for this real data differ by 13% from the MADCUBA values on average. 
Current limitations of our tool include not considering line width, source size, multiple velocity components, and line blending."
X9-008,Nicola Fulvio Calabria,INAF - Osservatorio Astronomico di Trieste - IA2 group,Exposing astronomical data with VOSSIA 2.0: a modular and customizable VO-compliant multi-protocol interface,"Exposing astronomical datasets with a standardized interface can be a real challenge. From one side the International Virtual Observatory Alliance (IVOA) defines standards in form of recommendations for protocols and output formats, on the other side the heterogeneous nature of datasets makes the use of a single tool for exposure problematic. VOSSIA 2.0 aims at introducing a flexible customization system to build services with custom IVOA-like protocols, of which the official ones are meant to be a predefined subset, at configuration level. Here are presented the main features of VOSSIA 2.0, its modular architecture and the basics of its configuration system, finally our perspectives and plans for its release."
X4-006,Arianna Bartolomei,"University of Camerino, University of Perugia, INFN Perugia, AHEAD 2020, ESCAPE EU",Localization of Electromagnetic Transient Candidates: a new Python plug-in for Aladin,"With the urgent request of efficient updates in astronomical facilities and techniques, in order to optimize GW events' analysis, we found interesting to extend functionality of the Aladin software.
The purpose behind our request is to introduce a new Python plug-in to the astronomical community. \\Our code allows anyone, from the expert researcher to the random user, maybe keen on astrophysics, to immediately receive information about the probabilities of having an EM transient in a given pixel of any loadable on Aladin skymap, just by clicking on it. 
The algorithm has been developed in the context of the AHEAD and ESCAPE EU projects.
Future improvements, projects and incoming versions of this plug-in will be discussed as well."
X1-014,RozanskiTomasz,"Astronomical Institute, University of Wrocław",Emission stellar spectra normalisation using SUPPNet,"Spectrum normalisation is an important and demanding task which is a necessary step of most stellar spectra analysis. To automate this step, we developed a deep neural network that is well suited for this task called SUPPNet (availible [on-line](https://git.io/JqJhf)). Here we present the method, latest advances in applying it to emission spectra and discuss normalisation uncertainties estimation."
X4-007,gilles Landais,CDS,Building workflows for FAIR astronomical catalogues,"Documented workflows are an important aspect of CDS operations, and are the basis for the CoreTrustSeal certification of the VizieR catalogue service. The worklflow processes include metadata assignment in conformance with the standards and the needs of the astronomy research community. Among the standards, the Virtual Observatory promotes data models and access protocols to provide reusable data. In parallel, other standards and concepts are emerging with the rise of open data, for example the widespread use of DOIs. The FAIR principles and the increasing importance of open data for science, overlap and complete the IVOA capabilities.
While IVOA standards enable a very high level of interoperability allowing data mining, workflows and software usage, the DOI enables the connection to a network of references resources well adapted for citation. We will describe FAIR-workflows developed in the VizieR catalogue service of the CDS and present how the data benefit from IVOA standards and DOIs."
X4-008,Robert Butora,"INAF, Osservatorio Astronomico di Trieste",Interoperable standardisation of the VLKB dataset access services,"The VLKB (Via Lactea Knowledge Base) consists of a set of
heterogeneous collection of source catalogs
(compact and diffuse), images and radio observational data cubes,
including interface services for discovery and access.
These latter ones allow positional search combined to spectral axes filtering and
other constraints, and direct cut or mosaicking of the original datasets.
The VLKB was initially built within the VIALACTEA EU-FP7 funded program
and is now used and maintained within other EU funded programs (NEANIAS,
ECOGAL), national projects and infrastructures (INAF CIRASA PRIN and
INAF IA2 data center).

A TAP service provides access to the catalog part of the database
while dataset collections are available through a discovery interface
to identify the wanted cubes and images,
and two services: one devoted to cutting the images or cubes based on
positional or spectral axis bounds and another merging adjacent datasets
whether the positional boundaries overlap multiple of them.

This contribution focuses on the datasets access, cutout and merge services,
originally presented through proprietary interfaces and now under
refurbishment using standardised interfaces levereging IVOA protocols and
other technology standards.

The discovery phase (VLKB-search) was fully replaced with SIAP-2.0 protocol
implementation developed in IA2 and called VOSSIA. Elementary operation
in discovery is calculation of overlap of two regions on the sky - that of
searched area and that of area covered by FITS-file. While original
proprietary implementation use AST library to compute the overlap, the VOSSIA
takes advantage of ""pgSphere"" - an extension to PostgreSQL relational database engine
with the ability to handle geometry types.
To support these database operations, the data in internal tables for of the
original services were mapped to the IVOA ObsCore data model. Each entry represents
one HDU of a FITS-file in VLKB.

The data-access solution comes in two forms: creation of sub-images to save
bandwidth in download (VLKB-cutout) and de-mosaicking (VLKB-merge) when discovery
phase yields sub-images stored in FITS-file representing neighbouring areas on
the sky. Unlike VLKB-search, these two services preserve the engine implementations,
but adopt VO-compliant interfaces.

VLKB-cutout's proprietary interface is replaced with SODA. Due to relatively low
response times only synchronous interfaces were implemented. Also, the
VLKB is specific to Galaxy, coordinate transformations are necessary to adopt
SODA interfaces.

VLKB-merging (demosaicing) service is build on Montage software suite.
Algorithms, when applied to VLKB-data represent complex computations with
response times too long for synchronous requests.
As a solution, we run demosaicing ""behind"" the UWS specified job model.

Additionally, some of the VLKB-data is considered proprietary and so its access
need to be restricted to authorized users only. This required extensions to internal
tables (ObsCore). The granularity of this restriction is one HDU: hence
the need to extend the internal tables. The VLKB services suopport the
widely used OAuth2/OIDC protocols for authorization."
X9-010,Travis Stenborg,University of Sydney,Bulirsch-Stoer Instability in ORSA with Java Plotly Visualisation,"ORSA is celestial mechanics software designed for numerical integration of the motion of planets, comets, asteroids and related bodies. ORSA is available both as a legacy GUI-based application, and as a modern numerical library. Validation of the legacy version's integrators (leapfrog, Runge-Kutta, Bulirsch-Stoer, Everhart's 15th order scheme) against analytical solutions revealed Bulirsch-Stoer instability. A repeatable test case which demonstrates the instability is described, then visualised from Java with the Plotly graphical library. This work was aimed at a) guiding best use of legacy ORSA, which remains in demand, by highlighting the Bulirsch-Stoer issue and b) providing an example of Java / Plotly integration that can be extended to other research visualisations."
X7-009,Manuel Parra,Instituto de Astrofísica de Andalucía (IAA-CSIC),"Asymmetric distribution of data products from WALLABY, an SKA precursor neutral hydrogen survey","The Widefield ASKAP L-band Legacy All-sky Blind surveY (WALLABY) is a neutral hydrogen survey (HI) that is running on the Australian SKA Pathfinder (ASKAP), a precursor telescope for the Square Kilometre Array (SKA). The goal of WALLABY is to use ASKAP's powerful wide-field phased array feed technology to observe three quarters of the entire sky at the 21 cm neutral hydrogen line with an angular resolution of 30 arcseconds. This is expected to detect half a million galaxies in the local universe, producing about 75 PB of raw data each year. Post-processing activities at the Australian SKA Regional Centre (AusSRC), Canadian Initiative for Radio Astronomy Data Analysis (CIRADA) and Spanish SKA Regional Centre prototype (SPSRC) will then produce publicly available advanced data products in the form of source catalogues, kinematic models and image cutouts, respectively. These advanced data products will be generated locally at each site and distributed across the network. This scenario will require the replication and synchronisation of database tables across all sites where a multi-site data repository environment will be essential for efficient distribution management of the access, location and delivery of data. Over the course of the full survey we expect to replicate data up to 10 MB per source detection, which could imply an ingestion of tens of GB to be consolidated in the other locations in real-time or near real time. An asymmetric replication model between the cooperative databases will be vital for experimentation on data products distribution across the network, where locations could be either sources of data products, or targets to replicate those products. Here, we explore the use of an asymmetric database replication model and strategy, using PostgreSQL as the engine and Bucardo as the asynchronous replication service to enable robust multi-source pools operations with data products from WALLABY. Consequently, this work would serve to evaluate this type of data distribution solution across globally distributed sites. Furthermore, a set of benchmarks have been developed to confirm that the deployed model is sufficient for future scalability and remote collaboration needs."
X0-016,Fenja Kollasch,HITS gGmbH,UltraPINK - New possibilities to explore Self-Organizing Kohonen Maps,"Astronomical data comes in various types, shapes, and formats. It remains
a challenging task to display and process the data correctly, as well as finding
points of interest. Frameworks like Parallelized Flipping-Invariant Kohonen
Maps (PINK) can help separating the wheat from the chaff. An underlying
neural network learns common shapes appearing in the data set. The result is
a compact representation of these shapes that can be analyzed furthermore.

During past venues, we presented UltraPINK, a web application helping
with the mentioned further analysis. With UltraPINK, researchers can train,
inspect, and explore self-organizing maps, whereby the toolbox of interaction
possibilities grows continually. The latest version includes responsive design to
allow the usage on various devices, flexible training forms to create individual
cartesian or hexagonal Kohonen maps, as well as an interface to Aladin Lite
and the option to compare spatially close objects.

Key feature of UltraPINK is the consideration of versality in astronomical data. By keeping the operations as abstract as possible and using design
patterns meant for abstract usage, we ensure that data is compatible with UltraPINK, regardless of its type, formatting, or origin.
Future work on the application will keep extending the catalogue of exploration tools and the interfaces towards other established applications to process
astronomical data. Ultimatively, we aim towards a solid infrastructure for data
analysis in astronomy."
X2-003,Salvatore Savarese,INAF Osservatorio Astronomico di Capodimonte,Software solutions for numerical modeling of wide-field telescopes,"This paper presents an integrated modeling software to analyze the PSF of wide-field telescopes affected by misalignments. Even relatively small misalignments in the optical system of a telescope can significantly deteriorate the image quality by introducing large aberrations. In particular, wide-field telescopes are critically affected by these errors, insomuch that usually a closed-loop active optics system is adopted for a continuous correction, rather than for sporadic alignment procedures. Typically, a ray-tracing software such as Zemax OpticStudio is employed to accurately analyze the system during the optical design. However, an analytical model of the optical system is preferable when the PSF of the telescope must be reconstructed quickly for algorithmic purposes. Here the analytical model is derived through a hybrid approach and developed in a custom software package, designed to be general and flexible in order to be tailored to different optical configurations. First, leveraging on the Zemax OpticStudio API, the ray-tracing software is integrated into a Matlab pipeline. This allows to perform a statistical analysis by automatically simulating the system response in a variety of misaligned working conditions. Then, the resulting dataset is employed to populate a database of parameters describing the model."
X4-009,Elisa Cartechini,"University of Camerino, University of Perugia, INFN Perugia, AHEAD 2020, ESCAPE EU",Using galaxy catalogues to localise gravitational-wave sources: a new Virtual Observatory plug-in to esteem their completeness,"In this poster we discuss the development of a new Python plug-in thought to contribute to the ongoing request of efficient tools for analysis in the context of the multi-messenger with gravitational 
wave. When searching for host galaxies of gravitational-wave sources, it is important to estimate the completeness of the galaxy catalogue used for the research.
Our plug-in will allow any user who is visualizing a GW sky localization to immediately have an estimate of the completeness of a chosen galaxy catalogue in the interested region. Moreover the 
plug-in will calculate the intersection areas between the gravitational-wave sky localization and the high Galactic dust extinction regions.
The plugin is based on mocpy module and is able to interact with Aladin Desktop via SAMP."
X6-003,Nancy Wolk,Smithsonian Astrophysical Observatory,The X-ray Galactic Center: 2 Decades of Image Releases,"ASA's Chandra X-ray Observatory has been observing the X-ray Universe for over two decades. One familiar target is the center of the Milky Way galaxy. From the earliest images to a more recent 64-day long compilation, Sagittarius A* and the broader Galactic Center have played a large role in Chandra's public release materials and garnered significant public attention. In addition to deep X-ray images, multiwavelength composites, virtual reality simulations, and even data sonifications have been utilized to bring people closer to the center of our own Milky Way and help assimilate Chandra's unique views. In this poster, Sagittarius A* and the Galactic Center are used as a case study to review how we have presented Chandra data to the public over time."
X3-019,Lexy Andati,"Centre for Radio Astronomy Techniques and Technology, Rhodes Univeristy",PolarVis: Towards Web-based Polarimetric Analysis,"Astronomers performing polarimetric analysis on astronomical images often have to manually identify locations on their objects of interest, such as galaxies, which exhibit the influence of magnetic forces due to interaction with their environments or inherent processes. These locations are known as Lines of Sight (LoS). Analysing the various lines of sight can provide insight into the electromagnetic nature of the astrophysical object in question and its surroundings. For each LoS, astronomers generate diagnostic plots to map out the variation of the corresponding electromagnetic field, such as those of fractional polarisation and Faraday spectra. However, associating the different LoS diagnostic plots to their positions on an astronomical image requires alternating between the plots and the images. As a result, determining whether the location of the LoS influences its magnetic field variation by analysing its diagnostic plots becomes arduous due to the absence of a direct way of linking the two. PolarVis is an effort towards allowing an almost instant view of the interactive diagnostic plots corresponding to a given line of sight at the click of a button on that line of sight on the image, using an interactive web-based FITS viewer -JS9."
X0-017,JuanBC,"IATE-OAC-CONICET, CIFASIS-UNR-CONICET, CONAE",Being nice to the server: Wrapping a REST API for a cosmological distance/velocity calculator with Python,"We present a Python client for the Cosmicflows Galaxy Distance-Velocity Calculator CF3 with a retry and cache system.
We describe the technologies and the engineering processes involved in the task of guaranteeing correct behavior and minimal stress load in the main CF3 server.
We also address Quality Assurance code standards and availability of the code."
X0-017,Martin Beroiz,California Institute of Technology,Being nice to the server: Wrapping a REST API for a cosmological distance/velocity calculator with Python,"We present a Python client for the Cosmicflows Galaxy Distance-Velocity Calculator CF3 with a retry and cache system.
We describe the technologies and the engineering processes involved in the task of guaranteeing correct behavior and minimal stress load in the main CF3 server.
We also address Quality Assurance code standards and availability of the code."
X6-004,Landman Bester,South African Radio Astronomy Observatory and Rhodes University,Tidying after cleaning,"The standard deconvolution algorithm used in radio astronomy, i.e. CLEAN, suffers from a number of drawbacks viz. it performs poorly on naturally weighted data, requires negative flux to reach the noise floor of the observation and does not represent extended sources faithfully. However, compared to more sophisticated deconvolution algorithms (eg. deconvolution based on sparsity promoting priors), it is remarkably fast and robust against calibration artefacts and/or unknown systematics in the data. Hence we introduce pfb-clean: a distributed python package that leverages the preconditioned forward-backward algorithmic structure to maintain robustness without any of the above mentioned drawbacks."
X0-018,Marie Terrell,Center for Astrophysics | Harvard & Smithsonian,Flexible Workflow Definitions for Reproducibility in Complex Systems,"In the ever evolving software landscape, it is important to maintain an environment for development that is consistent with production testing. Updates to a complex system that require multiple interrelated dependencies can present a challenge for reproducibility. Flexible definitions can be implemented to augment the existing workflow to allow for both a normal development cycle and testing of upgrades through the same framework. With the Chandra Data Analysis system CIAO as an example this paper will examine how implementing flexible workflow definitions in GitLab can allow for more fine control, decreased time for dependency updates, and reproducibility in complex systems."
X1-015,Jiri Nadvornik,Czech Technical University in Prague,HDF5 parallelization for Hierarchical Semi-Sparse data cubes,"Big Data is not only about big volumes but also a higher number of dimensions within the data. For every observed astronomical object, we usually have multiple observations in time, in different wavelengths, polarization, or even different instrument types. Intuitively, taking all of the relevant information into account will produce higher quality results for classification or clustering algorithms, rather than just focusing on a single aspect of the object. Most often we are talking about spectroscopic and photometric observations which can be combined into spectral data cubes. With the Hierarchical Semi-Sparse data cubes (HiSS cubes) engine we combine spectral and imaging data within the HDF5 format for efficient use within machine learning algorithms. The HiSS cube ensures this efficiency by implementing an indexing mechanism within the HDF5 that also takes advantage of the native chunking feature. Since the preprocessing that rescales the spectral and photometry measurements to be directly comparable takes significant time it needs to be parallelized and also takes advantage of the native HDF5 parallel IO feature. This poster focuses on the parallel performance of the Python version (h5py) of HDF5-based solution in construction of the HiSS cube."
X2-004,Peter Teuben,University of Maryland,The LMT Single Dish Spectral Line Toolkit,"With the goal of adding Science Ready Data Products to the archive of
the Large Millimeter Telescope (LMT), we have developed a toolkit that
allows fully interactive as well as automated pipeline processing of
LMT single dish spectra. The data products include automatic source
detection and spectral line detection using the ALMA Data Mining
Toolkit (ADMIT). Adopting SDFITS as the interchange format, we aim
that other observatories can use our toolkit and that LMT data can be
analyzed by other packages. Interoperability tests are planned for
this. We can produce Quick-Look results within 15 minutes after the
observation has ended for an on-the-fly map, and much faster for
pointed observations."
X9-011,F.-X. Pineau,CDS / Observatoire astronomique de Strasbourg,"MOCLibRust, a common library for MOCPy, MOCCli and MOCWasm","Multi-Order Coverage map (MOC) is an IVOA standard and a powerful tool
to create and manipulate discretized space, time and space-time (ST) coverages.
For example, one can retrieve the pre-built ST-MOCs of XMM and Chandra
and easily -- and quickly -- find the sky areas observed at
the same time by both instruments.

So far, two tools have implemented time and space-time MOCs:
the Java Library used in Aladin, and MOCPy.
Originally written in pure Python, a part of MOCPy has been rewritten
as a wrapper calling Rust code.
This effort has been pursued, resulting in an independent Rust library: MOCLibRust.
The main motivations were to improve performances and to be able to reuse the
same codebase in different tools.
In addition to MOCPy, MOCLibRust is now used in MOCCli, a standalone command line
program, and in MOCWasm, a WebAssembly library to manipulate MOCs from JavaScript
(and thus from tools like ALadinLite).
In the future, we may also consider the develpment of C and/or PostgreSQL wrappers."
X1-016,Humberto Farias Aroca,Universidad Técnica Federico Santa María,Deep Learning model for explainable instance segmentation of morphology galaxy,"The classification of galaxies based on their morphology is instrumental for the understanding of galaxy formation and evolution. Over the past few years, classification using Deep Learning (DL) algorithms has resulted in a significant advance for automated localization, classification and segmentation of galaxies based on their morphology in astronomical images. However, it is known that DL models work as blackboxes, which difficults the incorporation of such models in downstream applications for producing scientific-grade data because it is required that results are validated and, more importantly, show how the results are calculated. This work shows the first pipeline with instance segmentation (i.e., including galaxy localization, segmentation and classification) performing galaxy classification with explicability. The latter concept is approached from the DL perspective, which corresponds to the mapping of the gradient activation in the last convolutional layer before the classification layers. This allows us to interpret the results of the pipeline from the scientific perspective of the problem. Our work is based on Mask R-CNN architecture, which is a state-of-the-art model of Instance Segmentation, that incorporates a module that evaluates what the most important features are in an image for the model inference (for instance, spiral arms, bulge shape or size, HII regions, etc.)"
X0-019,Thomas Cecconello,University of Milano-Bicocca,Latent Space Explorer: Unsupervised Data Pattern Discovery on the Cloud,"Scientific research implies the production of data describing phenomena still not studied and well understood. Within the astronomical context the amount and rate of generation of produced data can be overwhelming, and tools supporting a computer assisted analysis of scientific data can support systematic forms of data driven analysis. Machine learning can be an instrument in an overall ﬂow including domain experts and computer scientists. Adopted machine learning approaches should preferably be unsupervised, employing just the input data as a teacher and not requiring prior knowledge or a large amount of annotated data. This kind of approach can potentially unveil new unexpected discoveries, previously hindered by the complexity and amount of available data, or maybe corroborate intuitions. We propose a workﬂow comprising three parts: (i) achieving a compact representation of elements of the dataset by means of representation learning techniques, shifting the analysis from cumbersome representations to compact vectors in a latent space, (ii) visualizing results of this analysis in a 2D or 3D space (further reducing dimensionality of the space) and (iii) potentially clustering points associated to instances to suggest patterns to the domain experts that will evaluate their potential meaning within the domain. This work presents the rationale of the approach within a cloud based setting, and results on three specific astronomical research topics, namely the study of galactic supernova remnants, compact radio sources, and star forming clumps, exploiting imagery from infrared and radio continuum surveys."
X7-010,Micaela Menegaldo,Heidelberg Institute for Theoretical Studies,Unsupervised classification of simulated black hole shadows,"In April 2019, the Event Horizon Telescope (EHT) Collaboration released the first image of a black hole (BH) shadow. Theoretical models that aim to describe the environments of BHs are complex and highly-dimensional numerical simulations are often needed to outline the problem. While previous work has employed the use of machine learning (ML) algorithms to predict BH shadow model parameters from image data, we assess the suitability of a particular class of ML algorithms, namely self-organising maps (SOM), as a tool to classify simulated BH shadow images. We employ the SOM network PINK, which spatially compares visual input using a flip and rotation invariant similarity measure, to generate a set of representative BH shadow prototypes for a library of simulated images. Using this and the clustered input data parameter distributions, we find that the shadow ring size, which is related to BH mass in the model, is the dominant class determining factor of the images. Other model parameters, especially those that influence the orientation of the shadow on the image plane, were less influential on the clustering given PINK’s flip/rotation invariance. Despite this, PINK may be useful in determining persistent image-plane features of BH shadows for other model parameters, given a constant BH mass, in order to curate a subset of meaningfully different models that can then be used in more advanced analyses reducing the volume of data needing further consideration."
X3-020,Jon Carifio,Center for Astrophysics | Harvard & Smithsonian,A Novel Data Visualization Experience with PyWWT and JupyterLab,"AAS WorldWide Telescope is a suite of free and open source software for showcasing astronomical data and knowledge. One element of the suite of WWT offerings is PyWWT, via which one can manipulate WorldWide Telescope via Python. In this poster we present an interesting and novel UX for Jupyter-based data visualization using PyWWT, which provides an alternative to the standard notebook-widget paradigm.

In order to provide a powerful interactive experience for users, PyWWT uses an 'app'-based approach, where its frontend variations (Qt, ipywidgets, JupyterLab) communicate with a common backend, known as the WWT research app, using a custom messaging framework. This overall approach allows all of these variations to take advantage of improvements in the research app, and provides a unified user experience. The messaging framework itself is lightweight, with no dependencies on other elements of the WWT suite, which offers the potential for straightforward extensibility to other environments.

In the case of a Jupyter notebook, the backend app may be running in a remote location, with the Jupyter kernel used to send messages between Python and the research app. The newest PyWWT release (0.13.0) contains a mechanism which allows the Jupyter kernel to handle user-trigged pywwt code while simultaneously receiving updates from the WWT backend. This allows users to write code that interacts with WWT asynchronously while preserving the frontend experience.

As an example of the power of this functionality, we examine a newly-available WWT feature, which is the ability for a user to interactively select and examine sources from Hierarchical Progressive Surveys (HiPS) catalogs. The WWT messaging framework allows pywwt to be immediately notified when a user makes a selection, and allows the opportunity for the user to add callback functions to be triggered whenever such a message arrives."
X6-005,Lennart,UCT/IDIA,The frocc spectral imaging pipeline,"Modern radio interferometers, such as MeerKAT, GMRT, VLA, ASKAP, LOFAR, and the upcoming SKA, deliver data in unprecedented quality and detail, but also at an enormous rate and volume. To efficiently address present and future science goals, petabyte-sized input datasets and terabyte-sized outputs need to be processed within days.

With the frocc (Fast RadiO Cube Creation) pipeline I focus on the generation of spectro-polarimetric
data cubes, which enables the visualisation and analysis of the full Stokes target spectra. Frocc is highly automated, parallelised and optimised."
X7-011,Evan Jones,UCLA,Building a training set of hundreds of thousands of galaxy images,"We describe the process of collecting and processing millions of galaxy images to obtain a curated quality data set for training and performing photometric redshift estimation. Due to the massive scale of data coming from large scale astronomical surveys, machine learning techniques using galaxy photometry are increasingly adopted to predict galactic redshifts which are important for inferring cosmological parameters such as the nature of Dark Energy. We have compiled a new galaxy training dataset from the Hyper Suprime-Cam Survey. We describe the challenges faced in the process of preparing a training set from raw survey observations for the purpose of exploring probabilistic deep neural networks for estimating distances to galaxies (redshifts) from photometric images."
X3-021,André Schaaff,"Université de Strasbourg, CNRS, CDS",Inclusive CDS,"Authors: André Schaaff, Romain Berger, Grégory Mantelet (CDS)

One of the workshop topics of Space & astronomy research accessibility 2020 was ""Getting everyone connected: software and hardware interfaces that make a difference for professionals and students with disabilities in on-line work; making online accessible to everyone around the World.""

Accessibility and technology issues for people with disabilities were not discussed at IVOA.
IVOA has developed useful standards and protocols. This huge work is one of the keys of the Accessibility… but in the sense of FAIR !
It sounded great to us to introduce a noble human concern in addition to the usual Scientific and technological concerns. And to start looking for solutions to make it happen.
We may already have some of the keys for the Accessibility in the sense of “to all” (We are not starting from scratch).
But it is a wide field of investigation (Variety of data and services: textual, numerical, images (how to use Aladin if I am blind), etc. 

It is enough to use a Braille device ?, a voice interaction ?, etc.

It helps but we could probably take into account accessibility upstream, at the beginning of developments, of data display and services design
We have so much technologies: AI, TalkBack, Seeing AI, etc.

A chance is that we have IVOA standards and protocols: Provenance, MOCS, Semantics works (just examples). 
We have not only data but data++.

We have started a six month study in September 2021 to learn about disabilities, to draw up an initial inventory of the accessibility of CDS data and services for people with disabilities, to find solutions to improve the existing, to set up upstream new ways of presenting data and services, developing interfaces and tools towards an inclusive CDS ?
Is it Unrealistic and too hard ? Hard, yes but it is a first step

The unrealistic part would be to do it alone.
We propose to discuss it, to gather all the experiments (in Astronomy and other fields) and ideas (Slack channel, small workshops, SARA 2021? …)
It concerns potentially a large panel of IVOA Interest and Working Groups

At the end this work will benefit to all, with or without disabilities."
X7-012,giuliano taffoni,INAF-OATs,A distributed computing infrastructure for LOFAR Italian community.,"LOFAR (LOw Frequency ARray) is a low-frequency radio interferometer composed by observational stations spread across Europe and it is the largest precursor of SKA in terms of effective area and generated data rates.
In 2017, the Italian community officially joined LOFAR project and it deployed a distributed computing and storage infrastructure dedicated to LOFAR data analysis. The infrastructure is based on 4 nodes distributed in different Italian locations and it offers services for the Key Projects’ pipelines execution, storage of final and intermediate results and support for the use of the software and infrastructure. As the analysis of LOFAR data requires a very complex computational procedure so a container based approach has been adopted to distributed software environments the different computing resources. A science platform approach is used to facilitate interactive access to computational resources. In this poster we describe the architecture and main features of the Infrastructure."
