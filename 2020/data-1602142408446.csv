"paper_id","name","answer","title","abstract"
"B10-129","Peter Teuben","University of Maryland","How to better describe software for discovery and citation","In this BoF we propose to discuss a variety of items to improve how
software is described and can be discovered. We will invite and
actively search for contributions to this discussion. Some examples of
what we could cover:

1. The codemeta.json file, under control of software
authors. Including a working session to write your own (or base it on
the ASCL starter file). This file (or itâs cousin
CITATION.cff) will also improve software citation, and we will explain
how.

2. Possible options to expand the codemeta file, e.g. keywords
describing the API and its one-liners.

3. Improvements to the Unified Astronomy Thesaurus (UAT) such that
software is better covered.

4. Define a well defined field in astrophysics and take an inventory
of the software used to categorize them. A conference would be an
ideal event to get all the stakeholders together (we have a candidate
for this).

We encourage contributions to this BoF."
"B10-133","Yan Grange","ASTRON","Best licensing practices","In 2016 the FAIR principles, aimed at improving the Findability, Accessibility, Interoperability, and Reuse of digital assets were postulated. Since then the scientific community has been working on interpreting those standards and applying them to their data. 

Among these digital assets, software plays a special role, as software as a tool for data exploitation is diversely used and dynamically developed. Therefore, principles that apply to data can not literally be copied to software, especially in relation to licensing and citation.

During ADASS XXIX it became clear that several groups worldwide are working on formalising the licensing of software and other digital assets. For this session, we will coordinate a discussion focused on what policies and tools help in making software open and accessible, and thus more suited for community reuse."
"B11-143","Jan Noordam","Madroon","BoF: Interoperability of Users, Developers, and Managers","Listening to the presentations of a wide-ranging software survey by
the Radio Camera Initiative (RCI), one message emerges loud and clear:
""We have the algorithms and the tools (e.g. containers etc), but we
are still failing to bring these to bear in such a way that users,
developers and managers are induced to work together effectively"".

This BOF continues a venerable ADASS tradition of offering a platform
to participants for formulating and discussing profound thoughts on
the Future of Astronomical Dataprocessing Software (FADS). In one way
or another, over the years, the FADS discussions have always revolved
around this question. It is perhaps time to put it to rest, if we can.


     Developers (15 min)

There are many clever developers all over the world (cheap too), but
we are not very good at adopting their stuff and integrating it into
our systems.  There are many valid practical reasons for this, but it
slows evolution.

     Users (15 min)

The trend is towards processing pipelines that nobody dares to touch
once they work. This begets ignorant users, and slows evolution. Users
should be able to experiment, and to interact with developers.

     Software Managers (15 min)

Managers should not concern themselves with content. Their role is to
quietly create the conditions in which users and developers can evolve
our systems together.


     Tentative Conclusion (15 min)

The easy answer is that Python already offers many of the things we
need. Users around the globe can easily access a huge variety of high
quality software by means of a single powerful interface (the Python
language). They can combine these into custom recipes (pipelines!),
which they can share with others via email. Very importantly, the
interface disciplines the many developers, while allowing them to
concentrate on whatever they are good at. The Python system is managed
by invisible guardians, and is freely available.

So, is Python sufficient, and can we limit ourselves to writing a few
Python modules of our own? Or do we have to create possibilities which
Python does not provide (yet). In that case we should hope that Python
will eventually adopt the stuff that we develop ourselves.

Keywords: interface, evolution"
"B2-73","G. Bruce Berriman","Caltech/IPAC-NExScI","Cost Management on Commercial Cloud Platforms","Commercial cloud platforms are a powerful technology for astronomical research. The Event Horizon Telescope has processed much of its raw data on cloud platforms (Akiyama et al. 2019 - ApJL 875, L1; Kim et al. 2020; A & A, 640, A69 ) The Ice Cube neutrino experiment recently performed a similation experiment with 15,000 GPUs on three cloud platforms. Despite the benefits of cloud computing - such as on-demand scalability, and reduction of systems management overhead - confusion over how to manage costs remains for many one of the biggest barriers to entry, exacerbated by the rapid growth in services offered by commercial providers, and by the growth in the number of these providers. The confuses arises because storage, compute, and I/O are metered at separate rates, all of which can change without notice. As a rule, processing is very cheap, storage is more expensive, and downloading is very expensive. Thus an application that produces large image data sets for download will be far more expensive than an application that performs extensive processing on a small data set.

This BoF aims to quantify the above statement by presenting case studies of the costing of astronomy applications on commercal clouds, covering a range of processing scenarios, including:

- Hosting the Rubin Observatory Interim Data Facility on a cloud platform.
- Creating an all-sky mosaic of TESS survey images.
- Summary of a cost management workshop at IPAC.
- Launching Sci Server on a cloud platform.
- Managing cloud services at STScI

​ Discussion of these and other cases are intended to answer the address the following questions:

- What are the best practices that I can employ for estimating costs?
- How do I pick the best platform for my application?
- How do I take advantage of free or reduced costs services (educators or researchers credits; spot pricing; use an academic cloud...)?
- What are the best practices for optimizing performance and reducing my costs?
- What are the fiscal ""black holes"" that I can fall into?
- Where can I find all this information?

Organizers: Bruce Berriman (Caltech/IPAC-NExScI); Gerard Lemson (JHU); William O'Mullane (Rubin Observatory); Ivelina Momcheva (STScI); Andreas Wicenic (ICRAR)."
"B4-154","Jessica Mink","Smithsonian Astrophysical Observatory","Standardizing New and Improving Old Data Formats in Astronomy","While FITS is still our most common standard data format, incremental improvements could better carry it into the future as part of an ecosystem including other formats, both coequal and structural. What are those formats, and where are they in the standardization process? We'll have our annual status reports and maybe some new ideas."
"B4-95","Mateusz Malenta","The University of Manchester","It worked on my laptop! - how to approach reproducibility in astronomy?","With large observatories that provide data to thousands of astronomers around the world already online or in the design phase and under construction, it is now more important than ever to approach the problem of reproducibility in astronomy. The last few years have seen a wide adoption of solutions that aim to address some of the reproducibility concerns, such as containers and Jupyter Notebooks. They help to provide a consistent processing environment by, for example, locking users to a single version of Python. This can, however, provide a false sense of security as on the lowest level, these solutions do not take any possible hardware differences into account. On the higher level, the lack of clear software and data format documentation can lead to easily-avoided mistakes. This is especially important in the new era of multi-wavelength astronomy where teams from different backgrounds, using different tools and file formats, come together to solve the same problem.  

Considering all of the above, what do we expect from reproducibility? What are we willing to sacrifice to achieve it (and do we have to sacrifice anything at all?)? Can we as a wider community come together and develop a clear set of guidelines and standards that will ensure the maximum possible reproducibility? If 100% reproducibility is not possible, how can we ensure that all the relevant parties are aware of the possible shortcomings and can include them in their analysis?"
"B9-50","Katharina Lutz","CDS, CNRS, Observatoire Astronomique de Strasbourg","Radio data archives round table","With SKA precursor and pathfinder operations in full swing, radio and (sub-)mm astronomy is entering the era of super big data. The big questions is how to make (sub-)mm and radio data available to the astronomical community, preferably using FAIR (findable, accessible, interoperable and re-useable) principles. There are already a lot of efforts going on around the globe: facilities such as ALMA, LOFAR, MWA, NRAO and ASKAP are already publishing much of their data in the form of ""science ready"" image products, SKA regional centres are being formed and a radio astronomy interest group has been initiated within the IVOA. We want to use this BoF to bring everyone interested in this topic around one informal, friendly, virtual table to hear about and discuss the following questions: Where are different groups in their efforts to expose both visibility and science ready data? What is already there, maybe has been used for decades by traditional observatories? Which pieces of information or technology are still missing? Where do we want to go, what needs to happen next?
We will start the BoF with short presentations from active players around the world and then look forward to a discussion with all attendees."
"B9-56","Mathieu Servillat","LUTH - Observatoire de Paris","Practical Provenance in Astronomy","Recently the IVOA released a standard to structure provenance metadata and several implementations are in development in order to capture, store, access and visualize the provenance of astronomy data products. This BoF will be focused on practical needs for provenance in astronomy. A growing number of projects express the requirement to propose FAIR data (Findable, Accessible, Interoperable and Reusable) and thus manage provenance information to ensure the quality, reliability and trustworthiness of this data. The concepts are in place, but now, applied specifications and practical tools are needed to answer concrete use cases. We propose to discuss which strategies are considered by your projects (observatories or data providers) to capture provenance in your context and how you consider a end-user might query the provenance information to enhance her/his data selection and retrieval. The objective is to identify the development of tools and formats now needed to make provenance more practical."
"B9-86","cosimoNigro","Institut de Fisica d'Altes Energies (IFAE)","Standardisation of Data Formats in Gamma-ray astronomy","The operation of the next-generation of gamma-ray telescopes as observatories poses gamma-ray astronomers the issue of opening their data and software to a wider community.    
A first attempt at defining a common scheme for high-level gamma-ray astronomical data has been initiated by members of different Imaging Atmospheric Cherenkov Telescopes (IACT)  experiments with the ""Data formats for gamma-ray astronomy"" forum (https://gamma-astro-data-formats.readthedocs.io/en/latest/).    
The forum, consisting of a series of documentation pages hosted on git, defines specifications to reduce high-level gamma-ray data to lists of candidate photons and instrument response function, stored in FITS files.    
Open-source softwares for gamma-ray analyses have recently developed to support this format and, as a result, a series of publications relying on standardised datasets and software have been recently issued, among them:
- The first public release of H.E.S.S. data (https://arxiv.org/abs/1810.04516);
- the first reproducible multi-instrument analysis of the Crab Nebula (https://arxiv.org/abs/1903.06621);
- an analysis of the H.E.S.S. data with the open-source software ctools (https://arxiv.org/abs/1910.09456);
- a validation of open-source gamma-ray analysis software employing the same dataset (https://arxiv.org/abs/1910.08088).     

This talk will provide a discussion on the standardisation effort and an overview of the projects that have already employed it."
"D1-102","Mario Juric","DiRAC Institute, University of Washington","Checkpoint, Restore, and Live Migration for Science Platforms","We demonstrate and discuss a functional prototype of (per-user) checkpoint, restore, and live migration capabilities for JupyterHub platforms. Checkpointing -- the ability to freeze and suspend to disk the running state (contents of memory, registers, open files, etc.) of a set of processes -- enables the system to snapshot a user's Jupyter session to permanent storage. The restore functionality brings a checkpointed session back to a running state, to continue where it left off at a later time and potentially on a different machine. Finally, live migration enables moving running Jupyter notebook servers between different machines, transparent to the analysis code and w/o disconnecting the user. Our implementation of these capabilities works at the system level, with few limitations, and typical checkpoint/restore times of O(10s) with a pathway to O(1s) live migrations. It opens a myriad of interesting use cases, especially for cloud-based deployments: from checkpointing idle sessions w/o interruption of the user's work (achieving cost reductions of 4x or more), execution on spot instances w. transparent migration on eviction (with cost reductions up to 3x), to automated migration of workloads to ideally suited instances (e.g. moving an analysis to a machine with more or less RAM or cores based on observed resource utilization). The capabilities we demonstrate can make science platforms fully elastic and scalable while retaining excellent user experience."
"D1-66","Oleg Smirnov","Rhodes University & South African Radio Astronomy Observatory","Radiopadre: remote, interactive, zero-admin visualization of data pipeline products","Modern [not only radio] astronomers are coming to terms with being separated from their data and pipelines. Sheer data size alone dictates that data reductions are hardly ever “local” in any sense, but rather have to run on a big node or cluster somewhere remote, with SSH gateways and network latency in between. The new work patterns of the covid-19 pandemic only exacerbate this separation. At the same time, the complexity of new telescopes and pipelines results in a far greater volume and variety of intermediate diagnostics and final data products. The following scenario is becoming familiar: my pipeline run has finished (or crashed), it’s produced 300 log files, 200 intermediate plots, 50 FITS images, and a dozen HTML reports -- on a remote cluster node, which doesn’t even have a basic image viewer installed (and which network lag would have made difficult to use in any case). How do I make sense of all this, without transferring gigabytes of products to my laptop or local workstation first?

Radiopadre (Python Astronomy Data Reductions Examiner, https://github.com/ratt-ru/radiopadre) provides (at least part of) the answer. It is a combination of a client-side script, Docker or Singularity images, a Jupyter Notebook framework, and integrated browser-based FITS viewers (CARTA and JS9) which allows for quick visualization of remote data products. Radiopadre is virtually zero-admin, in the sense that it requires nothing more than a web browser on the client side, an SSH connection, and Docker or Singularity support on the remote end. It allows for both interactive (exploratory) visualization via a Jupyter Notebook, as well as the development of rich, extensive report-style notebooks tuned to the outputs of a particular pipeline.

The demo will showcase the interactive visualization capabilities of Radiopadre, using the output of various MeerKAT imaging pipelines as a working example."
"D2-109","Demitri Muna","University of Texas, San Antonio","Introducing the Trillian Framework and Science Data Descriptors","This demo is a hands on introduction to the newly released Trillian framework. The Python package and supporting API will allow astronomers to retrieve data by simply describing it. For example, consider the statement ""all Galex images within two square degrees of ra=12°34’42”, dec=47°22’52” ”. This description completely and unambiguously defines a known set of data. The Trillian framework will allow one to directly work with data from nothing more than such a description, regardless of where the data is, what interface is required to retrieve it, what files are needed, and what format it is in. The demonstration that can be followed along with will show how multiple data sets can be accessed and how to contribute to the project to add more, plus how this framework is designed to scale from a laptop to a full cloud computing solution. A detailed description of scientific data descriptors will be also presented, a new scheme for addressing data that is one of the components of Trillian but is useful independently outside of the framework. Finally, a brief demonstration will be given of Cornish, a new Python interface around the Starlink AST library that is used to work with regions on a sphere."
"D2-29","Guillaume Eynard Bontemps","CNES","Pangeo Data Analysis Platform at CNES and Astronomical Use Cases","Pangeo is first a scientific community, but also a Python software ecosystem and a platform that can be deployed on many infrastructures. Its goal is to provide means ways to make scientific research and programming easier on big datasets coming from simulations ran on HPC clusters (climatic model) or from sensors like observation satellites.

In this demo or talk, we will see how a scientist or an engineer will be able to analyze and process huge data volumes interactively, in a few lines of code, using the software components that are at the heart of Pangeo: Jupyter, Dask and Xarray.

These main pieces of software will be presented:
- Jupyter is the main graphical interface, it advantageously replaces a terminal.
- Dask allows scaling computations and data analysis through many nodes or virtual machines.
- Xarray gives a high level representation of multi-dimensional scientific data.  

We will also describe the main possibilities for deploying a Pangeo platform: your personal laptop, a public cloud provider or an HPC cluster.

Finally, we will demonstrate Pangeo stack usage through some concrete use cases:
- Statistic computations and visualisation of Gaia data catalog.
- A multi-temporal analysis on Sentinel 2 satellite tiles, in order to watch the evolution of the NDVI (Normalized Difference Vegetation Index) on its pixels."
"D7-114","Cédric Foellmi (@arcsecond_io)","F52 Technologies","Arcsecond.io: an hybrid commercial/citizen-science project for astronomical observations","Arcsecond.io is an online SaaS platform aiming at offering cloud services for astronomers, collaborations and observatories. A significant part of it is public and free, with some pro tools that have been collected on the web and rewritten using modern techniques. It is backed by a rich backend of REST APIs, which offers a large number of free and public endpoints too.

However, Arcsecond.io is also a commercial venture, proposing cloud services dedicated to astronomical observations. In particular, small observatories can benefit from very competitive data storage dedicated to astronomical data, as well as sophisticated management tools to handle Night Logs, Datasets, Data Packages etc.

This focus demo will attempt to show parts of Arcsecond.io: Pro Tools, iObserve on the web with Startrack satellite checks, as well as Data Storage, Collaborations and Observatory Portals."
"D8-132","Simon Perkins","South Africa Radio Astronomy Observatory","Parallel Radio Astronomy Application Development with Dask and Numba","Dask is a lightweight Python parallelisation and distribution framework that seamlessly integrates with the PyData ecosystem to provide a rich framework for developing Parallel and Distributed Radio Astronomy Applications.

In this demo, we will demonstrate how to create a multi-core radio astronomy application using a combination of Dask and Numba. 

Time permitting, we aim to cover the following topics:

Dask

* General compute graph concepts.
* The Dask Array abstraction and its relation to NumPy arrays
* Dask Array chunk size effects on application memory and performance.

dask-ms 

* Exposing CASA Tables as xarray Datasets and Table Columns as Dask Arrays
* Choosing appropriate chunking strategies
* Updating and writing CASA Tables from Dask Arrays

Numba

* JIT-compiling Python code to speeds similar to the equivalent C/Fortran code.
* The relation between performance, input size and dropping the Python Global Interpreter Lock.
* Wrapping accelerated Numba code in Dask for multi-core performance.

Dask Distributed

* Scaling a Dask application up to a High Performance Computing cluster.
* Management of the cluster with dask_jobqueue.
* Annotation and manual scheduling of specific tasks for optimal placement and performance."
"H10-232","William Pence","NASA's HEASARC","CFITSIO - 3 decades of collaborative software development","CFITSIO was originally developed at NASA’s High Energy Astrophysics Science Archive Research Center (HEASARC) in 1991 as a simple set of C and Fortran callable subroutines for reading and writing keywords and data arrays in FITS format disk files.  This enabled the HEASARC’s data analysis software to directly process the FITS files in its archive.  Most other astronomical data analysis systems at that time required that FITS format files be imported into a local data format for processing, and the final data products then had to be translated back into FITS format before placing them in the public archive.

The ADASS conferences, which coincidentally also started in 1991, directly led to the development of many major enhancements in the CFITSIO software over the following decades.  This talk will describe some of the collaborations with more than 70 contributors, many of whom have been participants in the ADASS conferences over the years. This provides a good example of the benefits of open source software development."
"I10-39","Kelle Cruz","CUNY Hunter College","Astropy","TBD"
"I1-159","Vandana Desai","Caltech/IPAC-IRSA","Enabling next-generation science investigations with the NASA Astrophysics Archives","TBD"
"I2-60","Giuliano Taffoni","INAF - OATs","All the shades of the Cloud","Cloud computing is a powerful technology that in the last decades revolutionized computing and storage in particular for Industry and Private Sectors. Today, large investments are on-going to build public Cloud Infrastructure at National or International level (e.g. the European Open Science Cloud Initiative). Also, scientists are approaching commercial and public Clouds at different scales: single researchers test the clouds for small research projects, at the same time large international collaborations are evaluating Cloud technology to collect, process, analyze, archive and curate their data. 
The experience of large scientific instruments in the last years demonstrates how experiments are critically dependent on computing, data processing and storage infrastructures and our ability to utilise them through codes and algorithms.  This will become more and more important with the advent of new class instruments (e.g. SKA, CTA, LSST, EUCLID)  where new computational challenges will be faced. This will lead to the use of new infrastructures where exascale HPC and Clouds will converge to answer new challenges of (Big-) data analysis and (Big-) data analytics (HPDA).
New technologies (e.g. containerization) are driving the convergence of these “worlds” and the advent of science platforms as mean to access data, storage and computing (but also software and algorithms) is facilitating the use of cloud at different scales.

In this talk, I will discuss the actual use of  Cloud in Astrophysics at different scale using some examples and I will present future trends and possibilities that the use of cloud computing and its convergence with HPC will open: from HTC to HPDA, from scientific computing to data analytics."
"I3-20","Samuel Farrens","CEA Paris-Saclay, CosmoStat","PySAP: From Galaxies to Brains and Beyond","During this talk I will present the PySAP (Python Sparse data Analysis Package) package, a multidisciplinary image processing tool developed in collaboration between astrophysicists and biomedical imaging experts at CEA Paris-Saclay through the COSMIC project. I will provide some background on the core mathematical tools implemented in PySAP as well as demonstrating some of the diverse applications such as galaxy image deconvolution and magnetic resonance image reconstruction. I will also endeavour to relate how we managed the development of this package. Finally, I will conclude by sharing our plans for the future of PySAP and how this may impact other imaging domains."
"I5-175","Kai Polsterer","HITS gGmbH","From Photometric Redshifts to Improved Weather Forecasts: an interdisciplinary view on machine learning in astronomy","The amount, size, and complexity of astronomical data-sets is growing rapidly in the last decades. Now, with new technologies and dedicated survey telescopes, the databases are even growing faster. Besides dealing with poly-structed and complex data, sparse data has become a field of growing scientific interest. By applying technologies from the fields of computer sciences, mathematics, and statistics, astronomical data can be accessed and analyzed more efficiently.

A specific field of research in Astroinformatics is the estimation of the redshift of extra-galactic sources, a measure of their distance, by just using sparse photometric observations. Observing the full spectroscopic information that would be necessary to directly measure the redshift, would be too time consuming. Therefore building accurate statistical models is a mandatory step, especially when it comes to reflecting the uncertainty of the estimates. Statistics and especially weather forecasting has introduced and utilized proper scoring rules and especially the continuous ranked probability score to characterize the calibration as well as the sharpness of predicted probability density functions.

This talk presents what we achieved when using proper scoring rules to train deep neural networks and to evaluate the model estimates. We present how this work led from well calibrated redshift estimates to an improvement in statistical post-processing of weather forecast simulations. The presented work is an example of interdisciplinarity in data-science and how methods can bridge between different fields of application."
"I6-25","K. Azalee Bostroem","University of California, Davis","Developing a Data Carpentry Curriculum for the Astronomical Community","As the astronomical community moves into an era of big data, the paradigm of data processing is changing. We are transitioning from local end-to-end data processing (from taking or simulating observations to publishing the data) to retrieving pre-processed large datasets through database queries. The growing importance of such transactions are evident with current projects such as Tess, Gaia, SDSS, ZTF, HST, and Illustris and will become a necessity to fully utilize the next generation of astronomical surveys, telescopes, and simulations.  Interaction with these databases and visualization of these complex datasets will be essential skills. However, these skills are not part of the standard astronomical curriculum and training, thus far, has been focused on tools specialized to individual projects. In this talk, I will discuss my work developing an open source Data Carpentry curriculum for Astronomy, that builds on the software skills taught in the Software Carpentry curriculum. I will highlight key aspects of this work that have made it a success thus far, including funding for lesson development, drawing on the existing infrastructure and personal resources of the Carpentries,  building a core development team with experience in astronomy, education, and industry, and creating a broader network across the astronomical community that provided input at key points throughout the development process."
"I6-265","Matthew J Graham","California Institute of Technology","What did we get right? Lessons learned from the first 300 million alerts of ZTF","The Zwicky Transient Facility (ZTF) has been serving transient alerts to the astronomical community for 2.5 years and it has just passed the 300 million mark, announcing the detection of almost 5,000 supernovae, 25 tidal disruption events, numerous asteroids, and other astrophysical phenomena in that time frame. The Rubin Observatory will reach this milestone within the first three months of alert generation and efforts are underway to ensure that the necessary infrastructure for alert dissemination, reception, and response is in place. We are building, however, on concepts envisaged well over a decade ago and in this talk, I will review the trials and tribulations of ZTF in seeding such a framework and consider what might we have done differently. In particular, our concerns regarding scale and followup capability merit revisiting and potentially a new vision developed of what the landscape might actually look like during the era of LSST operations."
"I6-31","Dmitry Duev","Caltech","Machine Learning for the Zwicky Transient Facility","Astronomy, as many other branches of science, has been experiencing an explosive increase in the data volumes, doubling every two years or so. At the forefront of this revolution, the Zwicky Transient Facility (ZTF) – a robotic optical sky survey currently in operation at the Palomar Observatory in Southern California – performs accurate measurements of over a billion of astronomical objects and registers ~millions of transient events (such as, for example, supernova explosions, brightness changes in variable stars, or asteroid detections -- distributed to the world in real time via alert streams) in the dynamic sky every (clear) night. Machine and deep learning play an essential role in making sense of these vast quantities of data. In my talk, I will discuss the wide range of applications of machine learning in ZTF, including the astrophysical object classification, identification of near-Earth objects, detection and localization of comets, and cataloging/studying the source variability."
"I7-21","Becky Smethurst","Christ Church, University of Oxford","Galaxy Zoo & The Zooniverse - citizen science as a powerful tool in the age of big data","The success of the citizen science project Galaxy Zoo, launched back in 2007, has demonstrated the impact that hundreds of thousands of volunteers can make in a field of research. Whilst a powerful science communication and outreach tool, citizen science also allows for the classification of huge data sets in the era of ‘big data’. I will review some of the latest results from the Galaxy Zoo project, including re-evaluating Hubble’s galaxy classification system and some of the unexpected objects discovered. The success of Galaxy Zoo, saw the launch of the Zooniverse platform, now hosting thousands of citizen science projects across hundreds of disciplines; from zoology to history. Even as the pace of development of machine learning continues to increase, I will argue that human classification will still have its place in science in the years to come, with particular focus on the impact in astronomy with the advent of the LSST."
"I8-105","Eugene Magnier","University of Hawaii","The Pan-STARRS IPP : Lessons Learned and Looking Forward","The Pan-STARRS Image Processing Pipeline (IPP) has been running on
data from the Pan-STARRS telescopes for a dozen years.  To date, the
system has processed over 1.5 million exposures representing more than
2 Petapixels (2e15 pixels).  The IPP operates on data from both
Pan-STARRS telescopes, and has also been used to process CFHT Megacam
and Subaru HSC images.  In this talk, I will discuss the successes and
lessons learned from a decade of large-scale data processing, and
highlight our plans for future improvements."
"I8-57","Melanie Krips","IRAM","NOEMA: How 1Million lines of code make the optically dark universe visible","The upgrade of the former IRAM Plateau de Bure Interferometer toward
the NOrthern Extended Millimeter Array (NOEMA) by doubling not only
the number of antennas from 6 to 12 but also quadrupling the
instantaneous bandwidth with a next-generation correlator has
necessitated a large number of software upgrades, from antenna control
over calibration, monitoring and reduction pipelines, to new mapping
and analytical tools. In this talk, the major milestones and
challenges 
of this multi-layer and cross discipline expansion will be presented. 
The adaptation to even bigger and more sensitive data coming from 
powerful instruments such as NOEMA paves the way for a major step 
forward in mapping the optically invisible universe."
"I9-23","Fernique Pierre","Observatoire astronomique de Strasbourg, Université de Strasbourg","Data Interoperability - the CDS experience","""Provide astronomers with simple, efficient and rapid means of accessing the reference data (bibliography, catalogues, object identifiers, images, spectra, time series, ...) necessary for their research"". Since its creation in the early 1970s, the Strasbourg astronomical Data Center (CDS) has addressed this challenge. The context and technologies have evolved from post mail to touchscreen devices, but the challenge remains: identify, collect, homogenize, describe data, and then redistribute it so that the knowledge can be re-used. What is called nowadays: the FAIR principles. At the heart of this challenge: the data interoperability. Standards, data serialization, metadata, dictionaries, tools, languages, semantics ... Based on our experience at CDS, we will try to identify the good, but also the bad, of the various interoperability solutions that have marked the evolution of CDS: those that work, those that failed. We will draw some lessons from the past experiences, and try to consider what the interoperability might enable for future astronomical projects."
"O10-53","Henrike Fleischhack","Catholic University of America/NASA Goddard Space Flight Center/CRESST II","The Multi-Mission Maximum Likelihood Framework threeML: Multi-wavelength astronomy in practice","The Multi-Mission Maximum Likelihood framework (threeML) is a flexible python-based framework for multi-wavelength data analysis in astronomy. ThreeML allows joint likelihood fits of data recorded by many different instruments, from radio to gamma rays. This is achieved by encapsulating data access into instrument-specific plugins, leaving the rest of the analysis agnostic of the data format. In this talk, I will outline threeML's design and major components, with a focus on the modeling language (astromodels) and the data-access plugins. I will show use cases and analysis examples, as well as explain how to add new plugins for instruments/data formats that threeML does not yet support."
"O1-112","Lloyd Harischandra","Australian Astronomical Optics - Macquarie University","Realtime Data Transferring and Processing from Remote Telescopes Using Apache NiFi and MongoDB","Data central at AAO-MQ provides data archiving and hosting solutions to several external telescopes such as Anglo Australian Telescope(AAT) and Huntsman Telephoto Array in Australia. Up until recently AAT raw data had been manually processed and transported to be archived at AAO-MQ for the astronomical community to access. This was time consuming and took extra resources to manage the data flow. In this talk, we will discuss how we automated the data transfer and archiving using Apache NiFi and MongoDB resulting in raw data being available for the astronomers to query in real-time as they are produced by the telescope.

We examine the usability, flexibility and scalability of Apache NiFi and various processors and controllers available to collect, process and move data across multiple data points. We will discuss great features available in NiFi especially for data provenance, guaranteed delivery and loss tolerance. We will also discuss what flow files and flow templates are, the role of the Data Flow Manager and how to schedule processors. In addition to that we will show you how to secure site-to-site data transferring and utilise NiFi clustering to balance the load.

Lastly we will show how survey teams can write their custom data processing tasks and data reduction pipelines in their favourite language(Python, R etc.) and integrate them into the data flow to be automatically processed and ingested into their favourite data stores for further processing or public availability."
"O1-115","Nick Cox","ACRI-ST","Development of Scientific Data Applications for Science Cloud Platforms","In the era of big data and cloud storage and computing, new ways for scientists to approach their research are emerging, which impact directly how science progresses and discoveries are made. For astrophysics and particle physics, initiatives such as ESA Datalabs and ESCAPE aim to offer science data processing and analysis platforms to researchers to help them explore increasingly big data sets generated by space missions and large facilities. In this context there is also a need to provide sophisticated and targeted science data applications to be deployed in different cloud environments.

We will present ongoing inter-linked projects that address this need for cloud-deployed applications for the scientific exploitation of space- and ground-based astrophysics and planetary science data. These projects bring together scientists and engineers from public institutions and private actors and leverage artificial intelligence and human intelligence to provide collaborative, cloud-enabled thematic science applications focussed for the global space science community. Topics include Gaia science, Lunar exploration, data fusion tools for the JWST MIRI instrument, and machine learning tools for selected use cases in planetary science.

Our goal is to deploy containerised Scientific Applications and similar initiatives for uptake by the space science and astrophysics community to discover and exploit new science products in an open and FAIR approach."
"O11-156","Brigitta Sipőcz","DiRAC Institute, University of Washington","astroquery: a community driven collaboration of data providers and data consumers","astroquery (http://astroquery.readthedocs.io) is an Astropy coordinated package and is already well embedded in and a key component of the astronomical Python ecosystem. astroquery arose from users’ desire to access databases from the Python command line in a scriptable fashion. Script-based data access provides the ability to make reproducible analysis and pipelines in which the data are retrieved and processed into scientifically relevant results with minimal user interaction, thus astroquery enables the creation of fully reproducible workflows from data acquisition through publication. The package currently consists of over 50 modules that provide direct access to the most commonly used databases, archives and services. While most of the modules share a similar API, they are meant to be used independently. 

astroquery has received significant code contributions from throughout the astronomical community, including both the user community and major archives. 
This talk will describe the philosophy, basic structure, and development model of the astroquery package as well a roadmap for future development. We aim to smooth the coordination between maintainers and the observatories even more and hope the talk will be a seed to further discussions."
"O1-124","Stefano Alberto Russo","INAF","A microservice-oriented science platform architecture","A Science Platform (SP) is an environment designed to offer users a smoother experience when interacting with data and computing resources. Usually when referring to a SP, we assume a web based environment based on Jupyter notebooks and software containerisation, however also other approaches or definitions of SP may be feasible (e.g. full interactive desktop access).

In current SP architectures, the software available to the user is usually pre-defined and with little or not control by the users. This means that is almost impossible for a user to run a computing task using a specific analysis software (i.e. a specific version, a specific set of dependencies, or just a software not supported by the SP), and that from an administrative prospective there is a strong overhead in supporting the various (yet limited) software versions. 

We tested several approaches to overcome these limitations and we propose a solution based on changing the prospective and framing user task of the SP as microservices - independent and self-contained units. This enables great flexibility, security and sustainability, and empowers the users to choose, build and run their own software environments, which also improves reproducibility.

We successfully used this architecture in the context of the ESCAPE project, at INAF, where we implemented the computing tasks microservices both as Docker and Singularity containers. The first impressions and user feedback are very promising, in particular for interactive desktop applications for data reduction and analysis."
"O11-35","Francesco Pierfederici","IRAM","Organising ADASS in a Pandemic","ADASS (Astronomical Data Analysis Software & Systems) is a medium-size conference catering astronomers and engineers from all over the world. It is traditionally held every year in the fall in a different country, hosted by an astronomy organisation in that country. Or at least this is what has happened for the past 29 editions.

The 30th edition of the ADASS conference series was supposed to take place in Granada, Spain. We were supposed to bask in the sun enjoying southern Spain food and hospitality. Little did we know that our plans had to change dramatically because of a tiny threat with a long name. The ""severe acute respiratory syndrome coronavirus 2"" has been causing havoc all over the world for a number of months already and has reached pandemic proportions.

It quickly became clear by spring 2020 that our hopes for a quick end to the pandemic were quite naive and that an in-person conference was simply not going to be possible. Not even in November. The ADASS 2020 LOC had to quickly pivot to an online conference format. 

This talk describes in detail all the lessons learned in switching from a relatively advanced traditional conference organisation to an online event. It will hopefully be of use to future organising teams of other online scientific and technical meetings."
"O11-96","Cristobal Bordiu","Osservatorio Astrofisico di Catania (INAF)","Astronomical research in the next decade: trends, barriers and needs in data access, management, visualization and analysis","Astronomy is entering uncharted territory. The advent of next-generation observing facilities, such as the Large Synoptic Survey Telescope (LSST) and the Square Kilometre Array (SKA), able to map large regions of the sky with unprecedented detail, will result in a data deluge overwhelming the current management and analysis capabilities of the astrophysics community.

In the era of the PB-scale datasets, NEANIAS (Novel EOSC Services for Emerging Atmosphere, Underwater & Space Challenges) emerges as a unique opportunity to tackle the needs of the next-decade astronomy in advance. This ambitious H2020 project foresees the creation of a set of services aimed at boosting astronomers' capabilities in three major fronts: data management and visualization, enabling access to large datasets in a straightforward way under FAIR principles and exploiting novel visualization techniques such as Augmented Reality; map-making and mosaicking, providing workflows to generate large-scale multidimensional maps and fostering multimessenger astronomy; and data analysis supported with Machine Learning techniques, automating the extraction and characterization of compact sources and extended structures from all-sky surveys.

However, the development and deployment of such services represent a substantial scientific and technological challenge, that requires an accurate portrait of the current research landscape in the astrophysics community. With the aim of better understanding the current barriers and needs in space research, we have conducted a survey among astrophysicists and data scientists from several research institutions across Europe, including also the private sector, spanning a wide range of disciplines and expertise levels. In this talk, we will present the preliminary results of the survey, with a particular emphasis on the adoption of Machine Learning techniques and Open Science practices. This research opens a direct communication channel between NEANIAS and the scientific community, providing valuable inputs that will allow us to better tailor the foreseen services to actual research requirements. In this way, we will create a solid foundation to face the astronomical challenges of the next decade."
"O1-22","Mark Taylor","University of Bristol","TOPCAT Visualisation over the Web","The desktop GUI catalogue analysis tool TOPCAT,
and its command-line counterpart STILTS,
provide among other capabilities visual exploration
of locally stored tables containing millions of rows or more.
They offer many variations on the theme of scatter plots,
density maps and histograms, which can be navigated interactively.
These capabilities have now been extended to a client-server model,
so that a plot server can be run close to the data files,
and remote lightweight HTML/Javascript clients can configure
and interact with plots based on that data.
The interaction can include pan/zoom/rotate navigation,
identifying individual points, and potentially subset selection.
Since only the pixels and not the row data are transmitted to
the client, this enables flexible remote visual exploration of
large tables at relatively low bandwidth.
The web client can request any of the plot options available
from TOPCAT/STILTS.
Possible applications include web-based visualisations
of static datasets too large to transmit,
visual previews of archive search results,
service-configured arrays of plots for complex datasets,
and embedding visualisations of local or remote tables
into Jupyter notebooks."
"O1-32","gijsverdoeskleijn","OmegaCEN, Kapteyn Astronomical Institute, University of Groningen","Serving power-users and novices. Architecture and lessons learned with science platform MuseWISE.","Astronomers are data-centric cowboys: they hunt, sometimes unprincipled, for the best-ever calibrated data. 

A key challenge of science platforms is to cater to a community of astronomers that ranges from power-users to novice users. Astronomical power-users have advanced IT skills, know how to navigate archives of databases and bulk dataservers. They want these archives to be interoperable with their own suite of favorite software for data analysis. Power-users have a bi-directional workflow: they want to share through science platforms their improved versions of calibrated data, their subsequent data analysis. They want full data lineage to meet the standard of scientific reproducability.  Novices to data-intensive astronomy know only basic Python/C/sql, need tutorials, intuitive user interfaces and close guidance in  finding, accessing and analyzing calibrated data. They start with a one-directional workflow: downloading final products from static releases in archives. 

With science platform MuseWISE we have 7 years of experience and lessons learned in serving power-users and enabling novices to grow their skills on one science platform.  MuseWISE provides a platform for the Integral Field-Unit spectroscopic data of MUSE at the VLT. The people, databases,  bulk data storages and compute infra for MuseWISE  are spread over seven locations in France, Germany, Switzerland and the Netherlands.  MuseWISE provides its users a platform for science analysis, data quality assessment, data processing and archive exploration. Power-users contribute and improve both code and data products. They  use their expert knowledge of the information system and its structure. We started an experiment with Jupyter notebooks plus JupyterHub to support the novice that will transition from a consumer to a contributor.   

In this talk we review the data-centric architecture of MuseWISE and its relation to data-centric cowboys. We present the lessons learned in seven years of operations across Europe."
"O1-68","Matthieu Baumann","CNRS - Observatoire Astronomique de Strasbourg/CDS","Aladin Lite v3: behind the scenes of a major overhaul","Since its first version in 2013, Aladin Lite has gained significant traction and usage as an HiPS viewer running in the browser. Designed to be easy to embed, it is now used in more than fifty websites and portals in the professional astronomy community. Aladin Lite has been adopted as the sky visualisation component of popular applications: ESA Sky, ESO Science Archive or ALMA Science Archive.

We present a major overhaul of Aladin Lite taking advantage of the GPU with WebGL, and which responds to requests of users, developers and integrators in a context where browser-based applications and science analysis platforms are increasingly important. 

While keeping the strengths of the original code, Aladin version 3 will introduce several new features: support of multiple projections (Aitoff, Mollweide, Orthographic, Mercator), support of FITS tiles, display of FITS images, heatmaps visualisation of catalogue data, improved rendering pipeline and coordinates grids.
We will give an overview on the architecture used to develop these new functionalities, based on existing Rust code transpiled to WebAssembly, a portable high-performance low-level bytecode for the web supported in all modern browsers. We will also outline the technical challenges and limitations we encountered.
Short video footage sequences will demonstrate the existing prototype throughout the presentation.

These improvements have been partially supported by the ESCAPE project and will also benefit to ipyaladin, the widget enabling the usage of Aladin Lite in Jupyter notebooks."
"O1-98","Fenja Kollasch","HITS","Interactive exploration framework for big data sets","Astronomy has been a discipline that often suffers from big data issues. Large amounts of data are observed and need to be processed in order to find interesting phenomena. Automatic or semi-automatic approaches are welcome to solve this cumbersome task. In the past, several machine learning methods were proposed to organize, classify, or condense big data sets. However, this is not the end of the road. In most cases, researchers need to take further analysis by hand on automatically preprocessed data to gather valuable conclusions.

To facilitate the pipeline of data analysis, we suggest a generic front-end framework allowing the user not only to process the data automatically, but also to interactively explore and investigate the results of machine learning procedures. A compact visualization gives an initial overview and can be adjusted to point out the parts of interest. By providing abstract accommodation functions such as zooming, scrolling, filtering, and labeling, crucial data fragments can be found and marked in an intuitive way.

We present the prototype UltraPINK in order to demonstrate the idea of such an explorative visualization interface. UltraPINK is a web application to train, store, load, and browse self-organizing Kohonen maps. By using the Parallelized rotation and flipping INvariant Kohonen maps framework (PINK) as a back-end, a clear representation of common shapes in the data set is generated and displayed. The closeup-view of a map shows the generated prototypes and enables different methods to analyse it. Single prototypes can be selected and used to find the data points that resemble this prototype the most. Furthermore, prototypes can be labeled, whereby the given label can be transferred to all similar data points. It is also possible to view and label outliers that do not resemble one of the prototypes. All annotations that have been made to the original data-set are downloadable in various formats.

While our prototype is currently tailored to the PINK back-end specifically, the investigation and labeling functions underlie a generic pattern that can easily be adapted for various kinds of input data and machine learning algorithms. The ultimate goal would be an abstract framework accepting all different data types and algorithms."
"O2-14","Vicente Navarro","European Space Agency","ESA Datalabs: an e-Science Platform for Data Exploitation and Preservation at ESAC","Since the appearance of e-science as a tangible approach – a data-intensive approach to science, geared towards discovery – astronomy has been arguably the most successful example: it is a perfect fit for a data-intensive approach since most data is public and free of privacy concerns or commercial value. Also, and more recently, we have entered what could be called the golden age of surveys, with several large-scale projects, spanning decades, between finished, ongoing, and planned activities. ESA is responsible, or is a major partner, in several of these initiatives.

This change is profound and data has become the major technological challenge. Increases by multiple orders of magnitude in dataset size means that transferring data to a scientist is often unfeasible. But size is only one of the aspects in a data-intensive domain. There are layers of ingestion, curation and analysis happening in parallel and across many communities. Preservation is vital and is, in general, a largely unsolved problem, both in the technology side and in public policy. Finally, curation and analysis also create new challenges that intersect with a push for open science.

We present the current status in the development of the ESA Datalabs platform. This system allows users to bring their code to ESA’s infrastructure and have direct access to ESA’s archives. Datalabs are full computational environments and our catalogue of Datalabs ranges from new tools that have become de-facto standard for analysis, to complex legacy systems repackaged to run via a web browser. ESA Datalabs underlying architecture is domain agnostic; it fosters research and innovation through the integration of transversal access to big data, containerised applications, notebook technologies and domain specific software. For example, customised JupyterLab environments are readily available for astronomers, scientists in Earth Observation related fields, or researchers in global navigation. Moreover, ESA Datalabs support for development environments such as Octave, or reference tools such as TopCat in astronomy enable reusability of existing code baselines. 

We will discuss the challenges faced in developing a multi-domain exploitation platform capable of fulfilling user requirements that vary from execution of simple notebooks, to machine learning algorithms, to science pipelines. Finally, we will show functionalities already available for users as well as the future evolution plan."
"O3-130","Lucia Marchetti","University of Cape Town / IDIA","iDaVIE-v: immersive Data Visualisation Interactive Explorer for volumetric rendering","We present the beta release of iDaVIE-v, a new Virtual Reality software for data cube exploration. The beta release of iDaVIE-v (immersive Data Visualisation Interactive Explorer for volumetric rendering) is planned for release at the end of 2020. iDaVIE-v has been developed through the Unity game engine using the SteamVR plugin and is compatible with all commercial headsets. It allows the visualization, exploration and interaction of data for scientific analysis. Originally developed to serve the HI Radio Astronomy community for HI source identification, the software has now completed the alpha testing phase and is already showing capabilities that will serve the broader astronomy community and more. iDaVIE-v has been developed at the IDIA Visualisation Lab (IVL) based at the University of Cape Town in South Africa, in collaboration with INAF-Catania. During our talk, we will present the system and its capabilities as well as showing some examples in which the software has been used to analyze real data."
"O3-136","Claudia Comito","Jülich Supercomputing Center","Analyzing Scientific Big Data with the Helmholtz Analytics Toolkit (HeAT)","The exponential increase in data size over the last years means researchers are scrambling to port their previously cluster-bound data analysis to full-blown high-performance-computing (HPC) applications. In the 2020s, astrophysicists might get used to applying for supercomputing time to calibrate, image, and analyze their data, just as naturally as they apply for telescope time.

Python is the standard programming language within the scientific community, with the [SciPy](https://www.scipy.org/) stack the clear reference for data analysis. While parallelizing SciPy code can be relatively straightforward if the algorithm itself is ""embarassingly parallel"" (as in: chunk up the data, ship them to available compute nodes, run the calculations in single-node mode on those chunks), data scientists today are still mostly on their own  when it comes to solving more complex problems, requiring ad-hoc communication among CPUs/GPUs and generally sound HPC training.

The Helmholtz Analytics Framework ([HeAT](https://github.com/helmholtz-analytics/heat)) is meant to bridge this gap. HeAT is an open-source Python tensor library for scientific parallel computing and machine learning. Under the hood, low-level operations and high-level algorithms are optimized to exploit the available resources, be it a dual-core laptop or a supercomputer. At the same time, HeAT's NumPy-like API makes it straightforward for SciPy users to implement HPC applications, or to parallelize their existing ones. HeAT relies on [PyTorch](https://pytorch.org/) for its data objects, which implies fast on-process operations and GPU support. Our recent [benchmarks](https://github.com/helmholtz-analytics/heat/tree/master/benchmarks) show that the current early-phase HeAT can achieve a speed-up of up to two orders of magnitude compared to similar Python frameworks.

In this talk, I will show you HeAT's inner workings and what to keep in mind when you `import heat as np` to parallelize your astrophysical data analysis."
"O3-137","Marcos López-Caniego","ESA - ESAC / Aurora Technology for ESA","Exploring the Universe with pyESASky JupyterLab widget","ESASky is a web-based application developed by the ESAC Science Data Centre that allows both scientists and the general public to explore a wide variety of astronomical data sets across the whole electromagnetic spectrum in a simple and intuitive way. In this talk we will present pyESASky, an extension library for JupyterLab that has been developed by the ESASky team. This library can be easily installed with pip and gives the user full control of the application from a Jupyter notebook, showing the available data from ground-based and space observatories, selecting background observations, overlaying catalogues of astronomical objects and projecting the footprints of astronomical instrumentation on the sky, for example for James Webb Space Telescope."
"O3-78","Kimberly Kowal Arcand","NASA's Chandra X-ray Observatory, Center for Astrophysics | Harvard & Smithsonian","Holding the Cosmos in Your Hand: Developing 3D Data Pipelines","Three-dimensional (3D) visualization has opened up a Universe of scientific data representations.  3D printing has the potential to make seemingly abstract and esoteric data sets accessible, particularly through the lens of translating data into forms that can be explored in the tactile modality for people who are blind and visually impaired.  This talk will outline the current state of 3D modeling in astrophysics, astronomy, and planetary science.  It will also discuss 3D printed astrophysical and planetary geophysical data sets, along with their current and potential applications with non-expert audiences. Key to this analysis is the prospective pipeline and benefits of other 3D data outputs in accessible scientific research and communications, including extended reality and data sonification.  This pipeline extends across a variety of different file types."
"O4-144","Aitor Ibarra Ibaibarriaga","XMM-Newton Science Operation Centre (ESAC/ESA)","Supporting observatory coordination with new standards: current status","Multi-messenger astronomy and the observation of transient sources promise to address during the next decade some of the most important questions in astrophysics, Gravitational Relativity,  and other subjects like, e.g. the origin of ultra-energetic cosmic rays.

However, one of the main obstacles to create coordinated observation plans between heterogeneous observatories is the lack of standards. With different wavelength coverage, space or ground based facilities with totally different technical properties, users need tools for fast discovery of metadata and scheduling in a transparent way without expert knowledge on the telescope itself. To address this issue, during the Kavli–IAU Workshop on Multi-Messenger and Transient Astronomy, a white paper was produced identifying as first recommendation on Telescope Coordination the proposal of endorsement by IAU of the IVOA standard on observations planning discovery (Observation Locator Table Access Protocol - ObsLocTAP). Also, a second standard for target visibility by a certain observatory (Object Visibility Simple Access Protocol - ObjVisSAP) is under development by IVOA. By implementing these standards, observatories will allow scientists to discover observation periods when a certain astronomical target is visible for different observatories, identify coordinated observations, discover scheduled observations and to follow-up changes in the scheduling plan. 

We will present the use cases to be solved, the technical description and engineering architecture of the ObsLocTAP and ObjVisSAP IVOA standards designed by this team, the level of implementation of the services by different observatories, client implementations and feedback received from different hands-on workshops and challenges to be solved to implement these protocols.

Finally, we will review other future possible standards that could be defined to facilitate all the rest of the steps of the coordinated observations proposals, from the scientific idea to the final observation."
"O4-151","Katarzyna Wardęga","University of Warsaw, University of Texas Rio Grande Valley","Using Artificial Neural Networks to detect astronomical transients","To search for optical counterparts to gravitational waves, it is crucial to develop an efficient follow-up method that allows for both a quick telescopic scan of the event localization region and search through the resulting image data for plausible optical transients. We present a method to detect these transients based on an artificial neural network. We describe the architecture of two networks capable of comparing images of the same part of the sky taken by different telescopes. One image corresponds to the epoch in which a potential transient could exist; the other is a reference image of an earlier epoch. We use data obtained by the Dr. Cristina V. Torres Memorial Astronomical Observatory and archival reference images from the Sloan Digital Sky Survey. We trained a convolutional neural network and a dense layer network on simulated source samples and tested the trained networks on samples created from real image data. Autonomous detection methods replace the standard process of detecting transients, which is normally achieved by source extraction of a difference image followed by human inspection of the detected candidates. Replacing the human inspection component with an entirely autonomous method would allow for a rapid and automatic follow-up of interesting targets of opportunity. The method will be further tested on telescopes participating in the Transient Optical Robotic Observatory of the South Collaboration."
"O4-79","Giuseppe Greco","INFN-Perugia","On the capability to encoding gravitational-wave sky localizations with the Multi Order Coverage data structure: present and future developments","The IVOA standard Multi-Order Coverage map (MOC), a data structure based on the HEALPix tessellation of the sky, can be used to encode the enclosed area within a given probability level contour of a gravitational-wave (credible region) sky localization. MOC encoded credible regions can be created, visualised and manipulated using Aladin Desktop, allowing one to compare them with existing surveys and query the VizieR database. These sets of tasks can also be performed via python using the astropy affiliated package mocpy, efficiently displayed in javascript applications with Aladin Lite, and integrated within Jupyter notebooks through the ipyaladin widget.

In this talk, we present an enhanced MOC structure that allows us to include temporal information about gravitational-wave events.  This data structure, the SpaceTime-MOC, provides us with an effective way to develop new multi-messenger data analysis tools that will have a crucial role when the third-generation interferometric gravitational wave observatories, such as the Einstein Telescope (ET), will begin in operation."
"O5-36","Kiri Wagstaff","Jet Propulsion Laboratory, California Institute of Technology","Machine-Assisted Discovery Through Identification and Explanation of Anomalies in Astronomical Surveys","Data volumes in modern astronomical surveys are large, and human attention is comparatively scarce.  The most interesting sources are rare and may therefore go permanently buried and unknown in large archives.  Moreover, the science results that currently planned surveys propose to deliver (e.g., precise constraints on the physical properties of dark matter and dark energy from Roman, SPHEREx, and Euclid) require exquisitely precise control of systematic errors in measurements taken over billions of individual galaxies and stars. Validating these measurements in current surveys is already a massive undertaking, and existing techniques appear unlikely to scale the next generation of large sky surveys.

We are developing algorithms to enable discovery and validation processes to scale to large survey data sets.  The key innovation is the use of machine learning to identify, group, and explain anomalies within very large data sets.  The goal is to quickly distinguish erroneous measurements and expected patterns in the data from sources and statistical correlations with true astrophysical origins.  We illustrate the process of identifying and explaining anomalies in a study conducted on sources observed by the Dark Energy Survey.  In comparing outliers identified by this process on a subset of the sources (11M) to those that were independently flagged by years of manual review, we found that 96% of the automatically identified outliers were also discarded by humans.  The remaining outliers exhibit some additional issues not previously identified by the team, as well as several unusual objects that led to follow-up spectral observations with the Palomar Observatory.  This approach holds great promise for reducing the amount of manual effort involved in validating large survey data sets and also increasing the rate at which new discoveries are made."
"O5-37","Colin Jacobs","Center for Astrophysics and Supercomputing, Swinburne University of Technology","Probing neural networks for science: What is it they are learning?","Neural Networks are finding increasing use in many areas of astronomy, but often act as ""black boxes"". Many techniques exist to probe in the internals of neural networks but not all are relevant to scientists. In this talk I discuss some of the techniques developed in computer vision to investigate what neural networks are learning, and investigate some of their benefits and problems when applied to astronomy. I introduce a simple technique and software package, 'Sensie', to probe what neural networks have learned. I apply it to networks trained to find strong gravitational lenses in the Dark Energy Survey and draw some lessons that may help future searches."
"O5-77","Sergi Blanco-Cuaresma","Harvard-Smithsonian Center for Astrophysics","Exploring the use of Graph and Machine / Deep Learning technologies with the NASA ADS content","The NASA Astrophysics Data System (ADS) manages more than 14 million scientific abstracts with more than 5 million full text, more than 128 million citations and thousands of other relationships (e.g., articles’ keywords and data sources). NASA ADS users regularly explore the data using our website and API, which already relies on modern search technology such as Apache Solr. One of the next steps is to provide an even better service by, for instance, automatically enriching our dataset (e.g., article clustering/classification) or improving the search results (e.g., PageRank computation). To accomplish these goals, we are exploring state-of-the-art Graph and Machine/Deep Learning technologies such as Neo4J (Graph Database), BERT (Google’s Language Model for Natural Language Processing tasks) and Graph Neural Networks. We present our preliminary findings to shed some light on the challenges and opportunities that these technologies can offer."
"O6-104","Clara Brasseur","Space Telescope Science Institute","Astronify: listening to the stars","Sonification is the process by which data is represented by sound. There are nearly infinite ways to sonify data, and sonification has been applied in fields such as solar physics, stellar astrophysics, supernovae, and gravitational waves to the interest and delight of hundreds of thousands of listeners. We present Astronify, a new Python package for sonifying data series, particularly light curves. Astronify focuses on being scientifically useful, particularly for blind and visually impaired astronomers. This talk will describe the design and usage of this software, including both the underlying algorithms we use to turn data into sound and the user experience. This package is open source and welcomes community contributions. We will also discuss future plans and how interested parties can get involved."
"O6-30","Kun Li","Tianjin University","Storage Schema of Time Series Astronomical Data for Artificial Intelligence Analysis","Time series data is commonly used in time domain astronomy. Traditional manual processing methods are becoming extremely hard and infeasible during the dramatic growth of data volume. Artificial intelligence (AI) techniques provide the possibility for exploring the entire dataset but require that the time series data of all celestial objects in the dataset must be prepared in advance. We have designed a special tool called AstroCatR for reconstructing astronomical time series data from large-scale catalogues. In this paper, we focus on challenges in time series metadata structure, storage schemes, strategies and formats of exposition. A recommendation unified time series data (light curves) release standard needs to be proposed for analysis of time series data based on AI techniques."
"O7-153","Sandor Kruk","European Space Agency","Hubble Asteroid Hunter: exploring the ESA Hubble archives with citizen science","The Hubble Space Telescope (HST) archives can hide many unexpected treasures, such as trails of asteroids, showing a characteristic curvature due to the parallax induced by the orbital motion of the spacecraft during the exposures. 

We present a new citizen science project exploring the ESA HST (eHST) archive for serendipitously observed asteroids. Hubble Asteroid Hunter (www.asteroidhunter.org) was set up as a collaboration between scientists and engineers at the ESAC Science Data Centre (ESDC) and Zooniverse and launched on the International Asteroid Day in June 2019. Since then, more than 10,000 volunteers provided 2 million classifications of 150,000 HST images and uncovered 1500 asteroid trails in them, many of the asteroids yet to be identified. Finding the asteroids in HST images allows us to refine the ephemerides of their orbits, as well as to study their orbital distribution. In addition to marking the positions of asteroids, volunteers also tagged satellites in orbits higher than Hubble’s and discovered new strong gravitational lenses and collisional ring galaxies. We argue that a combination of citizen science and artificial intelligence methods is an efficient way of exploring archival data and highlight some of the interesting results found by this project with the invaluable help of the Zooniverse volunteers.

An example of an asteroid passing in front of the Crab Nebula, imaged by HST - http://www.esa.int/ESA_Multimedia/Images/2019/10/Foreground_asteroid_passing_the_Crab_Nebula"
"O7-155","Richard L. White","Space Telescope Science Institute","The Hubble Image Similarity Project","Archives of astronomical images allow users to find images by metadata: what camera, what filter, what PI, what declination. Catalogs of the objects contained in those images provide a limited search of the data itself: position, magnitude, position angle, Sersic index. In a previous ADASS, we presented how we are harnessing neural networks to answer a much harder question: If I have a complex image, how can I find all the images in the archive that look like it? We found that by using convolutional neural networks trained on terrestrial images and dimensionality reduction techniques, we could find edge-on galaxies with dust lanes, star fields, and rich scenes of star formation.

A major hindrance to our work is the difficulty of comparing algorithms to determine which one is best. At its core the question is whether the groups of images identified by one algorithm are more similar than the groups from another algorithm.  Our new Hubble Image Similarity Project aims to create a large database of similarity information between segments of Hubble images.  The images are compared by humans in a citizen science project, where they are asked to select similar images from a comparison sample.  We also designed the project for community impact: our citizen scientists are service-industry professionals from the local area near STScI in Baltimore who were impacted by the Covid-19 pandemic.  They are paid a fair wage for their work through the Amazon Mechanical Turk system.

The comparison measurements are analyzed to compute a distance matrix between all the pairs of images, and that distance matrix can subsequently be utilized to assess the accuracy of algorithms based on computer vision methods. The image similarity matrix shows that the collective visual wisdom of our neighbors matches the accuracy of the trained eye, with even subtle differences among images faithfully reflected in the distances. We will publish the data and the images for use as a test set."
"O7-45","Matteo Bachetti","INAF-Osservatorio Astronomico di Cagliari","CICLOPS: CItizen Computing Pulsar Searches","Most periodicity search algorithms used in pulsar astronomy today are highly efficient and take advantage of multiple CPUs or GPUs. 
The bottlenecks are usually represented by the operations that are not easy to automate, or that require an informed choice from an expert eye.
A typical case is the presence of radio-frequency interferences in the data, that often mimic the periodic signals of pulsars.
Another frequent issue is the need for visual inspection of hundreds or thousands of pulsar ``candidates'' that arise out of a number of preselected thresholds that are based on experience but often return a large number of false positives (or negatives).

CICLOPS is a citizen science project designed to transform the search for pulsars into an entertaining 3D video game. 
We take advantage of the best gaming technologies on the market to build a distributed computing platform, running calculations with the user's CPUs and GPUs and using the unique human abilities in pattern recognition to find the best candidate pulsations."
"O7-52","Sergio Pascual","Universidad Complutense de Madrid","Citizen science with the TESS photometer network","We present here the TESS network of night-sky brigthness photometers.
This network is an initiative of the STARS4ALL project, a platform
for promoting dark skies in Europe, funded by the EU.
The project involves citizens in monitoring light pollution of places
worldwide.
The hardware and software of the photometers are open. The data obtained
with the photometers is also open, available under a
Creative Commons license (cc-by 4.0).
We also describe the infrastructure required to gather measurements from the
photometers and to allow the citizens to inspect the data from their devices."
"O8-111","Sergio Pintaldi","University of Sydney, Sydney Informatics Hub","A scalable transient detection pipeline for the Australian SKA Pathfinder VAST survey","The ASKAP Survey for Variables and Slow Transients (VAST) is the study of astrophysical transient and variable phenomena at radio wavelengths, such as flare stars and supernovae, using the new Australian Square Kilometre Array Pathfinder (ASKAP) telescope. The large field of view of ASKAP means that large areas of the radio sky can be surveyed regularly at sub-millijansky sensitivities. This has not been possible with previous radio telescopes, and means that ASKAP is now providing an unprecedented view of the dynamic radio sky. For example, the first shallow all-sky survey completed by ASKAP provided 2.5 million source measurements. This creates a data challenge for VAST, where regular epochs of the sky will result in the need to construct lightcurves for millions of astrophysical sources, while also being able to swiftly identify the small percentage of sources that exhibit transient behaviour, in addition to providing a visualisation solution for such a large and rich dataset.

We have developed a modern and scalable code base written in Python that builds upon previous software efforts in the community that, due to their old technology stacks, were not scalable to ASKAP datasets. The modern technology stack allows us to perform fast mass source association using Pandas dataframes and well-known Astropy crossmatch functions. The transient detection code and the web interface have been unified under one code base, in which the user can run a pipeline job from both command line and web interface itself.

A database has been used to store the data model, the relationship between entities and to serve data to the web interface. We used Dask, a tool for scalable analytics in Python, as the tool for ensuring both vertical as well as horizontal scalability of the pipeline. The technology stack is: Python 3.6+, Postgres 10+, Astropy 4+, Django 3+, Dask 2+ and Bootstrap 4. The adoption of such a modern technology stack will ensure a long life expectancy of this pipeline.

In this talk we will give an overview of the pipeline architecture and implementation, discuss some initial results, and outline future challenges."
"O8-131","Simon Perkins","South Africa Radio Astronomy Observatory","Distributed Streaming Radio Astronomy Reduction with Dask","From 2010, the biyearly doubling in processor transistor counts predicted by Moore’s Law has slowed, along with the associated increases in single processor speeds. In a bid to offer an edge over competitors, processor manufacturers began offering multiple processors on a single die, leading to the current multi-core processor era. Today, multi-core CPUs are ubiquitous, while the massive performance offered by GPUs is predicated on multi-core programming models and architecture.

While greatly contributing to the ability to process large data volumes, simply adding more cores has proven insufficient to process the sheer quantity of contemporary data, often referred to as “Big Data”. Horizontal scaling, or the use of multiple compute nodes within either HPC or Cloud Computing environments, is routinely used along with strategies such as MapReduce, and cluster computing frameworks such as Spark.

Additionally, the software required to operate in such environments has necessarily changed: While the 1990s and early 2000s advocated Object-Orientated Programming, Big Data leans towards a streaming, chunked, functional programming style with minimal shared state. Consequently, individual tasks handling chunks of data can be flexibly scheduled on multiple cores and nodes. Legacy radio astronomy codes do not readily adapt to this paradigm. To process the quantities of data produced by contemporary radio telescopes such as MeerKAT, and future telescopes such as the SKA using the aforementioned paradigms, radio astronomy codes will need to adapt appropriately.

In this talk we will cover the principles and practices of developing HPC code with Dask, a lightweight Python parallelisation and distribution framework that seamlessly integrates with the PyData ecosystem to address the above challenges. We have found that the intersection of these technologies provides a rich ecosystem for contemporary Radio Astronomy software development. It has already led to a number of diverse packages (covered elsewhere at this conference) at various stages of development and/or release. These include tricolour, xova, shadeMS and QuartiCal."
"O8-63","Martin Kuemmel","LMU Faculty of Physics","Tiling the Euclid Sky","The Euclid satellite is an ESA mission scheduled for launch in 2022. It will observe an area of 15,000 deg^2 with two instruments, the Visible Imaging Channel (VIS) and the Near IR Spectrometer and imaging Photometer (NISP). Ground based imaging data in griz from surveys such as the Dark Energy Survey and LSST complement the Euclid data to enable photo-z determination. The mission investigates the distance-redshift relationship and the evolution of cosmic structures by measuring shapes and redshifts of galaxies and clusters of galaxies out to redshifts ~2.

Data processing is done in chunks that cover a specific area on the sky which are called tiles. When dividing a survey sky into tiles certain requirements need to be fulfilled:
* the tiling needs to cover the entire survey area;
* the tile size needs to be manageable by the infrastructure (data bases, storage and computing facilities) and should not create bottlenecks;
* the tiling needs to partition the resulting object catalogs such that each object uniquely exists in only one catalog;

We present the tiling that was developed to cover the Euclid Surveys. Each tile consists of an extended area to compute the object properties and a core area to uniquely associate each object to one tile. The core area is defined in healpix indices and stored as Multi-Order Coverage (MOC) maps. Special tiling is applied for the Euclid Deep Surveys, to have data transfer and computing time within reasonable limit. We also create larger tiles around bright, large galaxies to allow also measuring their properties. We discuss the Euclid tiling in the context of the processing that had been applied in other surveys such as SDSS and 2MASS, among others."
"O9-120","Pilar de Teodoro","ESAC/ESAC Science Data Centre","Data Interoperability at the core of The Euclid Scientific Archive System","Euclid is an ESA mission to explore the dark Universe. Euclid will map the 3D distribution of up to two billion galaxies and dark matter associated with them. It will hence measure the large-scale structures of the Universe across 10 billion light years, revealing the history of its expansion and the growth of structures during the last three-quarters of its history. In total Euclid will produce up to 26 PB per year of observations. The Euclid Archive System is a joint development between ESA and the Euclid Consortium and is led by the Science Data Centres of the Netherlands and the ESDC (ESAC Science Data Centre). The EAS is composed by three different subsystems: Data Processing System (DPS), Distributed Storage System (DSS) and Science Archive System (SAS). The SAS is being built at the ESDC and is intended to provide access to the most valuable scientific data, which is currently estimated in 10 PB of images, catalogues and spectra, after 6 years mission.

Nowadays, the Astronomy science is inherently interoperable from science data and models perspective but also from the tools and interfaces to exploit them. In this respect, the International Virtual Observatory Alliance, is the organization whose main mission pursues for the data interoperability in Astronomy in which the Euclid Archive has been part of from an early stage. The Euclid Archive was designed to provide interoperability capabilities to enable scientific discoveries on science: images, spectra and a major catalogue of 10 Billion sources. In this paper we will describe how Euclid, supported by its successful precursor Gaia, will make use of the most remarkable IVOA standards (TAP, ObsCore, ConeSearch, SIA, SSA, SODA, VOSpace, DataLink) but also how VO standars are part of the Scientific Platforms ecosystem enabling scientific discoveries to the community."
"O9-27","Perry Greenfield","STScI","The Advanced Scientific Data Format (ASDF): Why you should use it","STScI has developed a new data format for use with JWST and the Nancy Grace Roman Telescope that addresses some of the constraints and issues that the FITS format has engendered. This talk will briefly outline the motivations for developing this format and describe the structure and features of the format, highlighting the advantages over FITS, particularly with regard to data organization and WCS capabilities."
"O9-94","Mateusz Malenta","The University of Manchester","Using Docker in a radio-astronomy environment","Docker is widely used by the astronomy community and is arguably one of the most common choices for containerisation despite its primary target audience being Web, IoT and cloud industries and their users. Already an important part of the research ecosystem, if used properly, containers can shorten development and deployment times and ensure a consistent processing software environment across multiple machines. There is however a significant difference between deploying a web application using Node.js and running a real-time pipeline, as required to process tens of gigabytes of raw data per second. If Docker is not used properly, this can lead to misused and underutilised resources, which are often limited compared to those at the disposal of ‘the industry’. In the worst-case scenario a badly-implemented pipeline can corrupt the data and return incorrect results. 

In this talk I will present lessons learned from developing and maintaining a Docker-based pipeline used as part of the MeerTRAP single-pulse search efforts.  This pipeline is used to run a time-domain search for transients, such as pulsars and Fast Radio Bursts, in the radio part of the electromagnetic spectrum. Distributed across 65 compute nodes, it consists of multiple stages and overlapping CPU and GPU processing. It makes heavy use of C++ for the initial processing, with the post-processing including candidate classification using machine learning written in Python. I will present the best techniques that we have developed during almost 2 years of developing and maintaining the Docker images for the MeerTRAP project. These include ways to write clean and concise Dockerfiles that result in fast and lean images used for production deployments, reducing the overall size of the Docker ecosystem and maintaining a reliable private repository."
"P10-100","Brent Shapiro-Albert","West Virginia University","The Pulsar Signal Simulator: A Python-based Package for Simulating Pulsar Data for Science and Education","Pulsar astrophysics, from searching for new systems to using precision pulsar timing to detect gravitational waves, is both complex and computationally expensive. Here we present the Pulsar Signal Simulator (PSS), an open-source Python package that can be used to simulate pulsar observations. The PSS has been used to explore the complex covariances between interstellar medium and intrinsic pulsar emission effects, and can output simulated data in standard data formats. The PSS has a modular, easy-to-use interface, making it ideal not only for scientific simulations, but also for student education. The simulated data can also be used to test various data analysis pipelines and pulsar searching algorithms so that computing time can be optimally spent. Given the modular design of the PSS, other pulsar astronomers will be able to easily use and build upon the current existing framework, further improving the software."
"P10-101","Atreyee Sinha","LUPM, CNRS, Université de Montpellier","Gammapy: An open-source Python package for gamma-ray astronomy","Gammapy, a prototype for the Cherenkov Telescope Array (CTA) Science Tools, is a community-developed, open source Python package built on Numpy, Scipy and Astropy using open FITS based data formats. It provides a framework for gamma-ray data reduction and subsequent modeling and fitting to build high level science products like flux maps, spectra and light curves. In addition to methods used in traditional Cherenkov astronomy, it allows the user to do a simultaneous 3D likelihood fitting for spectra and morphology. Moreover, it enables joint likelihood analysis between different datasets, thus providing a simple platform for combined multi-instrument analysis. It has been successfully used to analyse data simultaneously from the four operational IACTs (H.E.S.S., MAGIC, VERITAS and FACT) and the Fermi-LAT, and has been shown to well reproduce results obtained with classical proprietary tools on the first H.E.S.S. DL3 data release. It was also used in the last CTA data challenge to characterise the instrument performance.  

In this contribution, we present an overview of the package, the current status and the development goals, and show a few analysis examples using some open H.E.S.S. and simulated CTA data."
"P10-123","Andrea Bulgarelli","INAF/OAS Bologna","Agilepy: A Python framework for scientific analysis of AGILE data","The Italian AGILE space mission, with its Gamma-Ray Imaging Detector (GRID) instrument sensitive in the 30 MeV–50 GeV energy band, has been operating since 2007. Agilepy is an open-source Python package to analyse AGILE/GRID data. The package is built on top of the command-line version of the AGILE Science Tools, developed by the AGILE Team, publicly available, and also provided by ASI/SSDC as a scientific software package. The main purpose of the package is to provide an easy to use high-level interface to analyse AGILE data by simplifying the configuration of the tasks and ensuring straightforward access to the data.  The current features are the generation and display of sky maps and light curves, the access to gamma-ray sources catalogues, the analysis to perform spectral model and position fitting, the background evaluation, and generation of test statistic maps. New features are planned for future releases. In addition, Agilepy provides an engineering interface analysis of instrument off-axis evolution. The tool is also used by the Flare Advocate team to analyse the data during the daily monitoring of the gamma-ray sky. Agilepy (and its dependencies) can be easily installed using Anaconda."
"P10-135","José Sánchez-Gallego","University of Washington","CLU: a new framework for message passing for operations software","Operations software often consists of multiple pieces of code, each one in charge of controlling  specific hardware component, that need to communicate among each other. In this talk I will present CLU (https://github.com/sdss/clu), a new library that streamlines the creation of clients and servers that follow the producer-consumer pattern. CLU builds on the legacy of the SDSS operations framework and updates it to use widely used, well maintained technologies (asyncio, RabbitMQ, JSON). I will discuss the lessons learned from 20+ years of SDSS operations software and how we are trying to improve upon it."
"P10-150","JING LUO","The Canadian Institute for Theoretical Astrophysics, University of Toronto","PINT: A Modern Software Package for Pulsar Timing","Pulsars are rapidly rotating neutron stars. Some pulsars spin stably enough that their periods can be measured to the precision of 10ns, which is able to reveal many subtle astrophysical phenomena. Such high precision demands both careful data handling and sophisticated timing models to avoid systematic error.  To achieve these goals, we present PINT(PINT Is Not Tempo3), a high-precision Python pulsar timing data analysis package, which can be accessed from https://github.com/nanograv/PINT. PINT is well-tested, validated, object-oriented, and modular, enabling interactive data analysis and providing an extensible and flexible development platform for timing applications.  It utilizes well-debugged public Python packages  (e.g.,  the NumPy and Astropy libraries) and modern software development schemes, e.g., version control and efficient development with git and GitHub and a test suite for improved stability. PINT is developed and implemented independently of traditional pulsar timing software (e.g. Tempo/Tempo2) and therefore provides a tool for cross-checking timing analyses and simulating pulse arrival times.  In this presentation, we describe the design, usage, and example of using PINT."
"P10-161","Elena Puga","European Space Astronomy Centre (ESA)","Herschel PACS Integral Field Unit data visualisation and analysis: an ESAC Science Data Centre notebook for Datalabs","ESAC Science Data Centre (ESDC) hosts a dozen of archives containing science data from over 20 space science missions and one of its primary goals is to maximize the scientific exploitation of ESA space science data sets. ESDC is set to provide Jupyter notebooks with scientific workflows to guide beginner users in the usage of the different python interfaces to the different archives (e.g. via astroquery, pyesasky...) and data analysis of discovery datasets. These notebooks are envisaged to live in the ESA Datalabs platform, a project that will allow scientist to write and execute code and processing pipelines on ESA infrastructure, with direct access to the data.
The first of these ESDC notebooks showcases the use of Herschel PACS observations, as this instrument was the very first Integral Field Unit in space between 2009 and 2013, with new python manipulation (specutils, spectral-cube) and visualization tools (SpecViz, CubeViz) that have been developed in the context of future space observatories (JWST, WFIRST). This is an example of legacy mission spectroscopic data meeting bleeding-edge visualisation and analysis tools, and a training arena for new tool's early adoption using existing calibrated and consolidated ESA data products."
"P10-179","Marina Vela Nunez","INAF - OSSERVATORIO ASTRONOMICO DI TRIESTE - OATS","Controlling and Monitoring Logging of the New Archiving Distributed Infrastructure (NADIR)","The Italian Center for Astronomical Archive (IA2) manage the NADIR system, which hold a large amount of astronomical data from different geographically distributed telescopes. Therefore, controlling the various activities across the different hosts could be complicated. In the event of an unexpected problem with specific data happen, NADIR is not configured to monitoring or control logs of stored data to find eventual users modifications, being harder to find root problems. For that reason, here we propose a NADIR integrated local logging system, that may collect and aggregate different types of logs in one central location and have a deep insight about the data flow behavior into the archives. The present system for monitoring and controlling logging help us to prevent downtime on our sites and servers, meanwhile it contribute in the preservation of scientific astronomical data."
"P10-18","Carter Rhea","L'Université de Montréal","A Machine Learning Approach To Sitelle Spectral Analysis","Machine learning is rapidly becoming another tool in an astronomer’s statistical toolbox. Recent papers have demonstrated machine learning’s ability to recover degraded data, estimate important photometric parameters, and decode spectra. In this talk, I will be focusing on the use of convolutional neural networks in decoding optical emission-line spectra. The imaging Fourier Transform Spectrometer, SITELLE, at the Canada-France-Hawai’i telescope creates exquisite data cubes with spectral resolution reaching R~10000 and over 4 million spatial pixels. While standard fitting techniques to parse out the kinematic parameters, such as the velocity and broadening of the emission lines, exist, the number of spectra in each cube makes these techniques computationally expensive. Using a convolutional neural network, we have demonstrated a recoverability rate comparable to traditional methods; more importantly, the computation time has been reduced from ~11 days to ~4 hours. Subsequent papers focus on extracting additional information, such as critical line flux ratios, from the optical spectra."
"P10-192","G. Bruce Berriman","Caltech/IPAC-NExScI","The International Virtual Observatory Alliance (IVOA)  in 2020","The International Virtual Observatory Alliance (IVOA) develops the technical standards needed for seamless discovery and access to astronomy data worldwide, according to the FAIR principles, with the goal of realizing the Virtual Observatory (VO). There are 22 member organizations. The newest member, Netherlands VO, joined in 2020. More astronomical communities from other nations have shown their interest in joining into the IVOA. This poster describes the activities of the IVOA in 2020, summarizes the May 2020 ""interoperability meeting"" and previews the November 2020 interoperability meeting that follows ADASS. The May meeting was the first held on-line, and was the first to have over 200 registrants. In particular, the poster will discuss IVOA engagement in global initiatives, including support for the IAU's Office of Astronomy for Development in remote teaching and learning program, and a CODATA initiative on standardizing global units of measure.

This poster is presented on behalf of the IVOA community."
"P10-207","Athanaseus Javas Ramaila","South African Radio Astronomy Observatory","The KERN Suite: Taming the Radio Astronomy Software Ecosystem","The building and installation of scientific software for use in radio astronomy can be a daunting task. This becomes inadequate because these software tools do not get substantial utilisation by most of the scientific community.  KERN is a released set of radio astronomical software packages available for long term supported (LTS) Ubuntu operating systems. It contains most of the standard tools that a radio astronomer needs to work with radio telescope data. The goal of KERN is to save time and prevent frustration in setting up scientific pipelines and to assist in achieving scientific reproducibility. Our packaging procedure makes extensive use of git and the packages are published to Launchpad. The current release of KERN is KERN-6 and is based on Ubuntu 18.04 (Bionic). The upcoming release of KERN will be based on Ubuntu 20.04 (focal).  A status update will be provided on the latest and upcoming releases of KERN."
"P10-209","Lexy Andati","Rhodes University","RaGaVi: A Radio Astronomy Gains and Visibilities Inspector","In radio interferometry, the goal of calibration is to mitigate instrumentation and atmospheric effects on observed visibilities, in an attempt to recover the “true” sky. The first step towards calibration is to generate solutions calculated from the observation of sources known as calibrators. These solutions are then applied to the calibrators’ visibilities to gauge their quality. However, inspecting the generated calibration solutions and the effectiveness of their application to the calibrator’s visibilites is paramount to determining the quality of the calibration process. Inspection can be done visually by plotting the solutions or the calibrated visibilities, before applying the solutions to the target source’s visibility data and imaging, to save time and computing resources. The existence of interactive tools for easier identification of faulty data points is advantageous. While there are tools available that offer this functionality, they do not necessarily provide interactive capabilities independent of Graphical User Interfaces (GUIs). Additionally, they may limit the maximum amount of data points that can be plotted due to RAM concerns or protract plot generation for plots involving large amounts of data, as is synonymous with radio-interferometric data presently.

RaGaVi is a python based tool that generates self-contained interactive plots of calibration solutions and visibilities. It has two aspects which perform these tasks, namely ragavi-gains and ragavi-vis, respectively. Ragavi-gains provides interactivity by displaying useful identification information for each data point in calibration solutions (such as scan number, antenna e.t.c.) in addition to other interactive actions. Interactivity is helpful to determine the origin of outliers or problematic solutions quickly. On the other hand, ragavi-vis not only provides some level of interactivity but also presents the capability of processing large volumes of data relatively fast, and with a reduced strain on RAM."
"P10-212","Alice Allen","Astrophysics Source Code Library/University of Maryland College Park","Making organizational software easier to find in ASCL and ADS","Software is the most used instrument in astronomy, and organizations such as NASA and the Heidelberg Institute for Theoretical Physics (HITS) fund, develop, and release research software. NASA, for example, has created sites such as code.nasa.gov and software.nasa.gov to share its software with the world, but how easy is it to see what NASA has? Until recently, searching NASA’s Astrophysics Data System (ADS) for NASA’s astronomy software has not been fruitful. Through its ADAP program, NASA has funded the Astrophysics Source Code Library (ASCL ascl.net) to improve the discoverability of these codes. Adding institutional tags to ASCL entries makes it easy to find this software not only in the ASCL but also in ADS and other services that index the ASCL. This poster presentation covers the changes the ASCL has made as a result of this funding and how you can use the results of this work to better find organizational software in ASCL and ADS."
"P10-229","Mattia Mancini","ASTRON","Community engagement in pipeline development using CWL, a case study for LOFAR","In modern experimental sciences, the increasing amount of data to process and the development of standardized procedures to analyze it, is driving the utilization of automatic processing pipelines. This is in particular true for radio interferometers where in order to create science ready data, a number of steps for data filtering, instrument calibration, and data transformation have to be performed on large datasets. 
The capability to abstract the process, and increase the descriptive definition, of the pipelines seems to be key for maintaining future pipelines. This goal can be achieved using a standard pipeline definition language such as provided by the Common Workflow Language (CWL). As part of the effort towards realizing the European Open Science Cloud (EOSC) we have converted one of our standard LOFAR calibration pipelines to CWL. A fruitful and successful collaboration with the community maintainer of the pipeline has resulted in a demonstration of how CWL can improve supporting sustainable pipeline development and its community uptake. In this poster, we present the lessons learned and the benefits we found in the conversion process."
"P10-241","Marco Lam","Liverpool John Moores University / Tel Aviv University","An update on the development of ASPIRED","We are reporting the updates on the development of the Automated SpectroPhotometric REDuction (ASPIRED) pipeline, designed for common use on different instruments. The default settings support common long-slit spectrometer configurations, whilst it also offers a flexible set of functions for users to refine and tailor-make their automated pipelines to an instrument’s individual characteristics. Such automation provides near real-time data reduction to allow adaptive observing strategies, which is particularly important in the Time Domain Astronomy. Over the course of last year, significant improvement was made in the internal data handling as well as data I/O, accuracy and repeatability in the wavelength calibration. It has been tested on several active instruments: SPRAT on the Liverpool Telescope, ISIS on the William Herschel Telescope, GMOS long slit mode on the Gemini North. We are also expecting to deploy a new common pipeline for SPRAT (on LT) and MOOKODI (on the Lesedi Telescope) towards the end of the year."
"P10-244","Benjamin Hugo","South African Radio Astronomy Observatory","Tricolour: an optimized parallel sumthreshold flagger for MeerKAT","We present Tricolour, a package for Radio Frequency Interference excision of wideband finely channelized MeerKAT correlation data. The MeerKAT passband is heavily affected by interference from satellite, mobile, aircraft and terrestrial transponders. Coupled with typical data rates in excess of 100 GiB/hr at 208kHz channelization resolution, excision poses a significant processing challenge. Our flagger is highly configurable, parallel and optimized, employing Dask and Numba technologies to implement the widely used SumThreshold and MAD interference detection algorithms. We find that typical 208kHz channelized datasets can be processed at a rates in excess of 130 GiB/hr for a typical L-band excision strategy on modern dual-socket Intel Xeon servers."
"P10-33","Patricio Cubillos","Space Research Institute, Graz","Bibmanager: Automated BibTeX Management for Astronomers","I will present bibmanager, a command-line application that automates
the handling of BibTeX entries for LaTeX projects and synchronization
with ADS.  Bibmanager maintains a centralized BibTeX database that
provides automated bibfile compilation for LaTeX projects.  A simple
interface facilitates building the database by merging bibfiles,
fetching entries from ADS, or editing manually the database (all with
automated duplicate detection).  Bibmanager can also double as a PDF
database of the entries.  The interaction with the ADS API enables
automated synchronization of the database with ADS, or perform queries
to add entries or fetch their PDFs.  Bibmanager is available at PyPI
(pip install bibmanager) and fully documented at
https://bibmanager.rtfd.io"
"P10-44","Tim Molteno","Department of Physics, University of Otago","DiSkO: Discrete-sky Bayesian Synthesis Imaging","This paper describes DiSkO, a wide-field synthesis imaging package based on a telescope operator on a discrete representation of a non-flat sky. The SVD of this operator provides an explicit representation of the null-space of the telescope operator, and leads to a natural basis in which the telescope operator can be inverted with quantified uncertainty.
   DiSkO generates images of the radio sky from visibilities without gridding or the use of the Fourier Transforms, does not require a co-planar antenna array, and requires no discretization in the U-V-W plane.
   The algorithm is available open-source, and has proved effective for synthesis imaging on data from an all-sky 24 antenna synthesis array telescope. Results on some standard radio sources using VLA data are also presented.
   The memory requirements scale with the product of the number of sky elements and the number of visibility measurements. While this algorithm requires more computational resources than simple 2D inverse FFT-based methods, it has advantages as it acknowledges the non-flat sky, and can provide a framework for Bayesian imaging of the full sphere, as well as sequential inference from multiple snapshot observations."
"P10-70","cosimoNigro","Institut de Fisica d'Altes Energies (IFAE)","gLike: likelihood maximization and profiling for astrophysical (and beyond) applications","Based on ROOT, gLike is a framework for the numerical maximization of likelihood functions.
The likelihood has one free parameter (named g) and as many nuisance parameters as wanted, which will be profiled in the maximization process. 
Multi-instrument likelihood can be created and jointly profiled by compounding together several likelihood terms.   
gLike has and is being used for Dark Matter searches in gamma-ray astronomy, its flexibility allows for the extension into the multi-messenger domain."
"P10-71","Marina Vela Nunez","INAF - OSSERVATORIO ASTRONOMICO DI TRIESTE - OATS","""Intuitive GUI to restore Corrupted Data: A Support for Preservation and Curation of the Astronomical Data""","The big data management challenge is one of the forefront topics in the fields of science and technology. At the heart of big data management lies the data preservation and curation process. This process consists of recovering lost or harm data, and transforming it into data curated by following specified standards and make it available for the end users.
The Italian Center for Astronomical Archive (IA2) developed the New Archiving Distributed Infrastructure (NADIR), which holds a large amount of astronomical data from different geographically distributed telescopes. When data is received, sometimes it arrives with some imperfections. The harm or lost of data could have some catastrophic impact on science. For such reason, our aim is to enhance the actual IA2 data management software platform by complementing the current NADIR system with an extra software, designed specifically for efficient, recovering of the corrupted, un-formatted or mismatched data files, in order to preserve astronomical data and to facilitate astronomers research work. The use of a GUI allowed more accurate and faster recovering of significant scientific observations that otherwise would be lost during archival ingestion. And most importantly, these observations would not usable by the reduction processes."
"P10-75","Simón Torres","SOAR Telescope / NSF's NOIRLab","Spectroscopic Data Reduction on your web browser","The goodman-pipeline package provides all the functionality needed to reduce raw data obtained with the Goodman High Throughput Spectrograph on the Southern Astrophysical Research (SOAR) 4.1m telescope, and produce 1-D wavelength calibrated spectra. However, many science use cases, especially those targeting transients and variable objects, depend critically on having fully reduced spectra available shortly after the raw data are written to disk; the goodman-pipeline was not designed to fulfill this requirement. To address this issue we have developed a system that combines the data reduction elements of the goodman-pipeline with a Web API implementation within a web-based structure, to provide the user with immediate spectroscopic processing through a web browser.
This approach provides users with fully reduced spectra just seconds after the shutter closes, which could be critical when making real time decisions such as whether to obtain more data of the same object, switch to a different configuration or instrument, submit an observation request to another facility, or simply discard the object as one of no interest for the specific science case, and move to a new target."
"P10-97","Sphesihle Makhathini","University of the Witwatersrand","Containerised scripting for radio interferometry","The process of creating science data products in astronomy can be cumbersome and unreliable for a number of reasons. The most important among these are;  i) the data reduction pipelines used are so complex that only a few individuals can understand, hence fully utilize them; ii) these pipelines are often poorly documented, making them unnecessarily opaque and difficult to debug; and iii) the software applications required to run the pipelines only work in suitably customized computing environments; operating system, library versions, and system settings. I will present Stimela, a scripting tool for radio interferometry (RI) data processing and synthesis that leverages container technology to circumvent the aforementioned challenges. The result is a simple,  system-agnostic, configurable, and portable framework that requires little effort to install while giving the user access to legacy as well as novel RI tools via a unified python interface."
"P11-146","Fabio Rossi","INAF","Numerical code generation from symbolic expressions in Python","An approach to numerical software development is presented, with the objective to leverage
 automatic symbolic expressions translation to universal functions of different numerical 
backends. SymPy is used to define the symbolic expressions while NumPy and CuPy are the currently
supported numerical backends. This allows the non-expert in GPU programming to easily exploit
GPU computational power for highly demanding numerical tasks, while having the chance to 
have the same formulas evaluated also on the CPU.
Our approach is demonstrated in the context of Optics related computations such
as Optical Propagation methods and Zernike modes covariance computation."
"P11-147","Maria Henar Sarmiento Carrión","ESAC Science Data Center (ESDC)","The technical challenges of implementing DOIs at the ESA Science Data Center (ESDC)","Scientific results from both observations and simulations require verification to be accepted by the scientific community. Therefore, scientists more and more enrich their papers by the data sets on which the results are based. As a matter of fact, American Geophysical Union journals (e.g. JGR) reject papers if the data are not made available to the referee.
 
The datasets are made available online via links, which only provide useful information if they adhere to the FAIR principle: Findable, Accessible, Interoperable and Reusable. Apart from that, the links need to be permanent which is often hard to achieve. A very common way for making these datasets available, is through Digital Object Identifiers (DOIs), which have been widely adopted by publishers and researchers. DOIs also are useful for data use tracking, which helps researchers secure grants and project extensions. 
 
A DOI actually is more than just a link between a publication and a dataset. They also contain a landing page making available metadata for the dataset. This metadata both describes the dataset and facilitates discovery via the Google Dataset Search engine. The Google Dataset Search engine was launched in late January 2020 and already indexes over 25 million datasets. This fast growing service may become the de-facto standard for searching for datasets and new datasets can be made discoverable by adding a JSON script in the DOI landing pages.
 
This presentation focuses on the technical aspects of the implementation of DOIs at the ESDC, the ESAC Science Data Center. The ESDC is in the process of generating DOIs for all space missions and a technical infrastructure is being created to ease the generation of DOIs both for future missions and publications. Given the great number of datasets involved, full automation of DOI generation, including landing pages, metadata and JSON scripts, is on its way at ESDC.     
 
Authors: M. H. Sarmiento, G. de Marchi, B. Merín, D. Baines, S. Besse, B. Martinez, A. Masson and D. Wenzel."
"P11-172","David Berry","East Asian Observatory, Hilo, HI.","STARLINK - the original and best","The  UK Starlink Software Collection (SSC), first initiated in 1980, is now maintained by the East Asian Observatory. This poster summarised recent developments in the SSC."
"P11-180","Dirk Muders","Max-Planck-Institute for Radioastronomy","APECS - The APEX Control System","APEX (Atacama Pathfinder EXperiment) is a 12m single dish submillimeter telescope at 5100m altitude near the ALMA site on the Chajnantor plateau in Chile. The APEX Control System (APECS) provides telescope control, instrument configuration, raw data acquisition and data calibration which is handling observing sequences (e.g. on/off) and instrumental or atmospheric corrections. It also implements automatic observation and device parameter logging. The user interface is based on Python and allows for efficient scripted observations.
Due to the generic interfaces, APECS has been able to accommodate all facility and PI instruments during the past 15 years. In recent time many improvements were made to handle the ever increasing data rates and volumes. New calibration schemes can cope with the very wide bandwidths of current and future instruments. APECS was also adapted to use the new wobbler and hexapod of the APEX-II upgrade.
Future developments will address porting the software to Python 3 and migrating the real-time computer to a modern platform."
"P11-188","Thomas Juerges","ASTRON","It is Tango time:  The M&C system for LOFAR2.0 stations","After ASTRON has gathered 10 years of experience with the LOFAR telescope's station M&C system, a reevaluation took place and it was decided that  the new hardware for LOFAR2.0 stations also warrants a new M&C system.

The foundation of the LOFAR2.0 station Monitor and Control system (M&C) will be the Tango Controls framework.  It allows for an object oriented software architecture which naturally leads to a hierarchical alignment of the software components.

The software will be developed in Python3 with the option to use C++11 where performance issues arise."
"P11-189","Yoko Okada","I. Physikalisches Institut, Universität zu Köln","Observing scripts in kosma_software used on GREAT/SOFIA: modularity, flexibility, efficiency","In this poster contribution, we would like to present the development of the observing scripts that are currently used in GREAT (German REceiver for Astronomy at Terahertz Frequencies) observations onboard SOFIA (Stratospheric Observatory for Infrared Astronomy), and has been used in SMART (SubMillimeter Array Receiver for Two Frequencies) observations on the NANTEN2 observatory. The scripts are part of the whole observing software, named kosma_software, which controls the communication to the telescope and the instrument backends for consistent operations. kosma_software has been developed since 25 years and its modularity allows it to be used in different observatories and instruments; KOSMA 3m telescope, SMART on NANTEN2, GREAT and FIFI-LS on SOFIA, and it is also planned to be used in the CHAI (CCAT-prime Heterodyne Array Instrument) observations on Fred Young Submillimeter Telescope. The observing scripts are the higher level bash scripts that create a sequence of commands in a fool-safe way and with a full flexibility to modify any parameter on spot. This is especially important in SOFIA observations which has a strict time constraint and the highest efficiency is demanded. We introduce general key features as well as SOFIA specific functions of the observing scripts."
"P11-196","Gabriele Riccio","National Centre of Nuclear Research - Warsaw","How reliable are galaxies physical parameters estimations for LSST main sequence sample?","The upcoming Large Survey of Space and Time (LSST), conducted by
Vera Rubin Observatory, will produce, over
a 10-year period, multi-petabyte archive of images and catalogs of astrophysical
sources on more than 18000 square degrees of the southern sky.
Reaching magnitude
depth of ∼26.5 (AB) in the six bands ugrizy, LSST data will be useful
to perform a wide variety of high precision statistical studies,
allowing to obtain
more accurate measurements of astrophysical quantities. I will present
studies based
on simulated LSST observations of real galaxies in the ELAIS-N1 and COSMOS
fields of the Herschel Extragalactic Legacy Project (HELP) survey.
Spectral Energy
Distributions (SEDs) were fitted to the real and simulated photometric
measurements
of 65,889 galaxies in the redshift range 0 < z < 2.5, using the latest release
of a galaxy SED fitting code CIGALE. We compare main galaxy physical parameters,
such as star formation rate (SFR), stellar mass and dust luminosity
obtained from
real data using ultraviolet and
infrared observations
to the same parameters obtained from the simulated optical LSST measurements only.
We conclude there is a possible overestimation of SFR, dust luminosity and
dust mass if they are calculated with LSST photometric measurements only.
This overestimation is found to depend on redshift, diminishing up to z = 2.5.
The least sensitive parameter is the stellar mass which was found to
be reliably estimated
even if based only on the optical bands."
"P11-197","Francesco Pistis","National Centre for Nuclear Research (NCBJ)","VIPERS: Fundamental Metallicity Relation and its projections. How observational bias affect their shape","Galaxy metallicity, a result of the integrated star formation history and evolution of the interstellar medium, is an important property describing the galaxy evolution. As such it has been widely studied in the local Universe with the data from the SDSS,
as well as its relations with galaxy stellar mass and SFR. The relation between these three galaxy physical properties, known as Fundamental Metallicity Relation (FMR), was shown not to undergo any significant evolution at least up to z~2. Despite that, different studies find some differences in 2D projections of this relation.
However, these studies are based on very different samples, with different data selection at different redshift ranges. In our work we aim at finding FMR evolution from z~0.6 to z~0, making use of the unprecedented statistics of the VIMOS Public Extragalactic Survey (VIPERS) and comparing it to the local SDSS sample. Having that goal in mind, we study the effect of different selection bias introduced into the SDSS sample on both the FMR and its 2D projections. We find significant differences occurring when different data selection, mimicking the selection of higher redshift samples, is applied. Then, we compare these results with the data from the VIPERS sample at z~0.6. We conclude that both FMR and its projection at z~0.6 to z~0 are not in agreement even when the data selection effects are carefully applied. This implies a small but statistically significant evolution of the FMR between z~0.6 to z~0 which needs to be taken into account in future studies."
"P11-199","Yuji Shirasaki","National Astronomical Observatory of Japan","JVO Subaru Suprime-Cam Mosaic Image Archive DR2","Suprime-Cam is a wide field imager attached to the Subaru Telescope,
which covers a 34'x27' field of view.
During its operation, which started in 2001 and ended in 2017, it
observed more than 1000 deg2 of the sky.
All the raw data are publicly available thorough the SMOKA system and the
mosaiced images are provided at the JVO system for a quick-look purpose,
both of which are operated by ADC/NAOJ.
We have reprocessed all the Suprime-Cam data by using the updated reduction
pipeline developed by JVO and replaced with the new dataset on the JVO Suprime-Cam archive.
Astrometric and photometric calibration were made by using Pan-STARRS1
DR2 catalog.
All the data are accessible through VO interfaces (SIA/TAP/HiPS).
We also redesigned and updated the GUI of the archive by utilizing
the functionality of Aladin-Lite.
We describe the measured quality of the processed data and the data
search system for this dataset."
"P11-203","Mahmoud","National Centre for Nuclear Research, ul. Pasteura 7, 02-093 Warszawa, Poland","Dust attenuation in ALMA-detected dusty star-forming galaxies in the COSMOS field","Despite its low contribution to the total mass of the interstellar medium (ISM), dust plays a crucial role in the stellar evolution and in the evolution of galaxies as a whole, and it has the biggest impact on the shape of their total emission. The affluence of infrared and radio detections of millions of galaxies in the COSMOS field, provided by powerful instruments such as Herschel and ALMA, has allowed us to study dust in galaxies and its variation over a wide range of redshift.
A key element in reproducing the total spectral energy distribution of galaxies, is assuming a dust attenuation law which accounts for the behaviour and the imprints of dust in the ISM. However, different studies have shown that a single law cannot fully model dust in a large sample of galaxies. This non-universality of attenuation laws should be considered in order to accurately account for dust, and therefore in deriving the physical properties of galaxies.
In this work, we study different attenuation laws in a statistical sample of ALMA-detected galaxies in the COSMOS field, and the resulting variation of key physical properties of these galaxies such as the star formation rate, the stellar mass and the dust to stellar mass ratio.
Furthermore, we investigate the dust morphology, as well as the environments of dusty star-forming galaxies, and discuss their effects on the total dust attenuation and the evolution of some of the important physical properties across the cosmic time."
"P11-208","Salvatore Savarese","INAF Osservatorio Astronomico di Capodimonte","An automated pipeline for the VST data log analysis","Every night the VST Telescope Control Software logs information about the telescope and instrument operations, executed commands, failures, weather conditions and anything relevant for the instrument maintenance and the identification of problem sources. All this information is recorded in text files and stored in a large archive. Individual files are often examined by the observatory personnel for specific issues and for solving well identified problems raised during the night, in the framework of dedicated and focused efforts. Thus, only a minimal part of the information is useful for this kind of daily maintenance.
Nevertheless, the analysis of the archived information collected over a long time span can be exploited to reveal useful trends and statistics about the telescope, which would otherwise be overlooked. Given the large size of the archive, a manual inspection and handling of the logs is cumbersome. An automated tool with an adequate user interface has been developed to scrape specific entries within the log files, process the data and display it in a comprehensible way. This pipeline has been used to scan the information collected over 5 years of telescope activity."
"P1-121","Alessandro Costa","INAF Catania Astrophysical Observatory","The architectural design for the logging, monitoring and alarm systems for the ASTRI mini-array","The ASTRI (Astrofisica con Specchi a Tecnologia Replicante) Mini-Array project is a wide international effort led by the Italian National Institute for Astrophysics to build and operate an array of nine ASTRI telescopes*.  The Mini-Array is  sensitive in the energy range 1-200 TeV with an angular resolution of a few arcmin and will be operated as an instrument dedicated to very high-energy gamma ray astrophysics.
Large volumes of monitoring  and logging data result from the operation of such a  large scale astrophysical infrastructure. In the last few years several “Big Data” technologies have been developed to deal with a huge amount of data especially in the Internet of Things (IoT) framework.
We present the logging, monitoring and alarm system architecture for the ASTRI Mini-Array in order to improve the operations of the telescope facility. A prototype was designed and built employing the latest software tools and concepts coming from Big Data and Internet of Things and a particular emphasis has been given to meet quality requirements such as performance, scalability and availability."
"P11-211","Graham Bell","East Asian Observatory","Five years of the new 'Hedwig' proposal system at the JCMT","'Hedwig' is an observing proposal handling system developed by the East Asian Observatory for use at the JCMT.  It has been used successfully for the last 5 years. The system is open source and designed to be easy to customize for use at other telescopes.  For flexibility, semesters, submission queues and calls for proposals are all defined via the web interface.  Time-consuming tasks (e.g. processing uploads and looking up publication references) are performed in the background to keep the web interface running smoothly even during busy times.

The system includes a number of integrated tools to assist with proposal preparation and review.  These include integration time calculators for SCUBA-2 and JCMT's heterodyne instruments, a 'clash' tool to compare target positions to large programs or archival data, and a target availability tool to check that objects are suitable for a given semester.

It is loosely integrated with the JCMT's Observation Management Project (OMP).  Project definitions are automatically created for accepted proposals and Hedwig provides an OAuth2 service to allow users to log in with a single account.  The sky coverage of archival data is fed back to the proposal system in MOC format."
"P11-214","Kuo-Song Wang","ASIAA","Efficient image visualization with CARTA - introduction and development update","CARTA, Cube Analysis and Rendering Tool for Astronomy, is a new generation of image viewer for visualizing large images from telescopes such as ALMA, VLA, MeerKAT, and ASKAP. The core design philosophy is to ensure scalability for the future telescopes and to provide best user experience. In this poster presentation, we update the community with the development progress since ADASS XXIX. The latest release is v1.4 (September 2020) which includes a number of practical functions and tools for common use cases. More information can be found in the CARTA homepage (https://cartavis.github.io)."
"P11-220","Yohei Hayashi","NAOJ","Automated system to generate calibrated MeasurementSet in East Asian ALMA Regional Center","We have developed a sophisticated system that automatically generates calibrated ALMA observation data called “calibrated MeasurementSet” (MS) by simply inputting the unique identifier of the dataset. 

The ALMA Science Archive (almascience.org) serves users with raw observation data, pipeline or manually reduced products including FITS images of science targets, and scripts to restore calibrated MS which is required to create FITS images. Users should execute the script by themselves on their own computers to obtain calibrated MS, since they are not served by the Science Archive. With our new system, users only have to specify the data set and run a command on the provided platform, and the data is ready to go. There is no need to download the data to restore calibrated MS, nor to prepare their own environment for it.

The script requires the same data reduction software (CASA: Common Astronomy Software Applications) including its version with which the product was created. Since there is a strong dependency between the versions of CASA and the operating system (OS), the end of life (EoL) of a certain OS could prevent the users from restoring the MS for ALMA. RedHat Enterprise Linux 6 and equivalents are planned to end their lives in November 2020, which may risk the users who need to restore the data processed with a certain version of CASA with this OS. To overcome this problem, series of CASA distributions tied to shortly obsolete OS are containerized using Docker technology to make them work on more recent versions of OS. As long as the Linux kernel remains backward compatible in the future, the container can be made to work. Keeping the software in a condition to work in the future, it is important to ensure the reproducibility of archived products in the future.

Since April 2020, for the dataset being reduced with previous versions of CASA, East-Asian ALMA Regional Center (EA-ARC) is starting a service to generate and provide the calibrated MS files upon the request from users of ALMA. The system has recently been deployed in EA-ARC to facilitate their operation. Users of ALMA have the benefit of this system through the service provided by EA-ARC."
"P11-239","Anastasia Alexov","National Solar Observatory","Lessons Learned regarding Software Tools used during the 1st DKIST Proposal Call”","The Daniel K. Inouye Solar Telescope's (DKIST) first Call for Proposals was open from May 15, 2020 through August 14, 2020. We will present the software tools used during the proposing phase focusing on:  Amazon Web Services for the Proposal Tools Web Interface, Atlassian's Jira Service Desk for the Help Desk and Confluence for the Knowledgebase, as well as Google Forms for surveying Proposers, and Slack for quick communication and troubleshooting amongst team members. We will discuss what software services and tools worked well for our purposed process and where improvements could be made. Furthermore, we will present some software-related lessons learned based on initial feedback from participants in the Proposal Call."
"P11-240","Carlos Espinosa","Instituto de Astronomía, Universidad Nacional Autónoma de México","HII regions in the CALIFA survey","We present a new catalog of HII regions based on the integral field spectroscopy (IFS) data of the extended CALIFA and PISCO samples. The selection of HII regions was based on two assumptions: a clumpy structure with high contrast of Ha emission and an underlying stellar population comprising young stars. We performed the segregation of HII regions using pyHIIexplorer. This code allows us to detect ionized regions base on two assumptions: HII regions have strong emission lines that are clearly above the continuum emission and the typical size of HII regions is about a few hundreds of parsecs. The catalog provides the spectroscopic information of 26,408  individual regions corresponding to 924  galaxies, including the flux intensities and equivalent widths of 51 emission lines covering the wavelength range between 3745-7200A."
"P11-254","Miguel Sánchez-Portal","Instituto de Radioastronomía Milimétrica (IRAM)","Improving the good: the upgrade of the IRAM 30-meter millimetre telescope","The IRAM 30-meter millimetre radio telescope is located in the Observatorio de Pico Veleta in Granada, Spain and  has been continuously operating around the clock since its inauguration in 1984. It is still one of the most sensitive and powerful telescopes  worldwide in its frequency range. It has been recognised as one of the facilities within the Spanish Map of Singular Scientific and Technical Infrastructures (ICTS).  IRAM  has embarked in an ambitious project to upgrade the 30-meter to keep it in the research leading edge in the coming years. The planned refurbishment will include a new servo control system for the mount and sub-reflector, comprising hardware and software components, and upgrades in the primary reflector to improve the thermal behaviour, surface accuracy and gain-elevation curve. This contribution gives some details on the planned actions, that will be co-financed  by European Regional Development Fund (ERDF) programme."
"P1-165","V.N. Pandey","ASTRON","LOFAR strides towards a truly multi-tasking radio telescope with its mega mode correlator","LOFAR's new brain/correlator (COBALT2.0) is turning LOFAR into a truly multi-tasking radio telescope without changing the collecting elements. It enables LOFAR to carry out  observations for several science cases  in parallel (LOFAR Mega Mode), resulting in increased efficiency. We present the performance capabilities and salient features of optimally designed COBALT2.0. The increased capacity also enables us to improve data quality and  further explore possibility for unexplored new science with LOFAR."
"P11-69","cosimoNigro","Institut de Fisica d'Altes Energies (IFAE)","agnpy: numerical modelling of AGN spectra in python","Built on numpy and astropy, [agnpy](https://github.com/cosimoNigro/agnpy) allows to compute the spectral energy distribution (SED) produced by the radiative processes of relativistic electrons accelerated in the jets of Active Galactic Nuclei.    
The photon emission via synchrotron and inverse Compton radiation can be computed for a population of particles confined in a spheroidal emission region (blob). While the radiation field of several AGN components (Accretion Disk, Broad Line Region, Dust Torus, the very same synchrotron radiation) can be selected as target for the inverse Compton, the gamma-gamma pair-production opacity of the latter can be simultaneously estimated.     
agnpy provides an important missing piece in the field of blazar modelling and in the growing community of python-based astronomical software."
"P1-184","Robert Nikutta","NSF's NOIRLab","Development, deployment, and testing of Jupyter notebooks at Astro Data Lab","Astro Data Lab is an open-data / open-access astronomical science platform built and operated at NSF's National Optical-Infrared Astronomy Research Laboratory's (NOIRLab) Community Science and Data Center (CSDC). It provides free access to hundreds of Terabytes of astronomical survey catalogs and over 2 Petabytes of associated images, paired with co-located compute resources. The main access mode is through Jupyter notebooks, using IVOA protocols and Data Lab APIs. Our poster describes how a Data Lab curated notebook is born and maintained with source control on GitHub (often in collaboration with users), how we deploy it to our servers using webhooks, how it is launched at scale on our notebook server, and how we test our entire notebook suite using nbconvert and a custom workflow."
"P1-186","John Beasley","NRAO","Accessing Radio Data with the NRAO NINE Program","The goal of the project carried out by NRAO NINE's summer students was
to prototype a more accessible way for astrophysics curriculum to be
created and distributed to lower income minorities.  A set of lessons
was chosen that could take a student with little to no knowledge to
doing analysis of astronomical data, specifically obtained through sky
surveys currently underway with the Very Large Array.  These lessons
were created and distributed using industry standard tools like Jupyter
and git. We found that the most accessible way to create a curriculum
was to make it engaging, interactive, and provide use outside the
context of the specific lessons, which was made possible by markdown
integration in Jupyter notebooks. The uses and implications of
server-side computing for a class of students was also revealed through
the development of this prototype"
"P1-190","Fulvio Gianotti","INAF-OAS","ASTRI Virtual Test Bed: from Prototype to Mini-Array","The ASTRI (""Astrofisica con Specchi a Tecnologia Replicante Italiana"") program  a collaborative international effort led by the Italian National Institute for Astrophysics (INAF) design and an end-to-end prototype of the same class as the Cherenkov Telescope Array Observatory (CTAO) The prototype, named ASTRI-Horn d'Arturo, is currently in operation at the INAF observing station ""G. Fracastoro”, located in Serra La Nave on Mt.Etna (Italy). The ASTRI project is  a mini-array of nine ASTRI telescopes to be installed at the Teide Observatory, in the Canary Island.
In order to support the operations of the ASTRI Prototype and Mini-Array, a specific and sophisticated software is necessary. This software be designed and developed following software engineering , i.e. going through the steps of: development, testing, distribution, and version control. . paper we show how the ASTRI Prototype Test Bed has been organized and how the virtual system that hosts it was built. We also present the system hardware architecture and its components and explain how this is used to create a virtualization system, in accordance with the Test Bed requirements. Prototype Test Bed has been exploited to understand and optimize the design of the virtualization system that host the ASTRI Mini-Array Test Bed. For the design of this system we started from the study of a different Open Source virtualization software: ProxMox. This software solves many of the problems we had with the previously adopted software. The decision to use the ProxMox software has a impact on hardware architecture. This kind of architecture can be realized in a very simple and economical wa, performance, redundancy, and reliability requirements."
"P1-228","Rafael Duarte Coelho dos Santos","Instituto Nacional de Pesquisas Espaciais","CV Portal: A collaborative cataclysmic variables web-based portal","Rafael Santos
C. V. Rodrigues
Marcia Beatriz Pereira Dominguez
Felipe Carolino
Alexandre Soares de Oliveira (Univap) 
Diogo Belloni (INPE)
Alessandro Ederoclite (IAG/USP)

We present the status of the development of a collaborative web-based portal devoted to cataclysmic variables (CV) research. CVs are binary systems composed of a white-dwarf star and a main-sequence star in a compact configuration that allows mass transfer between the stars. The matter flow is responsible for a plethora of observable variability, among them bright outbursts, which give them the cataclysmic adjective. The first version of the CV portal provides a cooperative catalog of CVs. Currently, the main available CV catalogs are Ritter & Kolb (2003, A&A, 404, 301 - v7.24 - Dec/2015) and Downes & Shara (1993, PASP, 105, 127 - v. Feb/2006), RK and DS, respectively, which are no longer updated. We used the data contained in these 2 catalogs as a seed for the CV Portal catalog. It has 2434 objects: 1361 from RK; 1830 from DS; being 757 objects common to both catalogs. The current version allows the user to inspect (or download) the entire catalog or search for objects by name or coordinates. The object page has direct links to ADS and SIMBAD queries of the object, which helps the user to access, for instance, bibliographic information. The user can also submit new entries to the catalog on a single-object basis by a form or by submitting a CSV file. Those submissions should be approved by one of the CV Portal administrators and, after that, are automatically included in the catalog. We anticipate a large number of CVs discoveries by new instruments as J-PAS, J-PLUS, e-Rosita, and LSST, making this collaborative portal a timely development.

References

Downes, R. A., Shara, M. M., 1993, A catalog of Cataclysmic Variables, PASP, v.105, p.127.
Ritter, H., Kolb, U., 2003, Catalogue of cataclysmic binaries, low-mass X-ray binaries and related objects (Seventh edition), A&A, v.404, p.301-303."
"P1-248","Bruno Khelifi","APC, IN2P3/CNRS - Université de Paris","A prototype for handling observations proposals for the CTA observatory","The CTA (Cherenkov Telescope Array) experiment is an international observatory dedicated to the observation of the gamma rays sky from ~30GeV to 300TeV. The achievement of its two sites (in Canaries islands and in Chili ) is planned for 2025. As 60% of the observation time will be reserved to open Announcement of Opportunity calls, the CTA Observatory will offer a web platform on its Science Portal to prepare, submit, evaluate and keep track observation proposals.
A demonstrator, the CTA Proposal Handling Platform (CTA-PHP), has been elaborated in this purpose. Using the high-level Python Web framework Django, the CTA-PHP offers an open-source prototype that minimizes the long-term maintenance regarding future evolutions of the proposal items, for both the web Front-End and the database Back-End.
This poster presents its goals, its design and a short description of the envisaged implementation."
"P1-255","sara bertocco","INAF-OATS","Requiement analysis for HPC&HTC infrastructures integration in ESCAPE Science Analysis Platform","ESCAPE (European Science Cluster of Astronomy & Particle physics ESFRI research infrastructures) is a project to set up a cluster of ESFRI ( European Strategy Forum on Research Infrastructures) facilities for astronomy, astroparticle and particle physics to face the challenges emerging through the modern multi-disciplinary data driven science. This cluster should state a functional connection between the interested ESFRI projects and the EOSC (European Open Science Cloud) providing tools and solutions according to FAIR (Findable, Accessible, Interoperable and Reusable) principles. 
One of the main goal of ESCAPE is the building of ESAP (ESFRI Science Analysis Platform), a flexible and expandable science platform for the analysis of open access data available through the EOSC environment. ESAP will allow EOSC researchers to identify and stage existing data​ ​collections for analysis, share data, share and run scientific workflows.
For many of the concerned ESFRIs and RIs, the data scales involved require significant computational resources (storage and compute) to support processing and analysis. The EOSC-ESFRI science platform therefore must implement appropriate interfaces to an underlying HPC (High Performance Computing) or HTC (High Throughput Computing) infrastructure to take advantage of it. Accessing data and deploying user-initiated processing and analysis tasks on this HTC and HPC infrastructures both in batch mode or maintaining interactivity and responsiveness in the analysis system will be a challenge.
This poster describes the analysis done to identify the main requirements for the implementation of the interfaces enabling the ESAP data access and computation resources integration in HPC and HTC computation infrastructures in terms of authentication and authorization policies, data management, workflow deployment and run."
"P1-59","Hugh Dickinson","The Open University","Discovering data with the ESCAPE Science Analysis Platform","We present a suite of online data discovery tools that have been developed for the ESCAPE Science Analysis Platform (ESAP). ESCAPE (European Science Cluster of Astronomy & Particle physics ESFRI research infrastructures) brings together the astronomy, astroparticle and particle physics communities to establish a single collaborative cluster of next generation facilities to build the astronomy and particle physics cell of the European Open Science Cloud.

ESAP is designed to be flexible and extensible with a modular structure that encapsulates its different functional components. The ESAP backend is written in Python using the Django framework and communicates with a Javascript frontend via a RESTful API. 

One of the core components of the ESAP is a generic interface that enables discovery of Virtual Observatory (VO) resources by querying the VO registry. Once suitable VO resources have been discovered, ESAP provides a flexible interface to select and retrieve specific subsets of data and stage them for interactive or batch data analysis.

The ESAP data discovery framework also enables support for non-VO data archives with minimal developer effort. As an example, we show how citizen science classification data from the Zooniverse platform can be accessed via ESAP."
"P1-82","Katrina Sealey","AAO - Macquarie University","Data Central Science Platform (Astronomy and Beyond)","Data Central (datacentral.org.au) is an e-research platform and data archive developed at AAO Macquarie that facilitates cutting-edge science. It provides web-based tools and archive functionality for scientists from a range of disciplines to explore, collaborate and make new discoveries.

Data Central provides a suite of tools aimed at facilitating the smooth and successful execution of large survey projects and management of the interactions between international teams of researchers. The Data Central Teams suite includes; an in-house content management system allowing survey teams to create and curate the documentation that describes their data; a private workspace to host their survey data, a ready-to-go wiki system; and a portal that allows teams to manage who has access to their data and services within Data Central.

Although created for and used by the astronomy community over the last 5 years, Data Central is now expanding beyond its astronomy boundaries to facilitate research in other fields, including biosciences, archaeology, and physics. This talk will describe the platform and discuss how and why we are broadening the Data Central horizon."
"P2-108","Demitri Muna","University of Texas, San Antonio","Removing What’s Unnecessary: A New Design Language for Working With Astronomical Data","Accessing astronomical data requires learning the details of what data is available, where it is located, how to retrieve it, managing it when it is downloaded, and how the data are organized within the dataset. This is then multiplied by the number of datasets potentially useful to a given analysis, e.g. across all wavelengths, and again dealing with the differences between working on a laptop, local servers, or cloud computing. However, nearly all of these concerns are deterministic implementation details. I will present a new framework that aims to pull together all of these details otherwise contained in headers, catalogs, web documentation, and journal papers into a single, unified interface, freeing the researcher to stop worrying about the minutia of how to get data and concentrate on understanding it. The design language that motivated the framework will be discussed, the current state of the project, and future directions."
"P2-122","Adam Brazier","Cornell University","SCIMMA: Scalable Cyberinfrastructure to support Multi-Messenger Astrophysics","Multi-Messenger Astrophysics (MMA) brings together communities working in astrophysics, gravitational wave science and particle physics. Progress in real-time MMA requires reliable, performant and scalable cyberinfrastructure that 1) connects experiments and researchers at low latency to remove communications barriers, 2) enables immediate data analysis as information is received, and 3) provides tools to coordinate observational follow-up and data sharing that takes ownership, embargo periods, and intellectual/project credit into account. The SCIMMA collaboration is working with the MMA community to prototype the cyberinfrastructure tools it needs and to conceptualize the organization necessary to ensure sustainability. We will discuss two key cyberinfrastructure products we have prototyped: a next-generated publish-subscribe system, SCIMMA Hopskotch, and an integration of Identity and Access Management systems into MMA cyberinfrastructure."
"P2-16","Eva Sciacca","INAF - Osservatorio Astrofisico di Catania","Novel EOSC Services for Space challenges: the NEANIAS first outcomes","The European Open Science Cloud (EOSC) initiative faces the challenge of developing an agile, fit-for-purpose and sustainable service-oriented platform that can satisfy the evolving needs of the scientific community by stimulating the design and prototyping of novel innovative digital services. 
 
The “Novel EOSC Services for Emerging Atmosphere, Underwater & Space Challenges” (NEANIAS) is an ambitious project that is promoting Open Science practices and is playing an active role in the materialization of the EOSC ecosystem by efficiently engaging large scientific and professional communities, actively contributing to the technological, procedural, strategic and business development of EOSC.  
 
This work presents the NEANIAS activities and first results related to the co-design and delivery of three innovative services, derived from state-of-the-art research assets and practices in the Space research, that mainly includes astrophysics and planetary  science  as  well  as computer  science  and  software  engineering tailored to image processing, computer vision and machine learning. The NEANIAS Space services, namely SPACE-VIS, SPACE-MOS and SPACE-ML, deliver a springboard of tools to enrich the workflows of a wide range of targeted users from academic/research institutions to industrial stakeholders e.g. aerospace engineering and related technological companies, but also national space agencies and public outreach bodies e.g. space museums and planetariums. The SPACE-VIS service provides an advanced operational solution for data management and visualization of astrophysics and planetary data adopting FAIR (Findable, Accessible, Interoperable, Re-usable) principles. The SPACE-MOS service provides functionalities for making high quality images from the raw data captured by instruments (map making) and for assembling those images into custom mosaics (mosaicking). Finally, the SPACE-ML service provides advanced solutions for pattern and structure detection in astronomical surveys as well as in planetary surface composition, topography and morphometry. The user requirements collected to guide the software development will be introduced as well as the services delivery processes following the NEANIAS release procedures and the integration into the EOSC platform, finally, the future plans for the next release will be summarized."
"P2-205","Ryan Raba","National Radio Astronomy Observatory","Next Generation CASA","The CASA team is working to design and prototype the next generation of Common Astronomy Software Applications (CASA) for radio interferometry data reduction.  The next generation CASA technology stack invests heavily in Python-based frameworks for parallelization and data manipulation, including Dask and Xarray, along with a new functional design paradigm.  This approach simplifies the code base to speed development, improve reliability, and reduce maintenance.  Prototyping efforts are currently underway to include a public demonstration package available in the near future."
"P2-219","Marco Landoni","INAF - National Institute of Astrophysics","Application of Commercial Cloud Platforms in Astrophysics. The case of Amazon Web Services","We review the application of Amazon Web Services (AWS) cloud platform in the context of the Italian National Institute of Astrophysics (INAF). In particular, we report how the service is offered to the community (IAM accesses, consoles, users and groups) and we report on the projects that have successfully exploited the capabilities of the AWS platform. We thus concentrate by discussing the general classes of problems that can benefit the availability of commercial cloud platforms technologies, especially in the light of the complexity of the computational models required in the domain of Astrophysics."
"P2-230","Nicholas Walton","Institute of Astronomy, University of Cambridge","Advanced Analysis Systems at CASU","This poster presentation will discuss the large range of advanced, high throughput, analysis systems that are currently under development in the Cambridge Astronomical Survey Unit (CASU). These include, for instance,  the major systems for the analysis and data management for WEAVE on the 4.2-m WHT, 4MOST on ESO VISTA, the exoplanet analysis system for PLATO, elements of the OU-VIS pipeline for Euclid, and a number of applications in the medical domain. We will note how CASU are providing (Jupyter notebook) access to their advanced data products, making use of the UK's IRIS high throughput data infrastructure."
"P2-236","Jan-Willem Steeb","National Radio Astronomy Observatory","Implementing a NUFFT in Dask for Radio Astronomy Applications","The Common Astronomy Software Application (CASA) is a widely used radio astronomy data reduction and analysis software package. It has a Python user interface and is implemented in C++ (with some Fortran). CASA has been under continual development for more than 14 years. Consequently, it has become complex and difficult to maintain. To demonstrate that the core functionality of CASA can be successfully re-implemented, the non-uniform FFT (NUFFT) was chosen as a representative function.

A pure Python framework was chosen for this test implementation due to the language's almost ubiquitous use in the astronomy community. One disadvantage of using pure Python is that it is an interpreted language which can be slow relative to compiled languages.  To overcome this limitation we adopt a framework that makes use of the following Python packages: Zarr, Numba, and Dask. Zarr is used to store visibility data as chunked N-dimensional arrays. If code can not be easily vectorized, Numba is used for just-in-time compilation. Concurrency is achieved using the dynamic task scheduling library Dask.

For datasets that can fit into memory Dask NUFFT and CASA NUFFT have similar performance. Dask NUFFT outperforms CASA NUFFT when the visibility data is larger than memory and we demonstrate an appreciable speed-up across a set of data sizes and hardware configurations, potentially indicating more efficient blocked data management using this open-source framework than our tuned custom implementation."
"P2-246","Marcellin","Rhodes University","xova - a Baseline-Dependent Time and Channel Averaging Implementation","Traditional radio interferometric correlators produce regular-gridded samples of the true uv-distribution by averaging the signal over constant, discrete time-frequency intervals. This regular sampling and averaging then translate to be irregular-gridded samples in the uv-space, and results in a baseline-length-dependent loss of amplitude and phase coherence, which is dependent on the distance from the image phase centre: this effect is known as decorrelation. 

In this poster, we discuss the implementation and the theory behind  Xova;  a software package that implements Baseline-Dependent Time and Channel averaging on Measurement Set data. uv-samples along a baseline track are aggregated into a bin until a specified decorrelation tolerance is exceeded. The degree of decorrelation in the bin correspondingly determines the amount of channel averaging that is suitable for samples in the bin. This necessarily implies that the number of channels varies per bin and the output data loses the rectilinear input shape of the input data.

The BDA averaging algorithm used in xova is implemented in the codex-africanus algorithms library via parallel blockwise operations on dask arrays. Each individual operation is accelerated with numba. CASA table columns are exposed as dask arrays by the dask-ms data access library, which also supports writing dask arrays to both existing and new Measurement Sets. As the number of channels varies per bin, xova writes variably shaped data columns to new Measurement Sets. While this is supported by the CASA table system and the MSv2.0 specification, DDFacet is the only imager currently capable of correctly reading these columns. To the best of our knowledge, this is the first implementation that can output Time and Channel BDA data to a Measurement Set.

Additionally, care has been taken to ensure that columns are averaged according to their nominal and effective definitions in the MSv2.0 specification. Columns such as TIME and INTERVAL include bad or missing (i.e. flagged) data in a nominal average, while TIME_CENTROID and DATA exclude bad or missing data in an effective average. Thus, fully flagged bins are not merely discarded but are retained so that correct TIME and INTERVAL grids remain established for the downstream applications.

Xova shows a promising compression capability while maintaining decorrelation constant across all the baselines when the BDA option is activated. BDA averaging of data produced by the MeerKAT telescope shows that a data compression factor of 40X and outer field-of-interest suppression are achieved."
"P2-259","Susana Sánchez Expósito","Instituto de Astrofísica de Andalucía - CSIC","SKA Regional Centre Prototype at IAA-CSIC: building an Open Science platform based on cloud services","The Square Kilometre Array (SKA) is a project to build a radio-interferometer capable of making revolutionary contributions to Astrophysics, Astrobiology and Fundamental Physics. It will be the largest generator of public data, producing around 600 Petabytes of data per year. These data will be delivered to a network of SKA Regional Centres (SRCs), which will provide access for an international community to SKA Observatory data and the analysis tools as well as the processing power necessary to fully exploit their science potential.

At the IAA-CSIC, in Granada, Spain, we are leading the Spanish effort to host an SRC. We are currently developing an SRC prototype aiming both to support users working with SKA precursors and pathfinders, and constituting a transversal, wavelength agnostic facility enabling knowledge exchange among a diverse community of users. We have recently deployed the first stage of the hardware, based on a cloud environment to be able to provide interoperable and flexible services. We will build a platform that integrates different components such as a JupyterHub server and Virtual Observatory services, among others, crucial to address the challenge of extracting scientific knowledge in a reproducible way, and hence the adoption of Open Science values. In this poster we will present the main characteristics of the hardware, software and services of the SRC prototype at the IAA."
"P2-263","Camilo Núñez Fernández","Universidad Técnica Federico Santa María","ChiVOData:  A scalable data science platform for  the heterogeneity of astronomical data in an HPC environment.","One of the limitations for any interactive astronomical data analysis platform is that in most cases there is a one-to-one relationship between observation and reduction pipeline. This limits the scalability and integration of these platforms with new data sources. On the other hand, these platforms must have the capacity to manage large volumes of data and offer the interfaces for the use of high-performance hardware. ChivoData is a new generation of astronomical data analysis platforms, built for the heterogeneity of astronomical data and its interoperability. It is based on microservices architecture, where each work environment is packaged with its libraries and software in an HPC container managed by Singularity. These containers are consumed as processing kernels by a jupyter notebook based platform. The latter allows the user to transparently use the HPC resources managed by SLURM. In addition, ChivoData is fully integrated with Luster file system, which allows the user to have their data centrally, only changing the processing engine according to the data they want to process. ChiVOData is a scalable full data science platform capable of supporting the heterogeneity of astronomical data in an HPC environment."
"P3-134","Damien de Mijolla","University College London","Disentangled Representation Learning for Astronomical Chemical Tagging","Modern astronomical surveys are capturing spectral data from millions of stars. These spectra contain chemical abundance information that is valuable for understanding the Galaxy's formation history. However, extracting the  information from spectra is challenging. Here, we present a data-driven method for isolating the chemical factors of variation in stellar spectra from those of other parameters (i.e. \teff, \logg, \feh). We do this with no knowledge of elemental abundances and hence bypass the uncertainties and systematics associated with synthetic stellar spectra. To remove known non-chemical factors of variation, we develop and implement a neural network architecture, with a disentanglement loss. We simulate our recovery of chemically identical stars in a synthetic APOGEE-like dataset. We show that this recovery declines as a function of the signal to noise ratio, but that our neural network architecture outperforms simpler modeling choices. Our work demonstrates the feasibility of data-driven abundance-free chemical tagging."
"P3-15","Sara Del Rio","RHEA Group for ESA - European Space Agency","SOCCI - A Multi-Mission Software Engineering Platform for Science Operations","In this age, high quality software engineering is the key to success. Currently, with the advances in information technology, a growing set of powerful and complex tools have appeared leading to software engineering environments to assume crucial importance.

The Science Operations Configuration Control Infrastructure (SOCCI) platform is born to address the specific needs for Science Operations at ESA's European Space Astronomy Center (ESAC). The platform aims to support software development process and science systems operations by offering a single framework which makes those tasks easier.
 
SOCCI provides a suite of products covering every aspect of software engineering, from requirements gathering to testing. The platform is based on state-of-the-art technologies such as Jira, Confluence, Bitbucket, Jenkins, SonarQube and Nexus. SOCCI is available in two provisioning models: Software as a Service (SaaS) and Software as a Package (SaaP).

SOCCI is operational software since November 2016 and is used by concluded, current and upcoming missions and teams (Integral, Gaia, BepiColombo, PLATO, the ESAC Science Data Centre, the GNSS Science Support Centre are some of them). New organisations, missions and teams are expressing great interest in this environment.

In future releases, improvements and many additional features, such us the Model-Based Software Engineering (MBSE) concept or container-based applications support, are planned."
"P3-162","Thomas Vuillaume","LAPP, CNRS","Open-source Scientific Software and Service Repository","The ESFRI projects in ESCAPE shall provide open access to their data, software and science tools to allow reproducibility of science results and the development of open science analyses across domains and infrastructures, necessary for opening the window to multi-messenger and multi-probe research.

Here we present one of the key component of the ESCAPE H2020 project to achieve this goal: the Open-source Scientific Software and Service Repository (OSSR) that aims at providing the tools necessary for the communities to share their science products in a harmonised way respecting the FAIR principles, promoting open science and maximising cross-fertilisation by software re-use and co-development."
"P3-169","Deborah Baines","ESA, ESAC / Quasar Science Resources","Results from the ESA Astronomy Space Science Archives User Survey","The majority of ESA’s space science missions’ archives are developed and maintained by the ESAC Science Data Centre (ESDC), in coordination with the science operations centres, the instrument teams and the consortia of the various missions. On the astronomy side, the current archives are the Gaia archive, the Herschel Science Archive, the ESA Hubble Science Archive, the ISO Data Archive, the Lisa Pathfinder Legacy Archive, the Planck Legacy Archive, the XMM-Newton Science Archive and the EXOSAT Science Archive. In addition, ESDC develops ESASky, the all-sky science exploration interface, which provides easy access to high-level science data from the ESA archives and archives from major space-based and ground-based observatories. 
In mid 2019, a survey requesting feedback on the ESA astronomy archives and ESASky was initiated by the ESA Astronomy Archives User Group and announced to the astronomy community via various channels. This poster presents the main results from the survey. The overall level of satisfaction manifested by the survey participants is high and is an improvement on the level of satisfaction expressed by participants in the 2011 survey (68% satisfied or very satisfied versus 60%). Individual responses express appreciation of the recent development efforts by the ESDC, highlights being e.g. the ESASky tool and ADQL queries. The future developments identified as highest priority include advanced web-based visualization and cut-out services; astroquery modules; advanced ADQL queries; and access to science platforms."
"P3-171","Eric Howard","Macquarie University","Efficient machine learning methods for cosmology","Machine learning-based analysis techniques are gaining interest in cosmology due to their computational ability to generate complex models in order to analyse and interpret large scale structure data sets, such as the matter density fields comprised of nonlinear complex features, like halos, filaments, sheets and voids. Data-driven cosmological discovery has seen a remarkable rise in the last decade, leading to unprecedented improvements in the ways we can gain knowledge, interpret and extract cosmological features from large data volumes, constraining cosmological parameters and modelling the large structure formation of the Universe. The cross-fertilization between cosmology and machine learning require the integration of traditional statistical techniques with modern machine learning models, providing promising opportunities with significant advantages for state-of-the art cosmological simulations. Multiple applications, from cosmic web simulations and predicting the cosmic structure in the non-linear regime to multi-wavelength structure Identification, 21cm reionization models or predicting dark matter annihilation and halo formation may benefit from robust and efficient data analysis methods, such as convolutional neural networks or generative adversarial networks. We present a number of powerful machine learning algorithms  (classification, regression, reinforcement learning) and data-analysis tools that can be used to predict the non-perturbative cosmological structure and non-Gaussian features hierarchically formed over all scales in the Universe, justifying the advantage of employing such methods for use in cosmology."
"P3-181","Laurent Michel","Observatoire de Strasbourg","MANGO: a VO Model for Source Data","The MANGO model proposes a flexible way to expose data related to astronomical source objects in an interoperable way.
It takes into account the huge diversity of source data in terms of feature description, format and usage.
The MANGO model attaches identifiers on astronomical sources and associates to each a flexible set of parameters (e.g. observed physical quantities) and other information like e.g. spectra, time series or preview images.
Parameters usually appear in the columns of a source catalogue. Additional data products are bound to the source to contribute to the science analysis and enhance data understanding.
Mango object parameters are built upon classes or extended classes of the IVOA Measure and Coordinates data model.
Associated data can be simple URLs, VO service endpoints or VO data model instances.
The roles of both parameters and associated data are qualified by semantic tags"
"P3-204","Mark Allen","CDS, Observatoire astronomiques de Strasbourg","ESCAPE - addressing Open Science challenges.","ESCAPE (European Science Cluster of Astronomy & Particle physics ESFRI research infrastructures) is an EC H2020 project that addresses the Open Science challenges shared by the astronomy, astrophysics and astroparticle physics facilities encompassed within the ESFRI roadmap. This project is being done in the context of the European Open Science Cloud (EOSC) and involves activities to develop a prototype Data Lake and Science Platform, as well as to support an Open Source Software Repository, connect the Virtual Observatory framework to EOSC, and to engage the public in citizen science. We provide an overview of the current status of the project."
"P3-227","Mattia Corpora","INAF - NATIONAL ISTITUTO FOR ASTROPHYSICS","A Multi-protocol Communication System for maintenance procedures of the ASTRI-Horn Cherenkov Camera","Usually, instrumentation for astronomy are complex systems consisting of different hardware subsystems, which communicate by standard communication protocols or their own protocols. This is the case of the ASTRI-Horn Cherenkov Camera, an instrument designed in the context of the ASTRI (Astrofisica con Specchi a Tecnologia Replicante Italiana) project which is a collaborative international effort led by the Italian National Institute for Astrophysics (INAF). The Camera is composed of several hardware subsystems (as power supply system, thermal control system, photon detection modules, etc.) which communicate by different protocols. The instrument commissioning and science verification phases are now completed. Currently, maintenance activities are ongoing and new features are being integrated based on the lesson learned. In this upgrade activity, an easy-to-use hardware and software system has been developed to be used during laboratory test operations in order to add new functionalities. This system allows computers to easily communicate with hardware devices that implement the usual standard communication protocols (as UART, SPI, I2C, etc.) by simplifying and speeding up the laboratory activities. The system consists of a hardware device and a software application and it can be used without knowing programming languages and/or protocol characteristics. In this contribution, a description of the hardware and software components is given."
"P3-231","Patrick Reichherzer","IRFU, CEA (Université Paris-Saclay)","Colibri - The coincidence library for real-time inquiry for multi-messenger astrophysics","Flares of known stable astronomical sources and transient sources can occur on different timescales, from only a few seconds to several days. The discovery potential of both serendipitous observations and multi-messenger and multi-wavelength follow-up observations could be maximized with a tool which allows for quickly acquiring an overview over both stable sources and transient events in the relevant phase space. We here present COincidence LIBrary for Real-time Inquiry (Colibri), a comprehensive tool for this task. 

Colibri's architecture comprises a RESTful API, a real-time database, a cloud-based alert system and a website as well as apps for iOS and Android as clients for users. The structure of Colibri is optimized in terms of performance and exploits concepts such as multi-index database queries, a global content delivery network (CDN), direct data streams from the database to the clients and cashing.

Colibri evaluates incoming VOEvent messages of astronomical observations in real time, stores them in the database and filters them by user specified criteria in the context of known sources from various catalogs. The clients provide a graphical representation with a summary of the relevant data to allow for the fast identification of changes in observed sky regions, and for analyses of those. In this contribution, the key features of Colibri are presented. Details about the architecture and the used data resources are explained. Current and possible future implementations of Colibri will be discussed."
"P3-257","Adrian Garcia Riber","Phd student from Polytechnic University of Madrid","FITS2OSC. A Sonification Pipeline for Lightcurve Interactive Auditory Exploration","With the double intention of expanding the possibilities in the use of astronomical information as sound source for music composition and to allow the interactive exploration of light curve databases through sound in a user-friendly non-time-consuming environment, this work presents the FITS2OSC sonification pipeline that converts FITS files light curve information into OSC data.  

Focused in the case study of the light curve databases captured by the Kepler and K2 missions from 2009 to 2018, the interactive data-driven sonification bridge here proposed makes use of astropy, numpy, matplotlib and python-osc libraries to enhance and maintain up to date the natural and historical relationship between Astronomy and Music. 

This cross-discipline data processing open-source pipeline is conceived as a first step to build a solid bridge towards the development of an astronomical data automatic composition system based on deep learning."
"P3-266","Rosa Diaz","Space Telescope Science Institute","Improvements to development and operations practices in data processing pipelines and astronomy software","While waiting for the James Webb Space Telescope (JWST) launch, we continue improving our development practices and processes, all of them to enhance the collaboration between all disciplines that support JWST science data processing and operations. In this poster, I want to give an update on what we have implemented in the last year and how these improvements have contributed to strengthening the collaboration among all the teams supporting the science and operations of JWST and HST. I also highlight how critical to our success is to be in an organization that recognizes the importance of all of them."
"P3-41","Sankalp Gilda","University of Florida","mirkwook: SED Fitting for the Twenty First Century","One of the most widely used methods for deriving physical properties of galaxies involves modeling their SEDs. While several modern Bayesian codes are routinely used to fit observed photometries with underlying models of star formation, evolution, and dust attenuation, results based on hydrodynamical simulations have quantitatively shown that uncertainties in the derived properties--for example, galaxy mass, star formation history, gas phase dust mass--at a factor of a few level are inescapable. Furthermore, Bayesian fitting is an intrinsically compute-intensive task, often requiring access to expensive hardware for long periods of time. To overcome these shortcomings, we develop 'mirkwood': an explainable, user-friendly tool comprising of an ensemble of supervised machine-learning models capable of mapping galaxy fluxes to their properties. For the first time, we delineate the contribution of each photometric band to galaxy parameters of interest via the use of Shapley values. We also derive both reducible and irreducible predictions errors -- the former refer to uncertainties arising both from missing observations in informative bands and intrinsic noise in observations, while the latter refer to those arising from finite training data and incorrect modeling assumptions. We demonstrate mirkwood's superior performance by training it on a combined data set of z=0 galaxies from SIMBA, EAGLE and ILLUSTRIS-TNG, and comparing the derived results with those obtained from the Prospector.  We envisage mirkwood to be an evolving, open-source framework that will phase out traditional SED fitting."
"P3-42","Sankalp Gilda","University of Florida","deep-REMAP: Probabilistic Parameterization of StellarSpectra Using Regularized Multi-Task Learning","In the era of exploding survey volumes, traditional methods of spectral analysis arebeing pushed to their limits. For efficient stellar characterization, there is a dire needfor accurate synthetic spectral libraries and automated, interpretable analytical tech-niques. In this paper, we develop a novel deep learning framework—deep-RegularizedEnsemble-based Multi-task Learning with Asymmetric Loss for Probabilistic Inference(deep−REMAP)—and demonstrate its effectiveness in predicting stellar atmosphericparameters of observed spectra. Specifically, we train our deep convolution neural net-work  (DCNN)  on  PHOENIX  synthetic  grid,  fine-tune  it  on  a  subset  of  MARVELSobserved spectra of FGK dwarfs, and use it to predict the three main stellar atmo-spheric properties—effective temperature (Teff), surface gravity (log g), and metallicity([Fe/H])—for FGK giant candidates. To incorporate MARVELS instrumental and ob-servational peculiarities as part of the training, we augment PHOENIX spectra withrealistic signatures. When validated on a small subset of MARVELS calibration stars,the fine-tuned model is again able to faithfully recover observed parameters and theirrespective uncertainties, thus demonstrating effective application of transfer learning.Whiledeep−REMAPwas trained on the PHOENIX synthetic grid for use with MAR-VELS observed spectra, it can be easily extended to work with other synthetic spectrallibraries, expanded wavelength ranges, and higher resolution spectral surveys coveringa wider range of stellar properties, including detailed elemental abundances."
"P4-126","Massimiliano Lincetto","Aix Marseille Univ, CNRS/IN2P3, CPPM, Marseille, France","The real-time supernova search of KM3NeT","The KM3NeT Collaboration is building the ORCA and ARCA deep-sea Cherenkov neutrino detectors in the Mediterranean, aimed at the study of atmospheric and astrophysical neutrinos in the GeV-PeV range. With ~200000 PMTs distributed on the two detector sites, KM3NeT will also achieve sensitivity to low-energy neutrino bursts from galactic and near-galactic core-collapse supernovae (CCSN). The online search for CCSN neutrinos is one of the first real-time analyses implemented within the KM3NeT multimessenger program. It has been operating since the beginning of 2019 on the data streams produced by the first ARCA and ORCA detection units. This contribution outlines the technical implementation of the pipeline articulated in three main parts: (i) the online analysis, from the real-time data processing for each detector to the estimation of a joint significance as a function of time (ii) the alert generation, management and follow-up strategies (iii) the integration of the system in the SNEWS global alert network and the perspective for supporting the SNEWS2.0 science program."
"P4-127","Massimiliano Lincetto","Aix Marseille Univ, CNRS/IN2P3, CPPM, Marseille, France","Status of the real-time multi-messenger program of KM3NeT","The KM3NeT research infrastructure in the Mediterranean is a multi-purpose cubic-kilometer neutrino observatory consisting of two detectors. ORCA is optimized to study atmospheric neutrinos between 1 and 100 GeV, while ARCA is primarily aimed at detecting cosmic neutrinos between several tens of GeV and PeV. The real-time multi-messenger program of KM3NeT is oriented towards the study of astrophysical transients. It enables the bidirectional exchange of alerts for follow-up analyses as well as joint sub-threshold searches with a network of partner observatories. The prompt and public dissemination of the results to the astrophysical community is a key objective of the program. This contribution outlines the first developments of the KM3NeT multi-messenger infrastructure, covering the event reconstruction and flavour classification pipelines, the alert distribution system and the perspectives for the KM3NeT integration in the global multi-messenger networks."
"P4-149","Francesca Capel","Technische Universität München","Exploiting multi-messenger data through hierarchical modelling","The goal of multi-messenger astronomy is to bring together diverse observations into a consistent physical picture, allowing us to test predictions and deepen our understanding. This is a huge challenge, with data from very different instruments, possible explanations from a range of complex models, unknown source populations and selection effects at play. I will discuss how we can leverage Bayesian hierarchical modelling as a statistical technique to address these challenges and make full use of the hard-earned experimental data. By including more information into the statistical analysis and carefully modelling uncertainties, it is possible to gain more insightful results. To demonstrate this approach, I will draw on examples from my research into the potential source populations of ultra-high-energy cosmic rays and astrophysical neutrinos."
"P4-167","Valerio","University of Urbino ""Carlo Bo""","GWSky: an Augmented Reality Mobile App for Gravitational Waves Sky Localization","As more and more Gravitational Wave Events are detected each year via interferometry,
the need for a clear, informative, handy representation and visualization of their characteristics is becoming
ever-more important. On one hand, such a tool can be a commodity for researchers,
allowing for an effortless overlay of gravitational data on top of the
electromagnetic view of the sky; on the other, it can be an effective
instrument for educative purposes, especially in the context of mobile
devices. In this poster, an Android/iOS application developed in association with the
LIGO-Virgo-KAGRA collaboration and the University of Urbino ""Carlo Bo"" which is capable of performing
these features is presented. While it may resemble a stargazing app at a glance,
it displays the real sky as well as any other hips fetched from the Virtual Observatory,
and it is able to localize gravitational events via the MOC data structure on this sky map; it can
display descriptions, sounds and data from the gravitational events and, even
more importantly, it provides a strong base upon which new features could
be added in a forthcoming version, such as real-time updates of new events
via a simple phone notification and the display of a greater and more detailed
volume of information."
"P4-174","Aleksandra Grokhovskaya","Special Astrophysical Observatory","Optical identification of X-ray sources in the HS47.5-22 field","We present candidates for optical identification of 122 X-ray sources in the central part of the HS47.5-22 field (~2.4 sq. degrees) from a mid-depth ROSAT survey (Molthagen et al. 1997). Candidates selection was based on the observations in 16 mid-band filters (FWHM = 250 A, spectral range 4000 - 8000 A) and 4 broadband (u, g, r, i SDSS) filters at the 1-m Schmidt telescope of the Byurakan Astrophysical Observatory (Armenia). Within the boxes of positional errors of X-ray objects, from one to several candidates for optical identification up to R = 24m are detected. Using the spectral energy distributions constructed from the medium band photometry, we classified the objects and determined the photometric redshifts with an accuracy of dz = 0.05 (1 + z) for AGNs and with an accuracy of dz = 0.01 (1 + z) for galaxies. New data have been obtained for faint (CR <0.2s-1) X-ray objects not identified in the Hamburg Quasar Survey and SDSS survey."
"P4-176","Jaime Rosado","Universidad Complutense de Madrid","ShowerModel: A Python package for modeling cosmic-ray induced air showers, their light production and detection","Cosmic-ray observatories necessarily rely on Monte Carlo simulations for their design, calibration and analysis of their data. Detailed simulations are very demanding computationally. We will present a fast python-based package based on simple parameterizations to model light emission in cosmic-ray induced air showers and its detection by an array of telescopes. It covers both Cherenkov and fluorescence emission. This tool may speed up some studies that do not require a full Monte Carlo simulation.
The package includes a number of functions to generate gamma and proton-like air showers with arbitrary direction and energy in a discretized atmosphere. The telescopes are highly configurable (e.g., quantum efficiency, pointing direction, field of view, detection area, number of pixels), allowing the modeling of fluorescence telescopes, imaging air Cherenkov telescopes, wide-angle Cherenkov detectors or hybrid designs. The package can produce the time evolution of signals as well as pseudo-images produced in a telescope camera.
This package is intended to provide a tool to complement simulation studies and data analyses. It would also be useful to explore new detection techniques and higher energy ranges that are prohibitive for simulations. Examples of results produced by this software and the comparison with the simulated Cherenkov and fluorescence signal registered by telescopes will be shown."
"P4-185","Fabrizio Lucarelli","ASI Space Science Data Center (SSDC) and INAF-OAR","Hybrid SEDs for neutrino events integrated in the SEDBuilder tool of the ASI-SSDC MWL envinronment.","The quest for the electromagnetic counterparts of the TeV-PEV neutrino events observed by the current neutrino detectors is one of the hottest topic in modern astrophysics.

Public online tools integrated in a multi-wavelength environment, like the SSDC SkyExplorer and the SEDBuilder tools, are of great utility to identify the possible neutrino source candidates, allowing to restrict the search around the neutrino source position uncertainty region (in average of the order of 1 deg in radius), and to derive the electromagnetic spectral energy distribution (SED) of the source candidates.

In this work, we present the SSDC catalog of candidate astrophysical neutrinos, based on the public events announced by the IceCube detector, that has been recently included in the SkyExplorer tool,  and a new feature of the SEDBuilder tool which allow the construction of the hybrid photon–neutrino spectral energy distributions of the neutrino source candidates.

The hybrid SED, used in conjunction with the SSDC SkyExplorer tool, will help the users to identify the best neutrino source candidates according to their classification and SED characteristics."
"P4-24","Nicolò Parmiggiani","INAF OAS Bologna, Via P. Gobetti 93/3, 40129 Bologna, Italy. Università degli Studi di Modena e Reggio Emilia, DIEF - Via Pietro Vivarelli 10, 41125 Modena, Italy.","RTApipe, a framework to develop Astronomical pipelines for the real-time analysis of scientific data.","In the multi-messenger era, astrophysical projects share information about transients phenomena issuing science alerts to the Scientific Community through different communications networks. This coordination is mandatory to understand the nature of these physical phenomena. For this reason, astrophysical projects rely on real-time analysis software pipelines to identify as soon as possible transients (e.g. GRBs), and to speed up external alerts' reaction time. These pipelines can share and receive the science alerts through the Gamma-ray Coordinates Network (GCN).
This work presents a framework designed to simplify the development of real-time scientific analysis pipelines. This framework provides the architecture and the required automatisms to develop a real-time analysis pipeline, allowing the researchers to focus more on the scientific aspects. This work was motivated by developing the real-time analysis of the AGILE satellite focused on the detection of cosmic transients. 
The pipelines developed with this framework can be easily configured with new or existing science tools implemented in different programming languages. The pipelines are able to execute all the analysis automatically after the initial setup. Through the interface with the GCN network, the pipelines are able to react to external science alerts sent by other projects, starting the processing as soon as the instrument data is available. The Slurm Workload Manager is used to execute the scientific analyses in parallel, to manage the priority between different processes and scale the workload on a cluster of machines. All the necessary services are provided to the users by containerization technology. The framework has been successfully used to develop real-time pipelines for the scientific analysis of the AGILE space mission data. It is planned to reuse this framework for the Super-GRAWITA and AFISS projects. A possible future use for the Cherenkov Telescope Array (CTA) project is under evaluation."
"P4-243","HISANORI FURUSAWA","National Astronomical Observatory of Japan","Discussion towards reliable and productive astronomical data archives","Astronomical data archives are recognized as an essential infrastructure for astronomical activities. They are also indispensable records of every single moment of the universe -- a heritage for the human beings. Despite the consensus on their importance, it has been a long-lasting issue to secure human and computing resources for maintaining and improving functionality of the data archives. Recently, the situation has been getting more challenging, as each community is required to accommodate forthcoming big projects within a limited budget frame. To overcome this difficult time, it is quite important to define solid missions of the data archives and consolidate efforts with commitments towards prioritized goals.  Specifically in the Japanese community, we have started to investigate demands and optimal ways of operation for reliable and productive Optical-NIR data archives. Our objectives include to identify 1) what to archive, 2) necessary functions, and 3) roles of individual teams involved in data flow. The current discussion indicates importance of understanding achievements and strengths of the data archives, making data available to public, and defining responsibilities of both data providers and archivers. We aim at putting the results into a proposal of data management in the Japanese community. To learn situations in oversee archives, we also made an unofficial survey for several archive people. It seems that in the US and European archives, stably and successful achievements with archival data have convinced the local community, which leads to proper valuation and financial support. This poster describes the current outcomes from the ongoing discussion."
"P4-252","Alessandra Berretta","Università di Perugia and INFN Perugia","Search for Spatial and Temporal Coincidence Between LAT/Fermi Exposure Maps and GW Sky Localizations","The poster describes a practical method to search for spatial and temporal coincidence of the LAT/Fermi coverage over a gravitational-wave sky localization. The method returns the overlap region between the two sky areas within a proper time window selected by the user. This approach offers a prompt setting of the observational strategies for searching potential electromagnetic candidates  as well as a fast cross-matching between the LAT and the LIGO, Virgo and KAGRA databases for dedicated post-processing analysis.
The tasks are performed using the encoded standard method named Multi Order Coverage Map and visualized in the Aladin Desktop."
"P4-34","Carlo Burigana","INAF-Istituto di Radioastronomia","On the cosmic isotropic background from the radio to the far-IR: a new method for theoretical predictions of the frequency spectrum from monopole to higher multipoles for a moving observer","We study how the frequency spectrum of the background isotropic monopole emission is modified and transferred to higher multipoles by boosting effects due to the observer peculiar motion.
The method, based on a linear system, is suitable for various background radiation models and here applied from the radio to the far-IR to several types of cosmic microwave background (CMB) distorted photon distribution functions and extragalactic background signals superimposed to the CMB Planckian spectrum.
We derive explicit solutions for the spherical harmonic coefficients up to any desired multipole, lmax, in terms of linear combinations of the signals at just N = lmax + 1 colatitudes. For appropriate choices of these colatitudes, the symmetry property of the associated Legendre polynomials with respect to π/2 allows the separation of the system into two subsystems, one for l = 0 and even multipoles and the other for odd multipoles, and improves the solution accuracy.
The simplicity and efficiency of this method can significantly reduce the computational cost needed for accurate predictions on the whole sky and for the scientific analysis of data from future projects. Moreover, in the presence of CMB spectral distortions, this formalism, combined with the representation of CMB intrinsic anisotropies, provides a new test to constrain the intrinsic dipole embedded in the kinematic dipole."
"P4-76","Michael Lam","Rochester Institute of Technology","A Next-Generation Data Processing Framework for Pulsar-based Gravitational Wave Detectors","Pulsar timing array detectors require large numbers of observations of many pulsars for the purpose of detection and characterization of low-frequency gravitational waves. Since each pulsar-Earth line of sight is unique, we require tailored models that describe the arrival times of pulses from each pulsar. Currently, the North American Nanohertz Observatory for Gravitational Waves observes 78 pulsars with weekly-to-monthly cadence at multiple observatories, thus requiring that our pipelines for processing and building the data set be highly automated. We describe our modernized workflow for the development of new timing models and the software that will (i) allow us to more uniformly develop the data set, (ii) enable students and newcomers in the field to more easily become involved in pulsar timing analyses, and (iii) ensure stringent version control of the software and reproducibility of the data and provide visual accessibility for us and others in the community to verify our results. Our system will be publicly available, and given its modular design, our code will provide other pulsar astronomers with tools for more standardized timing-model generation that can be modified as needed."
"P5-106","Aritra Ghosh","Yale University","Studying Morphology & Quenching of Galaxies in Large Datasets using Interpretable Machine Learning Algorithms","The traditional methods of obtaining morphological classifications of galaxies are not scalable to the large data volumes expected from future data intensive surveys like Euclid, LSST and NGRST. To overcome this, we have developed a publicly available convolutional neural network (CNN) called Galaxy Morphology Network (GaMorNet) (http://gamornet.ghosharitra.com/) that classifies galaxies according to their bulge-to-total light ratio. GaMorNet does not need a large amount of real data for training and can be applied to datasets with a range of signal-to-noise ratios and spatial resolutions. We first trained GaMorNet on high-fidelity simulations of galaxies and then transfer-learned using a small amount of real data to achieve misclassification rates of ≲5%. 

We have used GaMorNet to study the quenching of star formation in ∼100,000 (z∼0) SDSS and ∼20,000 (z∼1) CANDELS galaxies using morphology-separated color-mass diagrams. Galaxy morphology adds a third interesting dimension to the color-mass space. Because galactic disks usually do not survive major mergers and elliptical galaxies typically form as a result of merger events, morphology can be used as a tracer for the merger history of a galaxy. Using the GaMorNet classifications, we find that bulge- and disk-dominated galaxies have distinct color-mass diagrams. For both datasets, disk-dominated galaxies peak in the blue cloud, across a broad range of masses, consistent with the slow exhaustion of star-forming gas. In contrast, bulge-dominated galaxies are mostly red, with much smaller numbers down toward the blue cloud, suggesting rapid quenching and fast evolution across the green valley. 

In order to make sure we understand exactly how our algorithm works, we have also used a combination of different CNN visualization techniques to investigate GaMorNet’s decision-making process, making our results interpretable, reproducible and robust."
"P5-107","Jingyi Zhang","National Astronomy Observatories, CAS","Imbalanced Learning for RR Lyrae Star Detection","As standard candles, RR Lyrae stars are taken as distance markers and age indicators，which is helpful to estimate the distance and age of globular clusters as well as galactic and extragalactic (local group) locations. At the same time, as tracers, they reveal the structure, chemical and dynamic evolution of the galaxy, as well as the substructure of the halo. Compared with other types of stars, RR Lyrae stars are still not enough in number. Therefore, separating RR Lyrae stars from stars belongs to an imbalanced learning problem. It may be solved by popular imbalanced learning methods, which include the distribution change of different samples, improvement of the traditional algorithms and outlier detection methods. Based on large survey databases, we apply imbalanced learning methods (cost-sensitive random forest, cost-sensitive SVM, Fast Boxes) to select RR Lyrae stars candidates. The results showed that imbalanced learning methods are effective for distinguishing RR Lyrae stars from other stars."
"P5-116","Giorgio Calderone","INAF - Osservatorio Astronomico di Trieste","Finding new QSOs in photometric catalogs using machine learning: the QUBRICS survey.","The several photometric surveys currently available produced a huge amount of data stored in their catalogs.  Besides the primary scientific goals pursued by the individual experiments, there probably is a lot of information hidden in those databases, still waiting to be extracted.
This is certainly the case for the relatively scarce population of high-z QSOs in the Southern Hemisphere known today.

The QUBRICS survey (QUasars as BRIght beacons for Cosmology in theSouthern hemisphere) aims to identify these sources on currently available photometric catalogs using a machine learning approach, and confirm their QSO nature with dedicated spectroscopic observations.  More than 200 new, bright (i<18) and high-z (z>2.5) QSOs have already been identified by QUBRICS.

In this talk we will review the performances of the machine learning techniques and the current status of the survey."
"P5-140","Alcione Mora","Aurora Technology BV for ESA","The Gaia Archive Users","Gaia data have been used in hundreds of papers, making them one of the most popular data sets, both at ESA and worldwide. The ESA Gaia Archive is a complex infrastructure composed of multiple databases and accessible via Virtual Observatory protocols (see Gonzalez-Nuñez et al, this conference).

It has been devised and developed through the course of nearly a decade. It will evolve to accommodate the large data volumes of the latest releases, in the scale of the Petabyte. User feedback is essential, but sometimes difficult to incorporate in major architecture decisions (front- and back-end, VO, API, code to the data, ...). This is a common feature of many archives, where the popularity of a given service or data set is a-priori unknown.

One possible way forward is asking the users directly, which has been carried out through different surveys. A complementary approach, and the core of this talk, is to analyse the users activity. The Archive traffic monitor statistics and logs have been inspected to look for patterns. For example: GUI vs programmatic access, manual vs computer generated queries, single table vs multiple joins, data volume vs execution time, casual vs power users, etc.

Many archives have a similar architecture or aims (the most immediate example are the Gaia partner data centres), which makes these results relevant for the broad Astronomical community."
"P5-163","Sébastien Fabbro","NRC Herzberg","Complementary Theory and Data Driven Analysis for Sky Surveys","Computationally intensive simulations make systematic uncertainties hard to quantify when compared to real data. Guided by unsupervised domain adpation in machine learning, we show how we build an end-to-end differentiable pipeline that is capable of accelerating and correcting simulations by learning from large survey data sets. As an application to astrophysics, we build an emulator to stellar spectra simulations, which we complement with an unsupervised generative network learned from spectroscopic survey data.  The machine learning pipeline is then used for accurate stellar parameters estimation and to show the potential of the method to discover new spectral features associated to elemental abundances."
"P5-164","Griffin Hosseinzadeh","Center for Astrophysics | Harvard & Smithsonian","Photometric Classification of 2315 Pan-STARRS1 Supernovae with Superphot","The classification of supernovae (SNe) and its impact on our understanding of the explosion physics and progenitors have traditionally been based on the presence or absence of certain spectral features. However, current and upcoming wide-field time-domain surveys have increased the transient discovery rate far beyond our capacity to obtain even a single spectrum of each new event. We must therefore rely heavily on photometric classification—connecting SN light curves back to their spectroscopically defined classes. Here we present Superphot, an open-source Python implementation of the machine-learning classification algorithm of Villar et al. (2019), and apply it to 2315 previously unclassified transients from the Pan-STARRS1 Medium Deep Survey for which we obtained spectroscopic host galaxy redshifts. Our classifier achieves an overall accuracy of 82%, with completenesses and purities of >80% for the best classes (SNe Ia and superluminous SNe). For the worst performing SN class (SNe Ibc), the completeness and purity fall to 37% and 21%, respectively. Our classifier provides 1257 newly classified SNe Ia, 521 SNe II, 298 SNe Ibc, 181 SNe IIn, and 58 SLSNe. These are among the largest uniformly observed samples of SNe available in the literature and will enable a wide range of statistical studies of each class."
"P5-168","Jesús Vega Ferrero","IFCA / IIIA","Athena/X-IFU pulse identification using Convolutional Neural Networks","The Athena X-ray Observatory, to be launched in the early 2030s, is designed to implement the Hot and Energetic Universe science theme. The X-ray Integral Field Unit (X-IFU) proposed for the Athena mission is a cryogenic imaging spectrometer, offering spatially-resolved high-spectral resolution X-ray spectroscopy over a 5 arcminute equivalent diameter field of view. In this project, we aim to detect and reconstruct the X-ray pulses that will be detected by the X-IFU instrument using Deep Learning techniques over a dataset of simulations with SIXTE (a software package for X-ray telescope observation simulations). We construct and train a Convolution Neural Network (CNN) to differentiate between single, double and triple pulses (i.e., detections with one, two or three pulses in the same record, respectively). We use a hyper-parameter bayesian optimization to select the best CNN architecture. Finally, we present the results of our CNN classification on a test set of simulated pulses, covering all the possible cases in terms of energy ranges and pulse distances, achieving excellent performance metrics."
"P5-173","Brian Kent","NRAO","Machine Learning using the Nvidia Jetson Nano","The Nvidia Jetson Nano is a small embedded compute board with
four CPU cores and a 128-core Maxwell generation GPU.
We utilize the PyTorch framework to train a convolutional neural network image classifier
using a small imaging subset of the VLA Sky Survey that is split
into both testing and validation collections.  We contruct a simple shape classification
scheme and report on the loss function for the network that can be run on the Nano hardware."
"P5-177","André Rodrigo da Silva","Centrum Astronomiczne im. Mikołaja Kopernika - Polskiej Akademii Nauk","Chemo-kinematic analysis of metal-poor stars with unsupervised machine learning","Metal-poor stars play an import role on the understanding of near-field cosmology. Old and long-lived late-type stars provide important clues to the early galaxy chemical evolution. In particular, these stars may help to clarify one of the open problems in nucleosynthetic theory:the \textit{locus} of the r-process. In this work, we report on preliminary results of a chemo-kinematic analysis of a sample of metal-poor stars observed by the GALAH spectroscopic survey. GALAH provided chemical abundances and radial velocities for 340\,000 stars. We cross-matched this sample with the \textit{Gaia} DR2 catalog, which includes accurate and precise measurements of proper motions and parallaxes for more than 10$^9$ stars.Our final sample of objects with metallicities [Fe/H] $\leq$ $-$1.0 dex has 1\,146 stars. From these, 1\,072 stars have radial velocities determined by the GALAH survey. With this selection, we used \textit{galpy} to integrate stellar orbits adopting a Milky Way gravitational potential and derive parameters such as orbital eccentricity, farthest distance reached from the plane, action angles and angular momentum. We explored the chemical and orbital data with unsupervised machine learning (Hierarchical clustering, k-means cluster analysis and correlation matrices). Our final goal is to find an optimal way to separate different Galactic stellar populations and stellar groups originating from merging events, such as Gaia Enceladus and Sequoia."
"P5-183","Tracy Chen","Caltech/IPAC","Classification of Astrophysics Journal Articles with Machine Learning","The NASA/IPAC Extragalactic Database (NED) routinely reviews journal articles to extract fundamental data of extragalactic objects from the articles and join them across the spectrum into the database. The work of manually going through the journal articles, identifying if one is appropriate for inclusion in NED, and what kind of data are in the articles, is very labor intensive, especially given the ever-increasing numbers of publications each year. We present here a machine learning approach developed recently to help with the classifications of journal articles topics and content. We show that the application of this machine learning approach can reproduce the hand-classifications to an accuracy of over 90%."
"P5-187","Mauricio Rodríguez Alas","ULatina Costa Rica","VLASS Quicklook Image Classifier using PyTorch","VLASS is expected to catalog approximately 10 million radio sources, generating corresponding quicklook images during its lifetime operation. An efficient and automated way to classify these images needs to be developed. Training an Image Classifier using PyTorch to distinguish between common radio sources classifications e.g. Fanaroff-Riley or a simplified classification scheme.
A Neural Network model with PyTorch was developed, and began initial model training. The model is able to classify the quicklook images into 3 categories: single, multiple, and diffuse sources. A classification tool like this is relevant to the astronomy community in general, to be able to access specific classifications regarding radio sources.

Keywords: Radio Astronomy, Machine Learning, Image Classification, Radio Sources, VLASS, PyTorch."
"P5-19","Anita Petzler","Macquarie University","Bayesian Gaussian decomposition of multi-wavelength molecular spectra with Amoeba","The molecular interstellar medium (ISM) is often studied via observations of the four ground-rotational state transitions of the hydroxyl (OH) molecule (e.g. Dawson et al. 2014, Rugel et al. 2018, Nguyen et al. 2018, Engelke et al. 2019). The decomposition of these spectra into individual Gaussian components -- each corresponding to an isothermal ISM cloud along the line-of-sight -- is an essential but difficult task. Amoeba is an automated program (written in python) that takes a Bayesian approach to the simultaneous decomposition of observations of the four ground-rotational state transitions of OH. In this talk I will outline Amoeba, paying particular attention to the considerations that were required to make it fully automated."
"P5-213","Kazumi Murata","National Astronomical Observatory Japan","Fitting of optical galaxy spectra based on machine learning","This work presents a spectral fitting method based on machine learning.
Galaxy spectra at optical wavelength contain much information of the physical properties such as star-formation rate, stellar mass, metallicity and their history.
Conventional methods of spectral fitting are based on the least chi-square,  Bayesian statistics, etc.
However, these methods require extremely long time and may not be appropriate for a future large spectroscopic survey.
In contrast, a machine-learning based method enables us to perform a spectral fitting much faster than conventional methods. 

Our work investigates the feasibility of this method.
We constructed a neural network to decompose the input spectra into single stellar population (SSP) spectra.
We produced training and test data set using an SSP library (CB07) with various ages, velocity dispersions and dust extinction. 
A 1.0 % Gaussian noise is added both to the training and test spectra in this study. 
We found that our neural network accurately reproduces both the output physical parameters and spectra with  extremely short time, that is, less than one second for 10000 spectra.
This result indicates that the spectral fitting based on the machine-learning is appropriate for a future large spectroscopic survey."
"P5-215","Viktor Borisov","Lomonosov Moscow State University Faculty of Computational Mathematics and Cybernetics; Space Research Institute of the Russian Academy of Sciences","Probabilistic photo-z machine learning models for X-ray sky surveys","Accurate photo-z measurements for X-ray sources are important to construct a large-scale structure map of the Universe in the SRG/eRosita all-sky survey. We investigate machine learning models based on Random Forests to measure probabilistic photo-z of point X-ray sources. We use information about optical counterparts of X-ray sources from 4 photometric surveys (SDSS, PanSTARRS1, DESI Legacy Imaging Survey, and WISE), take into account Galactic extinction and uncertainties in photometric measurements, and apply the active learning approach for additional training of the model. The final training sample contains ~620000 objects from the SDSS spectral catalog (quasars and galaxies possibly associated with X-ray emission). We test our photo-z models on Stripe82X and XMM-XXL-N samples."
"P5-217","Ondřej Podsztavek","Faculty of Information Technology, Czech Technical University in Prague","Transfer Learning in Large Spectroscopic Surveys","Current multimillion spectroscopic surveys of SDSS and LAMOST provide enough data for analysis with the most advanced machine learning methods as are convolutional neural networks (ConvNets). However, each survey has a different strategy of target selection, based on individual scientific goals. Diverse target selection results in different statistical distributions of each class of celestial objects in the archive. Therefore, it is not straightforward to reuse a machine-learning algorithm trained on one survey to another due to statistical properties. One of the possible solutions is transfer learning.

Transfer learning is a method which reuses previously gained knowledge in learning a new problem. We present an application of transfer learning in the context of deep learning on the classification of quasars (QSOs), trying to identify unknown QSOs in SDSS archive using the ConvNet model trained on spectra from LAMOST and finally retrained on SDSS spectra. 

We used LAMOST DR5 v3 archive (more than 9 million spectra) and the LAMOST catalogue of all QSOs to discover QSOs unknown to SDSS DR14 QSO Catalog (DR14Q) which corresponds to SDSS DR14 (more than 4 million spectra). For the classification, we employed VGG Net-A ConvNet adapted to one-dimensional spectra.

Applying the ConvNet based on parameters transferred from LAMOST training to our test set of 100 thousand spectra randomly drawn from SDSS resulted in 84330 predicted not-QSOs, 14277 correctly predicted QSOs and 170 missed QSOs. There were also 1223 spectra predicted as QSOs but not identified so in SDSS DR14Q. Our inspection has confirmed a number of the 1223 spectra to be QSOs unknown to SDSS DR14Q. Extrapolating the inspection to 49729 false positives spectra from the whole SDSS DR14 suggest that our ConvNet can discover QSOs not listed in SDSS DR14Q. We further discuss our results and give examples of discoveries."
"P5-223","Carlos Feinstein","Facultad de Ciencias Astronomicas y Geofisica/IALP - UNLP/Conicet - La Plata, Argentina","Stellar Formation in NGC 2366: Searching for clusters and associations using unsupervised algorithms","We investigated the characteristics of the young star population in the spiral galaxy NGC 2366. We used unsupervised algorithms as Path Linkage Criterion (PLC), and density based clustering (HDBSCAN). 
In particular, we focused our attention in the hierarchical clustering distribution of the young population and the properties of its star groups. As this galaxy shows a prominent He blue sequence, we also analysed this population and used it to understand the spatial distribution of the stellar formation over time. 
Direct images of the galaxy were obtained in two photometric bands by the Hubble Space Telescope (HST). They covered almost the central and intermediate zones of NGC 2366, including all the mayor stellar formation regions. HST data allowed to select the blue and young stars and therefore to study the young population. Then, through the PLC and HDBSCAN, we found the young star groups and estimated their fundamental individual parameters, such as their stellar densities, sizes, number of members, and luminosity function slopes. We also performed a fractal analysis to determine the clustering properties of this population. We built a stellar density map corresponding to the galactic young population to detect large structures and depict their main characteristics."
"P5-225","Amita Muralikrishna","National Institute of Space Research (INPE), Brazil AND Federal Institute of Sao Paulo (IFSP)","A solar spectral irradiance prediction workflow using a recurrent neural network in a reproducible and replicable approach","The solar irradiance measured at the top of the Earth’s atmosphere is considered an influential parameter in studies of the atmosphere layers’ properties and the consequences of the disturbances they suffer from the influence of solar activity.  Studies about weather and climate on Earth recognized the great influence of solar irradiation for the creation of climate models, so the prediction of solar irradiance can be considered a service of great importance in this context. An irradiance prediction system would also be useful for the reconstruction of the measurement history (time series) of different instruments that do not cover the intended period data, due to reasons like mismatches in calibration or failures on the instruments. This work uses recurrent neural network to predict solar spectral irradiance in different wavelengths. In the experiments,  one and a half year’ daily records collected by the SDO (Solar Dynamics Observatory) mission are used. Two types of images of the solar photosphere, collected by the HMI (Helioseismic and Magnetic Imager) equipment, which highlight active regions and sunspots, are processed and used as the network input. For the neural network output, spectral irradiance data, collected by the EVE equipment, at two different lines of helium, at 30.5nm and 48.5 nm, and a line of hydrogen, the lyman alpha at 121.6 nm, are used. In addition to the forecasting task, this work is also an early step in the adoption of a modular workflow proposal that allows its fully validation, reuse and replication."
"P5-242","Travis Stenborg","University of Sydney","21st Century Supermoon Estimation in R","Supermoon estimation is prediction of the timing and distance of a perigee full moon. Some existing relevant tools can be opaque (lacking implementation details), functionally limited (giving a subset of the required information) or give mutually inconsistent results. A new supermoon estimator has been implemented to address drawbacks in, and to help benchmark, such existing tools. Specifically, the case of the 21st century supermoons is handled. Implementation in R is discussed, including numerical precision caveats, first-class function leveraging, general optimisations (e.g. Horner's method) and time scale transformations."
"P5-245","Tomasz Różański","Astronomical Institute of Wrocław University","Neural network for stellar spectrum normalization","We present a deep fully convolutional neural network trained
in the task of stellar spectrum normalization. We show that the proposed
model is able to fit spectral continuum, including non-smooth
instrumental pseudo-continuum, wide hydrogen and narrow
blended spectral lines. Proposed solution gives an opportunity to
automate this step of stellar spectrum pre-processing and achieve the
accuracy similar to that of careful manual normalization. This approach
may greatly simplify automated high-resolution spectra analysis."
"P5-256","Alex Meshcheryakov","Space Research Institute (IKI)","SRGz: machine learning engine for cross-match, classification and photo-z measurements in the SRG/eROSITA sky survey","SRGz system, developed by the Russian eROSITA Consortium, contains a set of software components needed to perform automatic cross-identification of X-ray sources (detected by eROSITA in the Eastern Galactic Hemisphere) in the large Optical/IR photometric surveys, classification of optical counterparts (according to QUSAR/GALAXY/STAR division) and photometric redshift (photo-z) measurements for extragalactic objects. Here I describe operating principles of the SRGz system and algorithms implemented in it."
"P5-260","Aidan Berres","University of Washington","Predicting Star Formation Rates Using SDSS Data","With the latest public data release from the SDSS MaNGA program, we are attempting to find a possible correlation between the fluxes from SDSS imaging of optical bands from galaxies to their Star Formation Rates (SFR). We aggregated our data from both SDSS and WISE survey databases. This full dataset contained over 7000 galaxies. With this data, we could calculate the H-alpha luminosities, distance, and SFR. We then used WISE data to correct for the effects of dust obscuration, similar to Kennicutt et al. 2009. We are in the process of using the SDSS flux data and calculated SFR to train a machine learning model so we can apply this correlation to a much larger catalog. When this correlation is found and applied, one would only need the SDSS band fluxes to predict the SFR for a galaxy."
"P5-262","Humberto Farias","UTFSM/ULS","Tensor Mask R-CNN: A Tensor-based Deep Learning approach for fast morphological classification and segmentation of Galaxies.","Modern astronomy has evolved into a highly computational scenario, as a result of the volume and complexity of the data that must be processed and analyzed. This has motivated the application of advanced computer vision techniques such as Deep Learning, which have shown outstanding results in several applications. However, the implementation of these models usually lack a key factor that impedes the incorporation of such models into complex scientific problems that demand large processing times: consideration of speed of inference as a constraint of the problem. To solve this problem, several authors have presented proposals that incorporate tensor layers into the architecture of CNN networks, showing satisfactory results. Following this rationale, we propose Tensor Mask R-CNN, a combination of a state-of-the-art regional convolutional neural network and tensor methods. This novel approach is based on the incorporation of Tensor Layers (T-L), that is, applying tensor decomposition over fully connected layers in the backbone of a Mask R-CNN network. These tensor layers provide more powerful and cheaper parameterizations for fully connected layers. This results in compressed parameter spaces that lead to a reduction in computational costs and, consequently, a reduction in inference time. As a test of our proposal, we apply Tensor Mask R-CNN to the classification of galaxies based on their morphology, where we validate the increase in inference speed, and reduced costs therefore, against the same architecture without tensor layers."
"P5-48","Daniel Nieto Castaño","Universidad Complutense de Madrid - IPARCOS","Reconstruction of stereoscopic IACT events using deep learning techniques with CTLearn","Arrays of imaging atmospheric Cherenkov telescopes (IACT)
are superb instruments to probe the very-high-energy gamma-ray
sky. This type of telescope focuses the Cherenkov light emitted from
air showers, initiated by very-high-energy gamma rays and cosmic rays,
onto the camera plane. Then, a fast camera digitizes the longitudinal
development of the air shower, recording its spatial, temporal, and
calorimetric information. A given air shower is typically observed by
more than one telescope in the array, thus providing stereoscopic
images for each event. The properties of the primary very-high-energy
particle initiating the air shower can then be inferred from those
images: the primary particle can be classified as a gamma ray or a
cosmic ray and its energy and incoming direction can be estimated.
This so-called full-event reconstruction, crucial to the sensitivity
of the array to gamma rays, can be assisted by machine learning
techniques. We present a deep-learning driven, full-event
reconstruction applied to simulated, stereoscopic IACT events using
CTLearn. CTLearn is a Python package that includes modules for loading
and manipulating IACT data and for running deep learning models with
TensorFlow, using pixel-wise camera data as input."
"P5-51","Bernardino Spisso","I.N.F.N.","Event analysis in KM3NeT using machine learning","The KM3NeT neutrino telescopes are already taking data while undergoing incremental construction in two locations in the Mediterranean Sea. KM3NeT/ARCA is a large-scale water Cherenkov detector located south-east off the Sicily coast, optimised for investigating astrophysical high-energy neutrino sources in the universe using on the order of a gigaton of seawater monitored by photo-sensors. KM3NeT/ORCA is the low-energy detector of KM3NeT, located off the French coast and sharing the same technology with a smaller and denser network of photo-sensors. The main goal of KM3NeT/ORCA is the determination of the neutrino mass ordering. 
This talk aims at demonstrating the general applicability of convolutional neural networks (CNNs) in the reconstruction and data analysis of neutrino telescopes, using simulated datasets for the KM3NeT/ARCA detector as training data. For this purpose, a Keras-based framework called OrcaNet has been used, originally developed for reconstruction and classification in KM3NeT/ORCA. In this work, CNNs are employed to accomplish reconstruction as well as classification tasks for neutrino events in KM3NeT/ARCA, promising complementary information to the very time-consuming analysis pipeline based on maximum-likelihood methods. Some CNN models will be described, which have proved to provide good performance in event reconstruction e.g. for the estimation of the energy and the direction of the incoming neutrino and event-shape classification (shower-like or track-like). For a thorough overview, a short report will also be given of the tasks and performances of CNNs in ORCA."
"P5-54","Hossen Teimoorinia","Canadian Astronomy Data Centre (CADC), Herzberg Astronomy and Astrophysics, NRC","An astronomical image content-based recommendation system using combined deep learning models in a fully unsupervised mode","We have developed a method that can cluster and map large astronomical images onto a two-dimensional map. A combination of various state of the art machine learning algorithms is used to develop a fully unsupervised image quality assessment and clustering system. Our pipeline consists of a data pre-processing step where individual image objects are identified in a large astronomical image and converted to smaller pixel images. This data is then consumed by a deep convolutional autoencoder jointly trained with a self-organizing map. The resulting latent data is further compressed using a second autoencoder and eventually mapped onto a two-dimensional grid using a second self-organizing map. We used data taken from ground-based telescopes and, as a case study, compared the system’s ability and performance with the results obtained by supervised methods presented by Teimoorinia et al. (2020). The availability of target labels in a subset of this data allowed a comprehensive performance comparison between our unsupervised and supervised methods. We investigated the accuracy, precision and recall and observed that our unsupervised method outperforms the supervised method in some aspects. In addition to image quality assessments performed in this project, our method can have various other applications. As an example, it can help experts label images in a considerably shorter time with minimum human intervention. It can also be used as a content-based recommendation system capable of filtering images based on the desired content."
"P5-58","Landman Bester","South African Radio Astronomy Observatory","A practical pre-conditioner for wide-field continuum imaging of radio interferometric data:","The celebrated CLEAN algorithm has been the cornerstone of deconvolution algorithms in radio interferometry almost since its conception in the 1970s. For all its faults, CLEAN is remarkable in two regards viz. speed and its ability to accurately model point sources. In this talk, we demonstrate how the same assumptions that afford CLEAN its speed can be used to accelerate more sophisticated deconvolution algorithms. In particular, we approximate the Hessian of the likelihood function as a convolution with the point spread function and use this approximation to develop an effective preconditioner for a proximal gradient based imaging algorithm. The resulting algorithm, dubbed pre-conditioned forward-backward clean (pfb-clean), is implemented using forward-backward iterations and is particularly suited to imaging radio interferometric data in the regime where the data size is much larger than that of the image."
"P5-61","Amandin Chyba Rabeendran","Institute for Astronomy at the University of Hawaii at Manoa, Colorado School of Mines","A Two-Stage Deep Learning Detection Classifier for the ATLAS Asteroid Survey","In this work we present a two-step neural network model to separate detections of solar system objects from optical and electronic artifacts in data obtained with the ""Asteroid Terrestrial-impact Last Alert System (ATLAS), a near-Earth asteroid sky survey system. A convolutional neural network is used to classify small ""postage-stamp"" images of candidate detections of astronomical sources into eight classes, followed by a multi-layered perceptron that provides a probability that a temporal sequence of four candidate detections represents a real astronomical source. The goal of this work is to reduce the time delay between Near-Earth Object (NEO) detections and submission to the Minor Planet Center. Due to the rare and hazardous nature of NEOs, a low false negative rate is a priority for the model. We show that the model reaches 99.7% accuracy on an average night of ATLAS detections with a 0.09% false negative rate. Deployment of this model on ATLAS has reduced the amount of NEO candidates that astronomers must screen by 89.1%, thereby bringing ATLAS one step closer to full autonomy."
"P5-62","Mikaël Jacquemont","LAPP, CNRS / LISTIC, University Savoie Mont Blanc","Single Imaging Atmospheric Cherenkov Telescope full-event reconstruction with a deep multi-task learning architecture","Imaging Atmospheric Cherenkov Telescopes (IACT) detect the Cherenkov light induced by particle showers generated by cosmic rays and gamma rays entering the atmosphere. A complex data analysis is then required to reconstruct the direction, energy and type of the incoming particle from the telescope images.
Since the 2012 ImageNet breakthrough, deep learning advances have shown dramatic improvements in data analysis across a variety of fields. Convolutional neural networks look particularly suited to the task of analysing IACT camera images for event reconstruction as they provide a way to reconstruct the interesting physical parameters directly from calibrated images, skipping the pre-processing steps of standard methods, such as image cleaning and image parametrization. Moreover, despite demanding substancial computing resources to be trained and optimised, neural networks show very good performance during execution. Such a performance proposes neural networks as online analysis tools for the Cherenkov Telescope Array (CTA), the  future  generation of IACT that will be one order of magnitude more sensitive than the current generation of experiments.
Here we present a complete reconstruction of IACT events using state-of-the-art deep learning techniques. The network is then applied in the single telescope context of the LST1, the first CTA telescope prototype built on La Palma's site. We show that the full event reconstruction is possible with a single multi-task network, improving the reconstruction performance by reducing the degeneracy introduced by the monoscopic atmospheric detection. In addition, using a single model reduces the computing needs. The obtained reconstruction performance will be shown and compared to other reconstruction methods."
"P5-64","Guido Cupani","INAF–Astronomical Observatory of Trieste","Astrocook: quasar spectral analysis made easy","Astrocook is a software environment to analyze quasar spectra in full depth, providing “a thousand recipes to cook your data"". It combines (1) a set of novel algorithms to model spectral features in emission and absorption (continuum component, spectral lines, complex systems); (2) a full-fledged graphical user interface to perform the analysis interactively; and (3) a scripting utility to combine recipes into complex workflow and automatize the procedure. Since its inception, the project has developed into a comprehensive suite of tools, compensating for the current lack of similar resources, while keeping flexible in incorporating different packages under the same environment.

The purpose of this talk is (1) to present the 1.0 version of the code in operation and (2) to foster contributions from the community, discussing some possible future developments."
"P5-80","Suchetha Cooray","Nagoya University","An Iterative Reconstruction Algorithm for Faraday Tomography","Faraday tomography provides pivotal information on the magnetoionic media along the line of sight through the Faraday dispersion function (FDF). Observing the magnetoionic media oﬀer insight on to magnetized astronomical objects, such as quasars, galaxies or intergalactic medium in galaxy clusters. The FDF can be obtained by the inverse Fourier transform of the observed linear polarization spectrum. However, the transform gives an insuﬃcient reconstruction of the FDF because the instrument limits the observable wavelength coverage. Up to now, the inability to solve the above inverse problem reliably has noticeably plagued cosmic magnetism studies. Inspired by the well-studied area of signal restoration, we propose the Constraining and Restoring iterative Algorithm for Faraday Tomography (CRAFT). The iterative technique is computationally inexpensive and only initially requires weak physicallymotivated assumptions to produce high ﬁdelity FDF reconstructions. We demonstrate the reconstruction algorithm for a realistic synthetic model FDF of the Milky Way, and the results show that our technique considerably outperforms other popular reconstruction techniques. The spectral dependence on the diﬀerent methods is also demonstrated with a simpler FDF. The proposed approach will be critical for future cosmic magnetism studies, especially with broadband polarization data from the Square Kilometre Array and its precursors."
"P5-83","Shabbir Nuruddin Bawaji","ThoughtWorks","Exploring Coronal Heating Using Unsupervised Machine-Learning","The perplexing mystery of how the solar corona maintains itself at a temperature of million K, while the visible disc of the Sun is only at 5800 K has been a long standing problem in solar physics. A recent study by Mondal et al. (2020, ApJ, 895, L39) has provided the first evidence for the presence of ubiquitous impulsive emissions at low radio frequencies from the quiet sun regions. Based on their observed characteristics, the authors find that these emissions, which we refer to as events, meet all of the requirements for being important for coronal heating. From a solar physics perspective, it is very interesting to understand their detailed morphological characteristics. This forms the objective of our study. To put the complexity of the problem in context, we note that in terms of their strength, the weaker features to be studied are about two orders of magnitude weaker than the weakest features reported earlier and are only a few percent of the steady solar emission; in terms of numbers, they occur at a rate of about five hundred events per minute. Based on earlier work with stronger flux densities and theoretical considerations, these features are expected to be compact in the image plane. To characterise the spatial structure of these events, we construct a peak fitting algorithm to find intensity peaks on the Sun, and fit Gaussian or quasi-Gaussian distributions to them. Density-Based Spatial Clustering of Application with Noise (DBSCAN), an unsupervised machine learning algorithm is used to classify the peaks as isolated or clustered. It is also used to obtain robust fits to these peaks, by rejecting noise features, or those that might be contaminated by artefacts from a bright active region present on the Sun. The final objective is to represent the information in the image plane as a set of features that are well-fit with Gaussians. The characteristics of these features can then be further examined to draw conclusions about solar coronal processes. To do a robust statistical analysis, we have applied this tool to a 70 minute dataset with images at every 0.5 seconds at 132 MHz. We present here the preliminary results from our work."
"P5-87","Akio Taniguchi","Nagoya University","Data scientific approaches to efficient sky noise removal in submillimeter single-dish spectroscopy","Removing sky emission from observed data is essential to detect astronomical signals for submillimeter spectroscopy with ground-based single-dish telescopes. The sky emission is intense and dominated by low time-frequency noises. In order to remove it, conventional observing methods based on sky position switch (PSW) alternately obtain spectra at on-source (the sky emission and astronomical signals exist) and off-source (the sky emission only exists) positions. They, however, need to subtract noisy spectra from each other, and results in not only worsening the sensitivity by a factor of √2 but also causing baseline instability in a spectrum, making it difficult to detect faint emission lines such as high-redshift galaxies.

In this talk, we present statistical approaches to remove the sky emission without causing the issues above. By obtaining time-series spectra at a high (>Hz) sampling rate as a data matrix, the sky emission can be instantaneously modeled by low-rank approximation. Meanwhile astronomical signals should be modulated spectrally or spatially so as not to be modeled as a low-rank component. We develop the frequency modulation (FM) method for heterodyne receivers (Taniguchi et al. 2020), and demonstrate that it dramatically improves the sensitivity compared to the PSW observations for point sources by a factor of 1.74. The greater improvement than √2 is because the FM method does not need off-source measurements and thus also improves the on-source time fraction. We also demonstrate that the sensitivity of PSW observations themselves is also improved by a factor of √2 if one could obtain time-series spectra during an observation (Taniguchi et al. in prep). These statistical approaches would be useful for deep spectroscopy driven by future 50-m-class large submillimeter single-dish telescopes in which mechanical PSW becomes difficult."
"P5-88","Vladimir Dergachev","AEI Hannover","Analyzing data from unknown statistical distribution.","Hunting for rare events in large datasets often brings up a lot of spurious noise. Such noise is typically generated by exceptional events happening during data collection and its statistical distribution is often unknown. We present a simple and efficient algorithm that can rigorously establish upper limits for data with unknown distribution. Interestingly, this algorithm can be represented as a small neural network."
"P6-110","Pablo Peñil","University Complutense","Systematic search for gamma-ray periodicity in active galactic nuclei detected by the Fermi-Large Area Telescope","Blazars can show variability on a wide range of timescales, however, whether this variability displays a specific pattern or not is still an open issue. In this context, the search for periodicity in the gamma-ray emission from blazars is an on-going challenge. In this talk, we present the results from Peñil et al. (2020), where we use the first nine years of gamma-ray data collected by the Fermi Large Area Telescope. We apply several time-series methods to pinpoint potential temporal patterns in the light curves of almost two thousand blazars included in the Fermi-LAT catalogs. Our analysis finds 11 AGN with significant evidence of periodicity, of which 9 are identified for the first time. The discovery of periodic emission in blazars can provide crucial information about the inner regions of the accretion disk, the structure of the jet, and potentially unveil the presence of binary supermassive black holes."
"P6-125","Fabian Jankowski","Jodrell Bank Centre for Astrophysics, The University of Manchester","Real-time triggering capabilities for Fast Radio Bursts at the MeerKAT telescope","Fast Radio Bursts (FRBs) are enigmatic radio pulses of roughly millisecond duration that come from extragalactic distances. We use the MeerKAT telescope array in the Karoo desert in South Africa to search for and localise those bursts to high precision in real-time. Our overall aims are to localise FRBs to their host galaxies and, thereby, to understand how they are created. However, the transient nature of FRBs presents various challenges, e.g. in system design, raw compute power and real-time communication, where the real-time requirements are reasonably strict. Namely, our GPU-based software instrument running on the MeerTRAP supercomputer must search the data streams, candidate events need to be clustered and classified, and genuine FRB detections must be communicated to other systems within only a few seconds. These requirements are essential to allow us to retain high-resolution data of the bursts, to localise them to high precision, and to minimise the delay for follow-up observations.

In my talk, I will give a brief overview of the MeerTRAP search instrument and will then focus on how we have implemented real-time triggering capabilities. For that, we utilise standard VOEvents for internal high-resolution voltage data dumps, as well as optical data retention by the co-pointed MeerLICHT telescope. I will touch on how we aim to update FRB localisation parameters, how the instrument could be triggered by external events that are in the field of view of the telescope and how our software ties into external services, such as the IAU transient name server."
"P6-152","Susan E. Mullally","Space Telescope Science Institute","The Time-series Integrated Knowledge Engine","The TIKE is a new service being offered by STScI to support astronomers working with the time-series data archived at MAST, such as TESS, Kepler and K2. This tool is built on the Pangeo deployment of JupyterHub, using Kubernetes in AWS. TIKE is a platform where astronomers can make use of data science utilities, astronomy software, and community software packages to retrieve and analyze data sets without having to download the data to their machines or maintain their own set of python packages. It is loaded with Jupyter notebook tutorials to teach time-series data analysis and visualizations.  It will also instruct users on how to make use of all the resources in the computational environment, such as how to spin up more processors, or how to access cloud-hosted data.  The TIKE will be available to all MAST time-series users in the near future and will be actively soliciting feedback for ways to improve this collaborative environment for astronomical research."
"P6-158","David Shupe","Caltech/IPAC","Working with large catalogs using the Astronomy eXtensions for Spark (AXS) framework","The Astronomy eXtensions for Spark (AXS) framework has been developed by a team led by the University of Washington's DIRAC Institute. AXS enables efficient cross-matching of large catalogs, and allows astronomers to make queries and run arbitrary processing using the Apache Spark big-data engine. As part of a collaboration with UW-DIRAC, a group at the NASA/IPAC Infrared Science Archive (IRSA) has been running AXS on our internal cluster to gain operational experience with this framework, with an eye towards possible future deployment in an archival science platform or as a service. This talk will cover our AXS processing of the 114-billion-row NEOWISE-R Single Exposure (L1b) Source Table into a prototype catalog of light curves stored in Parquet format, and how these files can be accessed outside the AXS framework using popular Python packages such as Pandas and Dask."
"P6-194","Radek Poleski","Astronomical Observatory, University of Warsaw","Microlensing Modeling Code - MulensModel","Fitting gravitational microlensing models to the data is a challenging task and will be more challenging when the upcoming NASA flagship mission - Nancy Grace Roman Telescope (formerly WFIRST) - begins its observing program. We present MulensModel package, which aims at allowing newcomers to conduct microlensing research. MulensModel allows several higher-order effects to be modeled (like binary-lens events with source limb-darkening). MulensModel is written in an object-oriented Python 3 and is already able to model simulated Roman Telescope data. Full documentation of the code is provided and the code is continuously developed."
"P6-218","Jakob van Santen","DESY","Transient processing and analysis with AMPEL","AMPEL is a software framework for analysis of data streams. It was developed for multi-messenger astronomy and new, high-throughput, wide-field surveys like ZTF that require flexible tools for the selection and analysis of astrophysical transients. A key component is its modular design, where user-contributed software modules can be added to the platform in order to filter, analyze, update, rank, or react based on transient properties. The framework encourages provenance and keeps track of the varying information states that a transient displays. The latter concept includes information gathered over time, but also tracks varying data access levels and augmentations, e.g. improved calibration. 

An AMPEL instance is currently operating at DESY, processing real-time alerts from optical, neutrino, and gravitational-wave telescopes. It is an open platform, where users can contribute both their own analysis units and channels that use them. AMPEL output can either be stored for offline analysis or used to trigger reactions in real time. The latter capability is used both for autonomous observations with robotic telescopes, submission of candidates to TNS, and transfer of data to different visualization frontends."
"P6-224","James Dempsey","CSIRO","IVOA Data Access Layer: roadmap as of year 2020","The International Virtual Observatory Alliance (IVOA) works towards standardising interoperability and curation of data and service  holdings of the global astrophysical community. Within the IVOA, the Data Access Layer (DAL) Working Group's goal is to provide Recommendations (an IVOA term for the released technical standards) for accessing data collections and catalogues; filtering data holdings based on their metadata; and retrieving the ones in scope, or operating on them.

In recent years the DAL community has addressed the multi-dimensional and multi-wavelength scenarios, and kept core standards up-to-date. It has also tackled new topics such as the observation location and object visibility information retrieval, and looked at older topics such as the outstanding cases of time domain and radio astronomy data.

DAL work in the next few years will involve a mixture of revising existing standards, listening to feedback on recently updated and released standards and the specification of new standards. The DataLink 1.1, DALI 1.2, ADQL 2.1 and ConeSearch 1.1 standards are being revised. The work on these will provide new features requested by the community, updates to match the current technical landscape, and integration of changes from other Recommendations. Other specifications have been recently updated (e.g. TAP 1.1) or are waiting further community feedback before starting a revision process (e.g. SIAP 2.0 and SODA 1.0). New protocols are under development such as the Object Visibility Simple Access Protocol (ObjVisSAP) and the Provenance TAP (ProvTAP) interface. ProvTAP is being managed together with the Data Model Working Group (DM), as is the Observation Locator TAP interface, since they include data model efforts.

Community feedback and contributions are needed for all DAL activities. Particularly important are the experiences of data providers and projects that are using VO technologies to address their scientific community's requirements. This is not a trivial task since the development of a standardised interface for the final user often requires more than one Recommendation to be implemented.

This submission aims to summarise the current DAL status and progress, in order to help new end-users understand how DAL can be beneficial. It also discusses the future changes to DAL standards. Finally, it lists topics (No-SQL, code-to-data, parametric querying, actionable retrieval, ...) worth investigating in the future to keep access layer standardisation up to date with current needs."
"P6-234","Adam Zadrożny","National Centre for Nuclear Research, Poland","Using Convolutional Neural Networks along image subtraction for detecting weak optical transients","In the poster I would like to present a method for enchasing image subtraction with CNN for detecting optical transients. The method has been tested on Transient Optical Robot Observatory of the South (TOROS) optical data connected with search of optical counterparts to gravitational wave events during LIGO O2 run. In test runs method proved to achieve more than 99% accuracy, much higher than algorithms based on random forest methods.  The method will be included into standard transient search pipeline for TOROS network in upcoming year."
"P6-251","Bruno Sanchez","Duke University","Two co-addition and image subtraction codes: properimage and ois","Difference Image Analysis and image co-addition are key time-domain astronomy tasks and 
the latest techniques are usually implemented as components of larger pipeline 
software frameworks. 
This work presents two standalone software projects that deliver simple
to use state of the art image subtraction and addition implementations. 
As a strong key point of this work is the use of open source and tested software, 
with focus in building reliable and maintainable tools for small observational projects."
"P6-40","Bilal","Institut de RadioAstronomie Millimétrique","Data handling at the IRAM 30m for NIKA2 operations","The IRAM 30m in Granada holds multiple facilities used 24/7 by our local teams and observers. 
One of these is NIKA2, a 3000 pixels continuum instrument installed in 2015, which started scientific operations in 2017.

This instrument is proposed to the science community in shared-risk observing pools. Their organisation rely on important data handling infrastructures to handle the large data-rate of the instrument. 

I will present the genesis of NIKA2 maps from preparation of the instrument to the final science result on the operation side. We also use a set of tools to monitor the instrument and telescope behavior live.

The first one is a weather forecast based on the SMA AM software, which we customized for the IRAM 30m to work with our local water vapor measurement facilities. This facilitate the scheduling and the preparation of observers for live operation with reliable forecast up to 3 days in advance.

The second is a pointing monitoring. The set of mirror installed to direct the light to NIKA2 is sensitive and need to be calibrated with a pointing model. This pointing model can be checked at all time during operation, to help quantify the pointing error and shift during operations. 

Finally, regarding data distribution and data reduction, I will present data reduced with our internal pipeline."
"P6-43","Brent Miszalski","Australian Astronomical Optics - Macquarie University","Orchestration of Dockerized Data Reduction Pipelines from a RESTful Web Service","Data reduction pipelines are traditionally run on a researcher's personal computer on a small amount of data. The pipeline may have complex software dependencies that preclude the researcher from installing it on a faster server. Even if the pipeline runs on the server, its deployment in a multiprocessing environment may be problematic. Here we present a modern solution to allow for the on demand reduction of the ever-increasing volume of data stored in telescope archives. We have developed a Python web service that accepts 2dF-AAOmega observations and determines the steps needed to reduce the data. Each step runs 2dfdr commands from a Docker container on a fast server. We utilise docker-py to remotely execute these commands from within Celery tasks, allowing for a robust, configurable data reduction workflow to be assembled and executed asynchronously by Celery across several processors. Data Central plans to offer the service to users when requesting data from the newly revamped AAT archive, allowing effortless access to freshly reduced data. The service is extensible to other pipelines and would form a solid basis for developing IVOA SODA services, while slight modifications could unlock quick turnaround reductions of transient triggered observations."
"P6-92","Jan Snigula","Max-Planck-Institut für extraterrestrische Physik","Observation meta data plotting tool - Omedaplot","LMU Muenchen operates an astrophysical observatory on Mt. Wendelstein with two telescopes (2m and 40cm) and five instruments (3 imagers and 2 spectrographs) for night time observations. Theses instruments provide a multitude of operating informations (mostly temperatures, pressures, motor positions) beyond the detector images. The observatory also hosts allsky cameras, weather stations, dome air conditioning etc. The observatory infrastructure logs everything to identify problems and their roots. We developed a webbrowser interface  to provide an easy access to this information. It allows to combine any source data from any time and time frame into simple plots with just a few clicks. This framework is easy to expand and adjust for the constantly changing observatory environment. The individual time resolved data can also be directly accessed from observatory status webpage tokens. We also use this tool to track down sources of features in observational data as the observational environment can have unforeseen impact on these data."
"P7-139","Bruno Merín","European Space Agency","Using deep learning to identify asteroid trails on ESA's Hubble data archive","The ESAC Science Data Centre is conducting Machine Learning experiments to provide AI-supported added value services to our users in the mid-term future. This presentation will describe some of the experiments conducted and our analysis of the applicability of Machine Learning in different steps of our value chain (data production, curation, archiving, valorisation, dissemination and support to users in their exploitation).

In particular, in the area of data valorisation, I will present results from one such projects, were we have used the Google AutoML Vision API to identify asteroid trails on archival HST images. For this project, we have used labels of asteroid trails from volunteer markings from the Zooniverse asteroidhunter.org project to train a Deep Learning Convolutional Neural Network embedded into Google's AutoML Vision API and then have used it to scan the whole archive to look for new asteroid trails that the volunteers had not identified. I will describe how the Precision and Recall of the machine learning model depend on the pre-processing of the images and show the very promising results from this analysis. I will end up by elaborating on how these type of initiatives will help enriching the search for data-driven patterns on archival data in the mid-term future."
"P7-166","Heidi Thiemann","The Open University","First data release of the SuperWASP Variable Stars Zooniverse project","SuperWASP is the most successful ground-based survey for transiting exoplanets, having discovered >200 hot Jupiters to date. However the capabilities of SuperWASP extend beyond exoplanet detection. The SuperWASP photometric archive contains more than 30 million light curves of bright stars (V<15) with a high cadence and long  baseline. A recent reanalysis of the entire archive has detected almost 1.6 million possible unique objects with detectable photometric periodicities on timescales from hours to years (Norton 2018).

The SuperWASP Variable Stars Zooniverse project is using citizen science to classify these 1.6 million light curves. Citizen scientists are asked to classify light curves as simplified variable star types: eclipsing binary stars, pulsating stars, rotationally modulated stars, or simply junk or unknown, and are asked whether the period is correct, incorrect, or half the correct period.

The classification of periodic variables based solely on the shape of the photometric light curve does not always provide a conclusive and unique variable type. However, it can be a good indication of the most likely type and it is useful for identifying interesting candidates which are worth following up.

We present the first data release from the SuperWASP Variable Stars Zooniverse project. It contains over 1 million classifications, corresponding to over 500,000 unique objects. The first data release consists of 4.5% pulsating stars, 5.3% detached eclipsing binary stars, 6.4% contact eclipsing binary stars, 9.9% rotating stars, 7.3% unknown & 66.6% junk. The project was launched on 05 Sep 2018 and has had engagement from ~4500 volunteers to date.

We demonstrate that the SuperWASP Variable Stars project can be used for population studies of individual variable types, as well as identifying candidates for follow up. We present the initial findings on various unique and extreme variable objects in the data release, including very short and very long period binaries, cepheids in eclipsing binary
systems, and extreme amplitude pulsators.

We propose to develop a UI similar to the ASAS-SN Catalogue of Variable Stars, in order that other researchers can easily access the outputs of this project."
"P7-206","Andrey Soroka","CMC MSU","Morphological classification of astronomical images with limited labelling","The task of morphological classification is complex for simple parameterization, but important for research in the galaxy evolution field. Future galaxy surveys (e.g. EUCLID) will collect data about more than a billion galaxies. To obtain morphological information one needs to involve people to mark up galaxy images, which requires either a considerable amount of money (in case of MTurk or Toloka commercial engines) or a huge number of volunteers (in case of citizen science projects like Galaxy Zoo). It’s worth nothing that any manual markup at some point becomes ineffective, as it does not scale well for the increasing amount of data.

Today, hybrid approaches (using both machine learning and human markup) make it possible to speed up the galaxy morphology annotation process (e.g. Beck et al. (2018) - SOTA model), but considerable efforts of humans are constantly needed to provide high accuracy of morphological classification. We search for fast and accurate machine classification methods with restricted usage of human markup resources.

We propose an effective semi-supervised approach for galaxy morphology classification task, based on active learning of adversarial autoencoder DNN model. For a binary classification problem (top level question of Galaxy Zoo 2 decision tree) we achieved accuracy 93.1% on the test part with only 0.86 millions markup actions, this model can easily scale up on any number of images. Our best model with additional markup achieves accuracy of 96.2%."
"P7-237","Pedro García-Lario","European Space Astronomy Centrer / European Space Agency, Madrid, Spain","Variable Stars in Gaia DR2: A Citizen Science project","We present a proposal to develop a citizen science project to inspect the epoch
photometry of variable sources identified as such in the Gaia science archive and
critically review/verify their variability classification. Given the large dataset of variable
sources provided in Gaia DR2, it is impossible for the experts to perform a systematic
detailed analysis of individual sources and assigned classifications. A comprehensive
exploration of the epoch photometry data with human eyes can help verifying the
classifications provided by the automated tools and identify misclassified sources
and/or weird sources displaying unusual properties, which may result in new
science and improvements in the classification software."
"P7-74","Graham Taylor West","Middle Tennessee State University","Data-driven fitness functions for optimizing simulations of interacting galaxies","Given observational data from systems of interacting galaxies, we seek to determine the values of various dynamical parameters through the optimization of numerical models via genetic algorithms. However, fitting these models can be quite difficult. The core challenges include 1) developing an objective fitness function for quantifying the similarity between model and target images and 2)  understanding the inherent symmetries of the dynamical system which promote morphological degeneracies and impede optimization. In this presentation, we show how naive implementations of fitness functions can yield unintuitive results. We then propose a novel fitness function which was developed by utilizing data from the Galaxy Zoo: Mergers project. These human-scored models were used to validate our fitness functions and led to the adoption of a tidal distortion term which dramatically improved results. We also give a characterization of various geometric and dynamical symmetries inherent within the system and show how the knowledge of these symmetries can be used to reduce the volume of the parameter search space when performing optimization."
"P8-117","Bart Scheers","Dataspex","The evolution of the MeerLICHT and BlackGEM data pipelines","Astronomical observatories call for innovative solutions for the Big Data
streams that their new facilities generate. In general, one or more telescopes
observe large patches of the skies continuously with high resolution and
sensitivity. The most important common scientific drivers are to study
transient and variable astrophysical sources on several time scales and their
light curves, and to build catalogs of all the observed sources. The enormous
amounts of data are only valuable if they can be processed and stored
automatically and if all data can be searched quickly and visualised
interactively.

It is becoming more difficult and costly for research institutions to purchase
and configure hardware and network infrastructures that meet these needs in
advance. Moreover, managing running dynamic production databases, data products
and backups, software versioning, continuous development and integration,
monitoring, tracing and logging for these systems makes a move to a more
flexible cloud environment logical.

The MeerLICHT and BlackGEM optical array telescopes, designed, built and
operated by consortia of universities, are a good example where the back-end
software and hardware designs are an integrated part of the telescopes. In this
talk, we will touch the aspects described above and focus on the evolution of a
classical in-house data pipeline towards a scalable distributed cloud data
pipeline, where multiple database, pipeline, webserver and storage nodes
interoperate in real-time and batch mode. This allows us to process larger
datasets and search the growing source catalogs faster than was possible only a
few years ago."
"P8-118","Leonardo Baroncelli","INAF OAS Bologna / University of Bologna","rta-dq-lib: a library to perform online data quality analysis of scientific data","The Cherenkov Telescope Array (CTA) is an initiative that is currently building the largest gamma-ray ground Observatory that ever existed. CTA will provide unprecedented sensitivity and angular resolution for the detection of gamma rays with energies above a few tens of GeV. A Science Alert Generation (SAG) system, part of the Array Control and Data Acquisition (ACADA) system, analyses online the telescope data – arriving at an event rate of tens of kHz – to detect transient gamma-ray events. The SAG also performs an online data quality analysis to assess the instruments’ health during the data acquisition: this analysis is crucial to confirm good detection. A Python and a C++ software library to perform the online data quality analysis of CTA data, called rta-dq-lib, has been proposed for CTA. The Python version is dedicated to the rapid prototyping of data quality use cases. The C++ version is optimized for maximum performance. The library allows the user to define, through some XML configuration files, the format of the input data and, for each data field, which quality checks must be performed and which types of aggregations and transformations must be applied. It internally translates the XML configuration into a direct acyclic computational graph that encodes the dependencies of the computational tasks to be performed. This model allows the library to easily take advantage of parallelization at the thread level and the overall flexibility allow us to develop generic data quality pipelines that could also be reused in other applications."
"P8-128","Peter Teuben","University of Maryland","Enriching Data Archives with Science Products","We review work undergoing at the ALMA Science Archive (JAO) to enhance
the archive with results from ADMIT, eventually allowing users to
select data based on interesting science queries. In a parallel project
we are building a prototype of this database, enhanced with selected
meta-data upon which users can make richer queries and select
data based on the science they are interested in."
"P8-138","Daniel Morcuende","Universidad Complutense de Madrid","Onsite processing pipeline for the CTA Large Size Telescope prototype","The prototype of the Large Size Telescope (LST) of CTA, located at the Observatorio del Roque de Los Muchachos (ORM) on the Canary Island of La Palma, is presently going through its commissioning phase. A total of four LSTs, among others, will operate together at ORM constituting the CTA North site.

A computing center endowed with 1760 cores and several petabytes disk space is installed onsite and used to acquire, process, and analyze the data produced, around 3 TB/hour during operation. LSTOSA is a set of scripts written in python which connects the different steps of the analysis pipeline developed for the LSTs, called lstchain. It processes the data produced by the prototype in a semiautomatic way producing high-level data and quality plots including detailed provenance logs. Data are analyzed before the next observation night to aid in the commissioning procedure and debugging."
"P8-142","Nat Comeau","University of Victoria","Automatic science-quality processing of the Gemini NIFS archive","Gemini NIFS (the Near-Infrared Integral Field Spectrograph)  has been in operation for 15 years. In this time, it has executed 345 science programs. Utilizing the CANFAR VM batch processing environment at the Canadian Astronomy Data Centre (CADC) and the IRAF/Nifty4Gemini data reduction tools developed by Gemini Observatory, we have produced 2805 science-ready data cubes. These processed cubes have been added to the CADC archive system for public retrieval and linked back to the input datasets within the archive.  We also describe how this work can be applied to Gemini NIRI (Near-Infrared Imager) data."
"P8-145","Nicolas Monnier","Univ Paris-Saclay, CNRS, Centralesupelec, L2S, France","Parallelisation of the wide-band wide-field spectral deconvolution framework DDFacet on distributed memory HPC system","The next generation of radio telescopes, such as the Square Kilometer Array (SKA), will need to process an incredible amount of data in real-time. In addition, the sensitivity of SKA will require a new generation of calibration and imaging software to exploit its full potential. The wide-field direction-dependent spectral deconvolution framework, called DDFacet, has already been successfully used in several existing SKA pathfinders and precursors like MeerKAT and LOFAR. However, DDFacet has been developed and optimized for single node HPC systems. DDFacet is a good candidate for being integrated into the SKA computing pipeline and should, therefore, have the possibility to be run on a large multi-node HPC system for real-time performance. The objective of this work is to study the potential parallelism of DDFacet on multi-node HPC systems. This paper presents a new parallelization strategy based on frequency domains. Experimental results with a real data set from LOFAR show an optimal parallelization of the calculations in the frequency domain, allowing to generate a sky image more than four times faster. This paper analyses the results and draws perspectives for the SKA use case."
"P8-148","Wayne B. Hayes","University of California, Irvine","Correcting Inter-Waveband Positional Errors in Sloan Images","The telescope of the Sloan Digital Sky Survey (SDSS [1]) was designed from scratch to minimize ""down-time"": the telescope does not stop to image a portion of the sky or change filters: instead, images in all 5 wavebands are captured continuously as the telescope moves along a Great Circle on the sky, with all five cameras--one for each waveband--capturing the same area of sky in sequence about 1 minute apart as that region drifts across the camera's field. This is called ""Drift Scan Mode"", and although it maximizes the light captured per unit time, it means that image pixels extracted from the SDSS database do not correspond to physical pixels on the camera, but were instead constructed by a software pipeline that ""tracks"" the sky as it drifts across camera pixels. Unfortunately, this pipeline makes no attempt to ensure that image pixels across wavebands correspond to the same patch of sky. This confounds attempts at cross-waveband comparisons of the same object, since there is no 1-to-1 correspondence
between inter-waveband pixels, and location on the sky.

To solve this problem and reconstruct images for all wavebands that are better suited to precise inter-waveband astrometry, we use the positions of stars visible in any two wavebands to estimate the inter-waveband positional shift. We then use the FITS file to transfer flux between adjacent pixels to construct new images in which the ""same"" pixel corresponds to the same patch of sky across all wavebands. In this abstract we describe the process used to achieve this and quantify the errors in flux and position introduced by our method. The attached image shows an example of an original SDSS image containing a galaxy, the same image shifted half a pixel (which seems to produce worst-case errors), and the residual after a second shift back to the original position. Note that the largest errors occur near stars, whereas the background pixels, and the galaxy, are virtually invisible in the residual image."
"P8-17","Dirk Petry","ESO, ALMA Regional Centre, Germany","Prototyping astronomical science pipelines with script generators","In modern astronomical projects, the amount of scientific data produced and archived is typically huge. Calibrating this data and extracting science products is work-intensive and requires automation in pipelines. However, developing these pipelines requires an intimate knowledge of the typical properties of the data, and that knowledge can often only be acquired by experience over the course of the first few months or years of instrument operation under realistic conditions. In other words, it is nearly impossible to have a completely automated pipeline ready at the time of the official start of science observations unless that start is intentionally delayed to take large amounts of test data and complete the development and testing of the pipeline. The only solution is to start operations with a prototype pipeline and gradually develop the final pipeline in parallel. In the ALMA project, we have successfully used data reduction script generators to provide such a prototype."
"P8-182","Nicolás Cardiel","Universidad Complutense de Madrid","Filabres: a new pipeline for the automatic data reduction of CAFOS direct imaging","Filabres is a new Python pipeline created with the idea of performing the automatic reduction of direct images obtained with the instrument CAFOS, placed at the 2.2 m telescope of the Calar Alto Observatory. The goal is to provide useful reduced images through the Calar Alto Archive hosted by the Spanish Virtual Observatory. The typical workflow with Filabres consists of the following steps: (1) Image classification (bias, flat-imaging, arc, science- imaging, etc.); (2) Reduction of calibration images (bias, flat-imaging) and generation of combined master calibrations as a function of the modified Julian Date; (3) Basic reduction of individual science images, making use of the corresponding master calibrations (closest in time to the observation of the science target). The main reduction steps considered here are: bias subtraction, flatfielding of the images, and astrometric calibration (performed with the help of additional software tools provided by Astrometry.net and by AstrOmatic.net). The behaviour of the data reduction is easily defined through a set of reduction rules set in a configuration YAML file, specifically built for the considered instrument and observation mode. Note, however, that the software has been designed to allow the future inclusion of additional observing modes and instruments.
The software is publicly available through GitHub at https://github.com/nicocardiel/filabres, and its documentation in https://filabres.readthedocs.io/."
"P8-191","Oleg Smirnov","Rhodes University & South African Radio Astronomy Observatory","shadeMS: rapid plotting of Big radio interferometry Data","Radio interferometry data was big well before “Big Data” was a glint in a proto-data-scientist’s eye. The raw outputs of a radio interferometer, i.e. the complex visibility data and all associated metadata, while of little interest to the end-user astronomer per se, contain a wealth of information about the functioning of the instrument and software pipelines, and can provide vital diagnostics during the entire data reduction process. It is therefore important to be able to visualize them in all sorts of ways. However, the sheer size of these datasets (e.g. upwards of a billion measurements for even a short MeerKAT observation) calls for fairly sophisticated plotting techniques that can represent both dense data and outliers, and do it in a reasonable timeframe. This is well beyond the capabilities of our trusted workhorse Matplotlib.

Two recently developed technologies make a solution possible. The Datashader suite (https://datashader.org), driven by Big Data developments in multiple fields, provides functionality for rendering huge datasets onto two-dimensional canvases, using a variety of aggregation and categorization options. The Dask-MS library (https://dask-ms.readthedocs.io, see also Perkins this conf.) provides a native mapping from the Measurement Set, the standard radio astronomy data format, to Dask arrays, which facilitate massively parallel computation (and are natively supported by Datashader).

The shadeMS tool (https://github.com/ratt-ru/shadeMS) brings these two technologies together to allow for the rapid plotting of radio interferometry data. The premise of shadeMS is to support the plotting of anything versus anything, aggregated by anything and coloured (i.e. categorized) by anything, via a straightforward command-line or Python interface. The use of Dask means that a large number of cores can be efficiently exploited, making the plotting process I/O-limited in many cases. This allows data processing pipelines to produce a rich variety of diagnostic plots with relatively little overhead."
"P8-193","Bjorn Emonts","National Radio Astronomy Observatory","The CASA Software for Radio Astronomy: Status Update from ADASS 2020","CASA, the Common Astronomy Software Applications package, is the primary data processing software for the Atacama Large Millimeter/submillimeter Array (ALMA) and NSF's Karl G. Jansky Very Large Array (VLA), and is frequently used also for other radio telescopes. This poster gives an overview of several exciting recent developments. With the arrival of CASA 6, users now have the option to download CASA through a modular pip-wheel installation, giving them the flexibility to integrate CASA into their personalized Python environment. Another major new development is the introduction of a new task for joint deconvolution of single-dish and interferometry data. This has been a major priority for our user base, and a growing number of test-cases reveal that this algorithm provides a great improvement in combining single-dish and interferometry data. We briefly also highlight the CARTA visualization software, and summarize the proto-type development of a next-generation CASA software and related infrastructure for future radio facilities, such as a Next-Generation VLA."
"P8-195","Vito Conforti","INAF","Performance improvement of the Data Acquisition System to support the observation quality system of the ASTRI Mini-Array","The ASTRI (Astrofisica con Specchi a Tecnologia Replicante Italiana) program was born as a collaborative international effort led by the Italian National Institute for Astrophysics (INAF) to design and realize, within the Cherenkov Telescope Array (CTA) framework, an end-to-end prototype of the Small-Sized Telescope (SST) in a dual-mirror configuration (2M). The pro
totype, named ASTRI-Horn, is operative, being installed at the INAF observing station located on Mt.Etna (Italy). The ASTRI project includes the building of a Mini-Array of nine ASTRI telescopes that will be installed and operated at the Teide Observatory (Spain). The ASTRI software supports the operations of the ASTRI-Horn prototype and, eventually, of the Mini-Array. 

The data acquisition system of the ASTRI-Horn telescope acquires, packet by packet, the read-out data from the back-end electronics of the ASTRI camera. The packets are then stored locally in one raw binary file as soon as they arrive. During the acquisition, the packets are grouped by data type (scientific, calibration, engineering) before processing and storing the decoded data in FITS format. A quick-look component, running on the same machine, that allows the operator to display the decoded data during the acquisition, was also implemented.

During software testing and operations performed with the ASTRI-Horn prototype, due to a cftisio writing method we experienced a bottleneck in the raw-to-FITS binary data conversion when the acquisition rate was greater than about 3 Mbps. We thus decided to apply a workaround and to postpone the conversion in FITS format of the ASTRI-Horn raw data after the end of the acquisition run. 
 
The ASTRI Mini-Array software requires monitoring the data quality as soon as data are available, and this evaluation is performed by the online observation quality system (OOQS). For this reason, and because the Mini-Array consists of 9 ASTRI cameras observing in parallel, we are evaluating a solution to directly send the decoded data from the data acquisition system to the OOQS, strictly reducing any bottleneck. 

In this paper, we will present a solution to improve the data transfer efficiency from the data acquisition system, based on redis, to the OOQS component of the ASTRI Mini-Array. Results of the comparison between the acquisition software developed for the ASTRI-Horn and this solution based on Redis will be also shown and discussed."
"P8-198","Yohei Hayashi","NAOJ","Pipeline Calibration and Imaging for the Nobeyama 45m Radio Telescope","Nobeyama 45m Radio Telescope (NRO45m) is a large single-dish radio telescope that has been operating for more than 35 years. The data processing pipeline for NRO45m (NRO Pipeline) is developed recently as an extension of the single-dish pipeline for the Atacama Large Millimeter/submillimeter Array (ALMA). The NRO Pipeline is able to process spectral line data taken with the On-The-Fly raster scan. The pipeline calibrates the data, protects spectral line features, subtracts residual spectral baseline component, detect and discard low-quality data, and performs imaging to produce FITS cubes for the science target. The pipeline is written in Python and is built on top of Common Astronomy Software Applications (CASA). 

This presentation describes overall summary of the framework of the pipeline, detail of the extension from the ALMA pipeline, and products of the NRO Pipeline. As part of this development project, another application to convert the NRO45m observing data into CASA native data format, MeasuremntSet, is developed as well. The NRO Pipeline, combined with the data conversion application, enables to establish semi-automated operational workflow from the data acquisition through data processing to the ingestion to the data archive."
"P8-210","Salam Dulaimi","National University of Ireland-Galway","The Light Curve Fitter: A novel application for deconvolving two superimposed sinusoidal waves.","The Galway Ultra-Fast Imager (GUFI) located on the 1.8m Vatican Advanced Technology Telescope (VATT) was tasked to monitor tight brown dwarf binaries.  However, due to the close separation between the components in these binaries, the GUFI photometer could not image each component of binary systems as a point source in our campaign. Therefore, we developed and employed our novel application, the ‘Light Curve Fitter’, which is capable of distinguishing two superimposed sinusoidal waves.  This application allows users to untangle the secondary component's variability signature from that of the dominant primary variability."
"P8-221","Jan Novotny","Institute of Physics, Silesian University in Opava","Implementing CUDA Streams into AstroAccelerate – A Case Study","To be able to run tasks asynchronously on NVIDIA GPUs a programmer must explicitly implement asynchronous execution in their code using the syntax of CUDA streams. Streams allow a programmer to launch independent concurrent execution tasks, providing the ability to utilise different functional units on the GPU asynchronously. For example, it is possible to transfer the results from a previous computation performed on input data n-1, over the PCIe bus whilst computing the result for input data n, by placing different tasks in different CUDA streams. The benefit of such an approach is that the time taken for the data transfer between the host and device can be hidden with computation. This case study deals with the implementation of CUDA streams into AstroAccelerate. AstroAccelerate is a GPU accelerated real-time signal processing pipeline for time-domain radio astronomy."
"P8-222","Jose Sabater","STFC UK Astronomy Technology Centre","Processing and calibration of the deepest low frequency images of the sky","The LOFAR Two-metre Sky Survey (LoTSS) Deep Fields have produced the deepest low-frequency radio astronomy images of the sky to date. In this poster, we show the processing methodology followed to produce these deep images focusing in the data calibration pipeline developed, the infrastructure used, and the challenges found. An initial data volume of ~2 PB was reduced to a snapshot of ~1.5 GB in which a sky density of radio objects of ~5000 sources per square degree is reached in the central 25 square degrees with a resolution of 6 arcseconds. The development of these new calibration methods and pipelines have been crucial in enabling the scientific exploitation of state-of-the-art radio infrastructures and will be relevant for future infrastructures like the Square Kilometre Array."
"P8-235","Karel Adámek","Czech Technical University, Thákurova 9, 160 00 Prague 6, Czech Republic","Implementation of the 3D degridding for the NVIDIA GPUs using CUDA","Deconvolution is a part of the image reconstruction in the radio interferometry used in the radio astronomy to calculate the image of the sky. The image reconstruction consists from several steps and transforms irregularly sampled visibilities, which is an amplitude and phase of electromagnetic waves detected by an interferometer, into the intensity of the radio waves at a given location on the sky called sky brightness. The first step in the image reconstruction is the gridding process, where we transform detected visibilities into a regular grid which allow us to use Fast Fourier Transform (FFT) instead of direct Fourier transform which is computationally expensive. The sky true brightness is then reconstructed by an iterative process where the brightest sources are deconvolved back to the positions of the irregularly sampled visibilities and subtracted from measured visibilities creating a cleaner image. This is repeated until technique converges. In this work, we have investigated the performance of a part of the 3D deconvolution process which calculates irregularly sampled visibilities from precomputed subgrids on the GPU. The number of visibilities which needs to be calculated can vary greatly by several magnitudes depending on the position in the image and the configurations of the telescope. This variability may cause load balancing issues. We have implemented this using CUDA for NVIDIA GPUs and have achieved performance of 540 million deconvolved visibilities per second."
"P8-238","Bob Watson","JBCA, University of Manchester","Development of a widefield mosaic imaging pipeline for e-MERLIN","Widefield imaging with a heterogeneous array, such as e-MERLIN, comprised of antennas with different primary beam sizes is problematic for sources on the edge of the beam. Furthermore, in mosaiced observations with contributions from overlapping fields is an extra complication, especially in the case of a weak lensing survey such as SuperCLASS which is reliant of shape measurement. The use of an A-term correction at the point of gridding visibilities can include the primary beam correction, but can be slow and memory intensive. We investigate using a modified version of WSCLEAN to carry this out."
"P8-247","Nuria Lorente","AAO - Macquarie University","PyCPL: the ESO Common Pipeline Library in the Python age","The ESO Common Pipeline Library (CPL) comprises a set of ISO-C libraries that provide a comprehensive, efficient and robust software toolkit to develop astronomical data-reduction recipes, which has been the fundamental tool for building data reduction pipelines for ESO VLT instruments since its original release in 2003, and which will continue to be the basis for ELT instrument pipelines. CPL was developed in C for reasons of efficiency and speed of execution, and because of its maturity and widespread use, it is well tested and understood.
However, as the astronomical community’s preference moves more and more towards Python as the language of choice for algorithm prototyping and data reduction, there has emerged a need to provide the CPL functionality for users who wish to make use of the power of CPL from a Python environment.
The PyCPL project aims to implement Python bindings for CPL through PyBind11, to provide the ability to execute and control data reduction pipelines from Python. It will also provide the ability to write recipes in Python and provide access from the Python environment to the products generated, thus providing the astronomical community the means to leverage the strengths and speed of CPL from a pure Python ecosystem."
"P8-249","Keri Heuer","Drexel University","Sensitivity of Line Emissivities to Atomic Uncertainties with PyAtomDB","With upcoming high-resolution X-ray spectroscopy missions such as XRISM and Athena, it is
important to quantify the impact that uncertainties on fundamental atomic quantities have on the
interpretation of observational X-ray spectra. We use the pyatomdb package to perturb ionization
and recombination rates as well as the direct excitation rates and A-values stored in the AtomDB
database to estimate final uncertainties on line emissivities and charge state distributions. We
implement plausible approximations to uncertainties in the underlying atomic data to provide
observers estimates of final uncertainties on spectral calculations."
"P8-258","Paolo Serra","INAF - Osservatorio Astronomico di Cagliari","Crystalball, a Dask and Numba accelerated DFT Model Predict","Producing an astronomical image from the data generated by a Radio Interferometer involves processing the observed visibilities, which are obtained by measuring complex voltages between antenna pairs. As part of this process, it is often necessary to build a model of the sky area observed with the Interferometer, and to turn that sky model into a set of model visibilities. The latter step, commonly referred to as prediction of the model visibilities, is computationally expensive.  The model visibilities are generated by the application of the Radio Interferometry Measurement Equation to the sky model, typically represented either as an intensity image or a list of discrete model components. In the former case, faster but less accurate convolutional degridding is applied, while a slower but more accurate Direct Fourier Transform (DFT) is applied in the latter.

The accuracy of the DFT method is required in a number of science cases. An important one is that of spectral-line interferometry. In this case, a continuum sky model must be subtracted from the observed visibilities before making spectral-line images. In practice, frequency-averaged data are used to produce both a continuum image and a component sky model where each component has its own spectral shape. The DFT method takes each component’s spectral shape into account when predicting the model visibilities, allowing one to properly subtract the continuum and thus obtain high-quality spectral-line data. 

Here we present Crystalball, an implementation of the DFT model prediction currently used as part of the CARACal radio interferometry pipeline. In this pipeline, the continuum image and sky model are obtained with WSClean. Crystalball performs a DFT Predict of a Component  Model, using the Python dask and numba packages to accelerate execution. Dask is a parallel processing framework based on computational graphs, while numba jit-compiles a subset of Python code to machine code with performance comparable to C or Fortran. Dask-ms exposes Measurement Set columns as dask Arrays, while the numba code, implemented in the codex-africanus package, is applied to individual chunks of these arrays.

Through the use of these technologies, the expensive DFT has been made tractable on contemporary datasets produced by the MeerKAT telescope. For example, a Measurement Set of 5M rows, 1,000 channels and 2 correlations takes 24 h to predict 10,000 components on an Intel Xeon 2.6 GHz processor with 32 CPUs/threads. All CPU cores are fully exercised during the duration of the prediction step, with modest and tunable usage of RAM. Crystalball has been useful in producing spectral-line data products from recent MeerKAT data (e.g., Serra et al. 2019, A&A, 628A, 122).

While the DFT has been made tractable, Crystalball still consumes a large portion of a CARACal pipeline run. As CARACal currently does not support GPUs, GPU-accelerated prediction as provided by packages such as Montblanc are not feasible. Future work would involve distributing the prediction over a compute cluster or writing a GPU predict should the CARACal pipeline support this processor type in the future."
"P8-26","Hariharan Krishnan","Arizona State University","A Novel Real-Time Radio Transient Imager for LWA-SV","In this paper, we describe our efforts towards the development of a real-time radio imaging correlator for
the Long-Wavelength Array station in Sevilleta, New Mexico. We briefly discuss the direct-imaging algorithm and present the architecture of the GPU implementation. We describe the code-level modifications carried out for individual modules in the algorithm that improves GPU-memory management and highlight the performance improvements achieved through it. We emphasize our ongoing efforts in tuning the overall run-time
duration of the correlator which in turn is expected to increase the operating bandwidth in order to address the demands of wide-band capability for radio transient science."
"P8-264","Ariel Tirado","Ariel Tirado","ChiVo-Tools: a deep learning tool for exploratory analysis of astronomical data for the Jupyter notebook","The diversity, volume and complexity of astronomical data increase as new astronomical facilities become online. As a result, tasks like the separation between point and extended sources or morphological classification of galaxies, to name a few, also present new challenges for their implementation. On one hand, new Computer Vision models based on Deep Learning have demonstrated revolutionary results, but there is a gap between these models and their day-to-day application by the astronomical community. We present ChiVo-Tools, an exploratory analysis plugin for the Jupyter notebook, that boasts a powerful instance segmentation engine based on the Mask R-CNN architecture. This new application allows the automatic classification and segmentation of galaxies according to their morphology, especifically between elliptical and spiral classes for SDSS images. From the user perspective, ChiVo-Tools works by taking a small region of the sky selected by the user and then, the Deep Learning engine generates an output catalog with the classification of the sources contained. Moreover, ChiVo-Tools seeks to become a trailblazer for a new generation of scientific tools, which will bring the exploratory analysis of astronomical data based on Deep Learning closer to the users."
"P8-28","Kristof Rozgonyi","International Centre for Radio Astronomy Research","Proof-of-concept gridded visibility stacking pipeline for deep spectral line interferometric imaging","Deep spectral line imaging is one of the greatest challenges for the SKA and its pathfinders, as the raw visibility
data-streams go beyond the practical capabilities of traditional long-term data storage and imaging techniques. The Deep Investigations of Neutral Gas Origins (DINGO) survey on the Australian Square Kilometre Array Pathfinder (ASKAP) is planning to observe for up to 2,500 hours per field for its deepest survey regions, likely spread over multiple years. As the corresponding raw datasets would be >100PB, and cannot be stored, the default pipeline will form daily images that are then averaged together to create the final deep image data products. This imaging method is severely limited for re-processing to improve final image quality. In particular, small systematic uncertainties, that may be unnoticeable on the daily images, could have a severe impact on the final data products. Therefore, compressed visibility storage is required for post-survey re-processing of the deep data. To meet this need, we developed an alternate DINGO pipeline in which visibilities are stored as a gridded data product. Gridded visibilities are sparse, so can be stored efficiently on a similar level as other compressed visibility formats, such as baseline-dependent averaged visibilities. This method is more than halves the long-term storage cost for the full ASKAP configuration (<30PB) and offers the lowest storage cost for a compact ASKAP configuration using only a subset of 30 antennas (∼5PB). Furthermore, gridding the data in this manner applies the correct kernels, whilst maintaining the ability to flag, reweight or even recalibrate the data. Thus, this approach addresses the greatest risks of the default daily imaging strategy. We present our proof-of-concept pipeline, and we demonstrate that our pipeline introduces no significant systematics as compared to accumulating the daily images. Furthermore, we report on our progress on imaging of DINGO pilot observations."
"P8-38","Aurelien Jarno","Univ Lyon, Univ Lyon1, Ens de Lyon, CNRS, Centre de Recherche Astrophysique de Lyon UMR5574, F-69230, Saint-Genis-Laval, France","Validating data reduction algorithms through advanced instrument simulation - the case of HARMONI","HARMONI is the first light visible and near-IR integral field spectrograph for the ELT. It covers a large spectral range from 450nm to 2450nm with resolving powers from R (≡λ/Δλ) 3500 to 18000 and spatial sampling from 60mas to 4mas. It can operate in two Adaptive Optics modes - SCAO (including a High Contrast capability) and LTAO - or with NOAO. The project is preparing for Final Design Reviews.

The HARMONI data reduction pipeline is currently being developed. Its goal is to transform raw observations into a fully calibrated, scientifically usable data cube. It is designed to be mostly run as an automated pipeline and to fit into the ELT data flow environment. Given the complexity of HARMONI we have developed an instrument numerical simulator, which simulates the instrument from the optical point of view and provides synthetic exposures simulating detector readouts of calibration exposures or of data-cubes containing astrophysical scenes. To develop the data reduction pipeline, it is often used with some effects disabled or amplified and with arbitrary data in input, which in turn gives access to the ground truth.

In this paper, we briefly present the HARMONI project, the pipeline and the instrument numerical simulator. We then explain how we use this instrument simulator to prototype and validate robust pipeline algorithms for each of the 44 scale/band combinations of the instrument, long before the it is built. We give examples of such algorithms (flat-fielding, wavelength, and geometrical calibrations, shift measurement, etc.). For each of them we detail the gain brought by the simulator, and how it has been used to provide feedback on the instrument design."
"P8-46","Stephen Gwyn","Canadian Astronomy Data Centre","The CFIS processing pipeline","The Canada-France Imaging Survey (CFIS), will cover the northern sky
above 25 degrees galactic latitude in the u and r bands using MegaCam
mosaic imager on CFHT. The data is being processed using a modified
version of MegaPipe, the MegaCam data pipeline at the CADC.  The data
is astronometrically calibrated using GAIA DR2. While the r-band data
can be easily photometrically calibrated using the Pan-STARRS 3PI
survey, the u-band is harder to calibrate. A combination of the SDSS
and the GALEX archive are used to generate in-field standards across
the sky. The calibrated individual images are resampled and coadded on
to a series of tiles evenly spaced in RA and Dec. Stellar photometry
is computed on the tile using a curve-of-growth (COG) method, with the
details of the COG generated by tracing each star back to the
individual input images."
"P8-47","Saverio Lombardi","INAF-OAR and ASI-SSDC","The data reduction and analysis software of the ASTRI Project","The ASTRI (""Astrofisica con Specchi a Tecnologia Replicante Italiana"") Project developed a technologically innovative solution for small size (~4m diameter) and large field-of-view (more than 10 degrees) telescopes of the same class as of the forthcoming Cherenkov Telescope Array Observatory (CTAO) Small-Sized Telescopes (SSTs). A prototype telescope, named ASTRI-Horn d'Arturo and deployed on Mt. Etna (Italy), started its scientific operations in 2018. The current aim of the project is the deployment of a mini-array of 9 ASTRI SSTs at the Teide Observatory (in the Canary Island of Tenerife), which will perform deep observations of the galactic and extragalactic sky at multi-TeV energies with an unprecedented sensitivity with respect to current Cherenkov telescope arrays.
Since its inception, the ASTRI Project embraced an end-to-end approach that included the development of the full data processing chain, from raw data to scientific products. To this end, a dedicated software package, called A-SciSoft (ASTRI Scientific Software), has been developed for the reduction and analysis of both ASTRI prototype and mini-array data.
In this contribution we present the main features of the A-SciSoft software package, in terms of high-level requirements, data model, data flow, functional design, pipelines, and framework. The latest validation tests and the main results achieved so far from the processing of simulated and real ASTRI data are also reviewed."
"P8-55","Jonathan Kenyon","Rhodes University & SARAO","QuartiCal - Accelerating calibration using Numba and Dask","Calibration in the era of MeerKAT and its ilk is becoming an increasingly daunting task due to the sheer volume of data produced by these instruments. With the SKA fast becoming a reality, it is critical to design and implement calibration algorithms which exploit parallel hardware whilst minimising memory footprint. Furthermore, distributed systems are increasingly necessary to process interferometer data in a reasonable amount of time. QuartiCal is a Python package implementing calibration in this context. To this end, QuartiCal makes use of a combination of Dask and Numba. Dask, using its powerful schedulers, allows appropriately written code to scale from executing on a laptop to a compute cluster. Numba is a just-in-time compiler for Python/NumPy which can provide C-like speed whilst retaining much of Python’s simplicity and syntax. Preliminary experiments have shown that the synergy between the above technologies outperforms QuartiCal’s predecessor - CubiCal - in both speed and memory footprint. This result is hard-won, as employing these cutting-edge technologies comes with a unique set of challenges in terms of debugging and optimization. In overcoming these challenges, wisdom and patterns have been developed which may be of use to the community at large."
"P8-67","Felix Stoehr","ESO/ALMA","The ALMA Re-Imaging for Legacy Project","The ARI-L project is an European Development project for ALMA Upgrade approved by JAO and ESO that officially started in June 2019. Recently, the ARI-L project completed successfully its first year review, with the reported activities well ahead of schedule. The first 60000 science FITS files have already been ingested into the official ALMA Science Archive.

The project aims to increase the legacy value of the ASA by bringing the reduction level of ALMA data from Cycles 2-4 close to the level of the more recent Cycles processed with the ALMA Imaging Pipeline. In three years, the ARI-L project will produce and ingest into the ASA a uniform set of full data cubes and continuum images at native spectral resolution and average weighting scheme, covering at least 70% of the observed data from Cycles 2-4. These cubes and images do complement the much smaller archived QA2-generated image products, which cover only a tiny fraction of the available data for those Cycles. The complete set of ARI-L imaging products will be highly relevant for many science-cases and significantly enhance the possibilities of exploitation of archival data."
"P8-84","Ruben Lopez-Coto","INFN Padova","lstchain: An analysis pipeline for LST-1, the first prototype Large Size Telescope of CTA","The future Cherenkov Telescope Array (CTA) will have telescopes of different sizes, being the Large Sized Telescopes (LSTs) the largest ones. Located on the island of La Palma, the LST-1, the prototype of the first LST, started taking astronomical data in November 2019, detecting the first gamma-ray sources right afterwards. The analysis pipeline, that processes data from raw inputs until high level products is called lstchain and is heavily based in the CTA prototype pipeline framework ctapipe. In this presentation I'll show the pipeline that performs signal integration, image cleaning, image parameter calculation, and machine learning methods for true parameter reconstruction."
"P8-90","Andrea Modigliani","European Southern Observatory","The High Level Data Reduction Library","The European Southern Observatory (ESO) provides pipelines to reduce data for almost all Very Large Telescopes (VLT) instruments. In order to reduce the cost of development, verification, and maintenance of ESO pipelines, and at the same time to improve the scientific quality of pipelines data products, ESO develops a limited set of versatile and instrument-independent high-level scientific functions to be used in the pipelines. These routines are provided by the High-level Data Reduction Library (HDRL). HDRL is already used by more than half of the ESO public pipelines with clear benefits on implementation and improved quality of the final data products. HDRL is also used in the core functions of the ESO Toolkit Pipeline (esotk). The latter includes a few recipes to perform stacking of 1D spectra and the detection and correction of fringes on images. Providing HDRL functionality via esotk, instead of using them in a given pipeline, offers some advantages, but also creates a few drawbacks. This talk describes design choices, summarises our experiences implementing HDRL modules and using them in the pipelines, and provides an overview of the different functionalities. Finally, we will focus on a number of recent HDRL improvements."
"P8-91","Valentina D'Odorico","INAF - Trieste Astronomical Observatory","Quasar continuum fitting with the ESPRESSO Data Analysis software","Modelling the emission continuum in quasar spectra is a notoriously difficult task, which affects several important science cases in the field of Observational Cosmology. The Data Analysis Software (DAS) developed for the VLT ESPRESSO spectrograph provides one of the few off-the-shelf solutions to iteratively disentangle the emission components from the absorption lines. Compared to other solutions, the DAS continuum recipe does not require large training sets to work and proved to be effective for a large range of quasar emission redshifts; furthermore, its algorithm is naturally applied to any medium-to-high-resolution spectrum (R between ~1k and ~100k). In this presentation we give a review of the DAS continuum fitting algorithm based on a set of test cases, and discuss its future development."
"P9-103","Siddha Mavuram","The University of Maryland, College Park - ASCL","ASCL's New API: Query Our Database","We have developed an API for the Astrophysics Source Code Library (ASCL) that enhances the ability of users to conduct complex and automated queries on ASCL indexed codes. The API is public and allows anyone to programmatically search and filter the ASCL software database via an HTTP request. For example, the search https://ascl.net/api/search/?q=”supernova”&fl=credit returns a list of authors with ASCL-indexed codes involving supernovae in JSON format. We will demonstrate the API and show its use in answering a researcher’s questions regarding the growth and usage of both interpreted and compiled languages in the database, gaining a more nuanced understanding of trends in astrophysics software development. Our findings confirmed a piece of conventional wisdom: that Python is growing in market share, while low level programming languages like C and C++ remain very popular. Further documentation for the API is available at https://github.com/teuben/ascl-tools/tree/master/API"
"P9-113","Giuseppe Riccio","INAF - Istituto Nazionale di Astrofisica","A modular web application for astronomical instrument data analysis and monitoring services","In the last decade, Astronomy has been the scene of the realization of panchromatic surveys, with sophisticated instruments acquiring a huge quantity of exceptional quality data. Such instruments pose the need to integrate advanced data-driven science methodologies for the automatic exploration of huge data archives, and the need for efficient short- and long-term monitoring and diagnostics systems, keeping the quality of the observations under control, and being able to reveal and circumscribe anomalies and malfunctions, facilitating rapid and effective corrections and ensuring correct maintenance of all components over time. In particular, this requirement is crucial for space-borne observation systems, both in logistical and economic terms.
AIDA (Advanced Infrastructure for Data Analysis) is a portable and modular web application, which was created with the aim at providing an efficient and intuitive software infrastructure to support monitoring over time, diagnostics and data analysis of a generic system, particularly suited for astronomical multiple focal plane instruments.
Given its modular system prerogative, it is possible to extend its functionality, by integrating and customizing telemetry monitoring and diagnostics systems, as well as scientific data analysis solutions, which can be based on machine learning and data mining techniques and methods. A specialized version of AIDA has been already used as diagnostic and monitoring service for focal plane instruments of the ESA Euclid mission."
"P9-119","Filippo Quarenghi","OROBIX Srl.","Hangar at VIRGO","Hangar is an open source data versioning tool that is geared towards reproducibility and collaboration on numerical datasets, with semantics that are similar to git. Its intrinsic adaptability to different data structures make Hangar a valuable tool for any physics data analysis pipeline. It also represents a flexible framework in the context of machine learning projects, allowing to choose the best suited training and test sets for the goal to be achieved.

As a collaboration between VIRGO/EGO and OROBIX during the ESCAPE H2020 project, here we present the implementation of Hangar into a machine learning pipeline of gravitational wave physics at VIRGO, providing scientists with git-like features to manage and control their experimental data. In the context of the VIRGO Collaboration, data versioning provided by Hangar also is envisioned to be used in general data processing pipeline, to browse among different versions of scientific data, depending on the calibration process."
"P9-141","Juan Gonzalez-Nuñez","ESA - ESDC","Preparation for Gaia EDR3 in the ESA Gaia Archive","In the process for releasing Gaia EDR3, due 3rd December, the ESA archive has significantly increased its functionalities from DR2. From the possibility to query external TAP services, to ADQL language extensions and new product linking capabilities, the ESA Gaia archive makes easier than ever to manipulate and relate Gaia data with any other dataset. We review the different capabilities and how they enable cross-data manipulation with other sources.

After 3 data releases, development process, operational procedures, methodologies and associated infrastructure have been extended, optimised and streamlined. This has lead to more predictable release cycles, better resources utilisation and higher systems availability. We explore how this has been performed, and the lessons learnt along the way."
"P9-170","Thomas Boch","CDS, Observatoire astronomique de Strasbourg","Innovative tools fostered by the HiPS ecosystem","In 10 years, HiPS has evolved from a prototype experiment led by CDS to a real ecosystem, supported by more than 20 data centers exposing their own HiPS node. This trend has been pushed by advanced and simple clients (Aladin Desktop, Aladin Lite) or portals (ESASky, ESO Science Portal) and thanks to  Hipsgen. Today the HiPS ecosystem gathers 900 HiPS datasets published by 20+ HiPS nodes

We will describe different tools and services that benefit from having a large collection of multi-wavelength datasets available in the same format: hips2fits (FITS cutouts generation from HiPS tiles), on-the-fly generation of RGB tiles from pre-existing HiPS, HiPS as a container for 1d and 2d histograms, CatTiler (dynamic generation of HiPS density maps from IVOA TAP queries),  computation on the HiPS grid, generation of Spectral Energy Distribution from FITS tiles.

Most of these tools are either already available in production or as prototypes."
"P9-178","David Rodriguez","Space Telescope Science Institute","The Common Archive Observation Model at MAST","With over 30 million observations across more than a dozen missions, the Mikulski Archive for Space Telescopes (MAST) offers a vast amount of astronomical data accessible to users all across the world. To organize the diverse formats and metadata content each mission provides, we utilize the Common Archive Observation Model (CAOM). CAOM consolidates metadata to a homogeneous data model that can be easily queried and is in operational use by other astronomical archives as well, such as CADC and ESAC. Our various MAST services, such as the Portal, TAP, Astroquery, and more, all access CAOM to provide the information users need. In this poster, we will outline how metadata is organized in CAOM and how users can better leverage the capabilities of CAOM to empower the discovery of data."
"P9-201","Caroline Bot","CDS, Observatoire Astronomique de Strasbourg","Hipsgen: the Hierarchical Progressive Survey generation tool","Hierarchical Progressive Surveys (HiPS) have become a widely used standard for storing and exploring large image surveys in the last decade. Among the many reasons for this success, the Hipsgen tool has had a special role. Hipsgen enables anyone to easily transform a set of individual images into a HiPS that can be visualized in a  progressive way. Its usage has enabled the creation of a vast collection of surveys, spanning the whole wavelength range, at widely different resolutions, covering different fractions of the sky, … all in the same multi-scale format. This poster will review the basic properties of Hipsgen and the ingredients that make it a versatile and easy to use projection tool."
"P9-202","Jesus Salgado","Quasar for ESA","Re-engineered ISO Data Archive. An infrared astronomy legacy treasure","ISO (1995-2001) was an important ESA legacy mission that observed the sky in the infrared wavelength. This satellite performed around 30,000 science observations, including parallel and serendipity modes of the instruments (observational modes in which an instrument could observe while another instrument was prime). Spectral data is still a reference in infrared astronomy and ISO data have special interest these days as it is being used by scientists e.g. to help on the preparation JWST proposals, both because of the energy range that ISO covered and for transient data identification. 

The ISO data archive was the first archive produced at ESAC by the Science Archives Team, using, at that time, powerful technologies like Java Applets and three-tier-architecture. 25 years from is launching, the archive has been reengineered with new technologies, including a powerful user interface (GWT), command line access, Virtual Observatory protocols (SIAP, SSAP, TAP, ObsCore) and, also, an astroquery module to allow the access using astronomical python libraries. This new archive has, also, accurate geometrical queries with detailed footprints that can be displayed on an Aladin Lite visualizer. As part of the user interface capabilities, dynamic visualisation is offered for spectral data (highcharts) and images (js9).

Also, apart from the pipeline data, data processed  by expert users (HPDP - Highly Processed Data Products) can be queried and downloaded from the interface. Finally, publications linked to ISO observations can be also found providing a deep access to all the scientific treasure that this satellite produced."
"P9-216","Gilles Landais","CDS","Exploring Provenance tracing for VizieR catalogues","Data origin is one of the basic metadata expected in the FAIR context. The information exists in the VizieR database and is made available for the end-user through the VizieR Web pages, the DOI metadata or in the IVOA Registries catalogue record. However, most of the VizieR queries are executed in the Virtual Observatory framework using standards which don't include information like the reference of the article, the authors or the date of publication.

For this reason, we explored the implementation of the recent IVOA Provenance data model in the VizieR context in order to provide the information in a machine readable format. The Provenance model allows to outline the VizieR table curation activity. It specifies data origin, involved actors (persons or institutions) and exposes metadata consumed in the publication workflow : photometric calibration with metadata extracted from remote services, reference article, etc.
The exploration results in a prototype which provides the catalogue provenance information in YAML serialisation or in a VODML based mapping."
"P9-226","Jutta Schnabel","Erlangen Centre for Astroparticle Physics (ECAP)","The KM3NeT Open Science Portal","The KM3NeT neutrino detector, a water Cherenkov experiment to detect high-energy relativistic charged particles, is currently under construction at several locations in the Mediterranean Sea. The KM3NeT collaboration will produce scientific data valuable both for the astrophysics and neutrino physics communities as well as marine biologists. In order to facilitate FAIR data sharing of the research results, an Open Science Portal and infrastructure to provide public access to open KM3NeT data, software and services are under development. In this contribution, the current architecture, interfaces and usage examples are presented."
"P9-233","Sarah Weissman","Mikulski Archive / Space Telescope Science Institute","Building a Search Discovery System Powered by Graphs at MAST","As part of an effort to create a unified search system for MAST, we created a prototype search discovery layer built on graph database and search index technology. We present the design of this prototype, challenges encountered in creating a knowledge graph of the MAST archive, and future directions for this work."
"P9-250","Michèle Sanguillon","LUPM - CNRS","Storing Provenance information in a data processing workflow: one CTA use case","Assessment of the quality of the data and the need to be able to reproduce processing requires a good understanding of the provenance of the data and the data processing steps responsible for producing them. The management of this provenance information takes place at various levels such as capture, storage, selection, distribution and visualization.
Here we focus on storing the provenance information returned by a prototype for the reconstruction pipeline, one of many pipelines of the CTA observatory. This pipeline is launched via the CTADIRAC interware in order to optimize the processing on different infrastructures such as the EGI grid, and the provenance information is ingested in an independent external database. The data model used for the Provenance data conforms to the ProvenanceDM data model defined by the IVOA."
"P9-253","François Bonnarel","CDS ObAS CNRS/Université de Strasbourg","CDS ""virtual"" Simple Image Access service","CDS ""virtual"" Simple Image Access service 

F.Bonnarel (CDS), T.Boch (CDS), C.Bot (CDS), Chaitra (INAF OAT), P.Fernique (CDS), L.Michel (ObAS)

Taking benefit of the uprising of HiPS technology in astronomy allowing to homogenize access to pixel data and thanks to the large collection of HiPS it created and operates, CDS recently developed the hips2fits service.  This service allows to provide a cutout and reprojection functionality for a large collection of image surveys. In this poster, we present the SIAP2 interface we developed to allow discovery and tuning of images that can be generated by hips2fits. The interface query responses are built using the MocServer information. This service paves the way towards a new generation of virtual data Simple Image Access services."
"P9-49","Katharina Lutz","CDS, CNRS, Observatoire Astronomique de Strasbourg","Spreading the word – current status of VO tutorials and schools","With some telescopes standing still, now more than ever simple access to archival data is vital for astronomers and they need to know how to go about it. Within European Virtual Observatory (VO) projects, such as COSADIE (2013-2015), Asterics (2015-2018) and Escape (since 2019), we have been offering Virtual Observatory schools for many years. The aim of these schools are twofold: teaching (early career) researchers about the functionalities and possibilities within the Virtual Observatory and collecting feedback from the astronomical community. In addition, the VO dissemination team at CDS started to explore more and new ways to interact with the community: a series of blog posts on AstroBetter.com, a lunch time session at the virtual EAS meeting 2020, a Spanish VO school, GAVO supported events and their Virtual Observatory Text Treasures, and contributions to online archive workshops. In the proposed talk we will present the different formats in more detail, and report on the resulting interaction with the community as well as the estimated reach. Based on these we will discuss which methods work well in which setting, where we can still improve in the future and which methods might become more important and interesting in the future."
"P9-72","Mireille Louys","CDS and ICube Laboratory, University of Strasbourg","Radio Astronomy visibility data discovery and access using IVOA standards","Radio astronomical archives for large facilities (VLA, ASKAP, LOFAR, JIVE, MeerKAT) used to mostly store raw or calibrated visibility data . However, the situation is changing and some facilities (e.g. ALMA), store and distribute science ready data. This will also be the case for future radio telescopes, like SKA, where calibrated and imaged data products will be provided. Naturally, the major goal of these archives is to make their data discoverable and accessible to the astronomy community. Today, many archives give access to visibility data with project-specific web interfaces in order to allow users to reprocess the data with fine tuned reduction parameters. 
Science platforms  such as ESAP developed within the ESCAPE (\footnote{https://projectescape.eu/}) project will allow this.
IVOA decided to make integration of radio astronomy data in the Virtual Observatory (VO) a science priority. Various radio astronomers and projects (NRAO, ASKAP, LOFAR, JIVE, ALMA, SKA, INAF, NenuFAR \footnote{https://nenufar.obs-nancay.fr/en/homepage-en/}, etc.) joined the Radio Astronomy Interest Group of the IVOA, which was recently founded. Together they are paving the way to a better integration of their services in the VO infrastructure and are proposing evolution of IVOA standards to help achieving this goal.

In this context CDS created a prototype for exposing visibility data observations, split in a consistent list of datasets in an ObsTAP service, for coarse grain discovery. Additional metadata such as number of antennae, frequency ranges, uv_density plots, frequency-phase and frequency-amplitude plots, primary and synthesized beams are also provided either by adding column metadata or by using the DataLink technique (\footnote{http://www.ivoa.net/documents/DataLink/}).

This presentation  describes the process of metadata integration in the system, explores how to extend the ObsCore data model specification for radio visibility data and details the choices which have been made. We furthermore show results of data selections through various VO applications. We conclude with the possible evolutions of the prototype and lessons learnt from this exercise.
authors : 
Mireille Louys, Katarina Lutz, Yelena Stein, Anaïs Egner and François Bonnarel"
"P9-81","Gyula I. G. Jozsa","South African Radio Astronomy Observatory","CARACal - The Containerized Automated Radio Astronomy Calibration Pipeline","CARACal has been under development since mid-2017 and was publicly released in May 2020. The project started with the goal to allow the MeerKAT Fornax and MHONGOOSE Large Survey Project (LSP) teams to produce top-quality radio images and data cubes from their raw data in a standardized, easy, and reproducible manner. This goal has now been achieved.

However, CARACal exceeded expectations, in its scope, acceptance, and from an educational point of view. CARACal uses the Stimela containerized pipeline framework (see contribution by Makathini et al. at this conference), which makes it possible to combine the best available (including third-party) software packages into a single pipeline. CARACal is open-source and freely available to not only the LSP teams but anyone who wants to utilise it. It is highly flexible, tunable, and able to reduce data from more than one telescope (uGMRT, JVLA), and it involves scientific analysis steps beyond the mere production of images (to date, source finding and characterisation). Since CARACal is open-source and reiles on containerization technology, any data reduction is transparent and reproducible. The development team involves computer scientists as well as astronomers at various academic levels, from students to professors. This, together with good review strategies, facilitates knowledge exchange on all sides and provides the required feedback to stay user-friendly as well as to sustain an appropriate level of programming style.

In this paper we will discuss the functionality of CARACal, its development structure, including crucial ingredients as well as pitfalls, and discuss future steps to improve its functionality to fully support FAIR principles, in particular interfacing to archives and standardized metadata."
"P9-89","Mathieu Servillat","LUTH - Observatoire de Paris","OPUS: an interoperable job control system based on VO standards","OPUS (Observatoire de Paris UWS System) is a job control system developed using the micro-framework bottle.py. The Universal Worker System pattern v1.1 (UWS) as defined by the International Virtual Observatory Alliance (IVOA) is implemented as a REST service to control job execution on a work cluster. OPUS also follows the recent IVOA Provenance Data Model recommendation to capture and expose the provenance information of jobs and results. By following well defined standards, the tool is interoperable and jobs can be run either through a web interface, or a script, and can be integrated to existing web platforms. Current instances are used in production by several projects at the Observatoire de Paris (CTA/H.E.S.S, MASER, ComPOSE)."
T1,"Brian Broll","Vanderbilt University","A Hands-on Introduction to Deep Learning with DeepForge","This tutorial will provide a hands-on introduction to deep learning using DeepForge, a gateway to deep learning for  scientific computing. DeepForge provides an easy to use, yet powerful visual interface to facilitate the rapid development of deep learning models by novices as well as experts. Utilizing a cloud-based infrastructure, built-in version control and multi-user collaboration support, DeepForge aims to help with the steep learning curve of machine learning. It promotes reproducibility, data provenance, and enables remote execution of machine learning pipelines on various compute platforms. The tool currently supports TensorFlow/Keras and integrates with SciServer (among others).

The tutorial will start with a brief background on some of the basics of machine learning and deep learning then proceed into a hands-on example training a neural network to predict redshift values from galaxy spectra. The example will be end-to-end starting with the exploration of the original dataset (checking for common problems like class imbalance). The tutorial will finish with the evaluation of the trained model including visualizing the data in the learned feature space.

The tutorial will be provided using DeepForge, attendees will need a laptop with a web browser and a (free) SciServer account."
T2,"Peter K. G. Williams","American Astronomical Society / Center for Astrophysics | Harvard & Smithsonian","Interactively exploring and visualizing data on the sky with Jupyter and pywwt","Astronomers routinely work with images of the sky or tables of celestial objects. In this tutorial, participants will learn how to explore such data sets interactively, and in their astronomical context, inside Jupyter notebooks. The key enabling technology is pywwt, a module that allows researchers to embed the AAS WorldWide Telescope (WWT) visualization engine in Python applications and Jupyter notebooks, providing a sophisticated “ds9-like” experience inside the notebook that can be controlled through code as well as manually. Hands-on activities will stitch this Python module together with other elements of the Python ecosystem for working with astronomical data such as astroquery. Participants will learn how to explore their sky-based data sets using a modern suite of software tools."
T3,"Humberto Farias","UTFSM/ULS","From Semantic segmentation to Instance Segmentation using DeepLearning.","The tasks of location, classification, and segmentation are known and applied by astronomers in various problems such as: Morphological classification of galaxies, Transient detection, search for supernovae among others.

Is widely known in this decade will see a series of astronomical mega-projects coming into operation producing complex data whose dimensionality and volume will exceed any current scale. This requires the application of a new generation of machine learning (Deeplearning) models for  classification, location, and segmentation.
In this tutorial we will cover the latest advances in Deep learning applied to Semantic segmentation, Object localization and Instance Segmentation.  The tutorial modality will be divided into blocks of 30 minutes as follows:
Part 1: Introduction (Theory)
Part 2: Semantic segmentation (U-NET)
Part 3: Object localization (YOLO3)
Part 4: Instance Segmentation (Mask R-CNN)
Prerequisites: Intermediate Python knowledge is strongly recommended.

Level: We assume you are comfortable with deep learning basics as layers, neuron, activation function, loss function among other basics concepts."
T4,"Hendrik Heinl","CDS, Observatoire astronomique de Strasbourg","Multimessenger Astronomy with ObsTAP and pyVO","The Tutorial will have two parts: the first part will be dedicated to data discovery with ""ObsTAP"", a VO standard that was developed for this exact use case. Participants will get a little insight of how the standard works, and how it can be used from within software like TOPCAT, Splat-VO or Aladin. 

The second part of the tutorial will focus on how to use ObsTAP within python scripts with the help of the astropy affiliated package pyVO. We will define criteria, and let the script ""find"" feasable data services, and query those. 

We will use a multi messenger use case to emphasize the benefits of the used standards and tools."
"O2-157","Brian Hayden","Space Telescope Science Institute","How to better describe software for discovery and citation","With the Hubble Space Telescope (HST), STScI and NASA pioneered ‘science ready data’ in the archive and made it possible for the community to go from observations to science as quickly as possible. The technologies required for calibrating observational data broadly breaks down into two main components: 1) the calibration pipeline that encodes the algorithms necessary for optimal extraction of data and 2) the supporting data management infrastructure used to run, manage, and monitor the pipelines and store the calibrated data products. In this talk I will present a project to deploy the HST calibration pipeline on commercial cloud infrastructure. I will discuss the trade-offs of using cloud-native vs. non-native services and changes we have had to make to our approach to processing in order to accommodate this workflow. I will also discuss new opportunities made available by the use of cloud services. Finally, I will compare costs between the two approaches."
